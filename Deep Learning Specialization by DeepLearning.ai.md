# Deep Learning Specialization by DeepLearning.ai

This provides all the available free material from this class.  The Coursera Specialization is at [https://www.coursera.org/specializations/deep-learning](https://www.coursera.org/specializations/deep-learning).  Here is [this link](https://www.youtube.com/playlist?list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K), [this link](https://www.youtube.com/playlist?list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud), and [this link](https://www.youtube.com/playlist?list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7) to the YouTube videos.  This just brings everything together!

## Course 1: Neural Networks and Deep Learning

### Week 1: Introduction to Deep Learning

1. [**Video:** Welcome](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/Cuf2f/welcome)
2. [**Video:** What is a neural network?](https://www.youtube.com/watch?v=yV3e8LwWs3U&index=3&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
3. [**Video:** Supervised Learning with Neural Networks](https://www.youtube.com/watch?v=Jt6uYZJ0xss&index=4&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
4. [**Video:** Why is Deep Learning taking off?](https://www.youtube.com/watch?v=j4-QFpTVCtM&index=5&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
5. [**Video:** Geoffrey Hinton interview](https://www.youtube.com/watch?v=-eyhCTvrEtE)

### Week 2: Neural Network Basics

1. [**Video:** Binary Classification](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/Z8j0R/binary-classification)
2. [**Video:** Logistic Regression](https://www.youtube.com/watch?v=6tByJTacCOc&index=7&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
3. [**Video:** Logistic Regression Cost Function](https://www.youtube.com/watch?v=g8xPrtJ2Hg8&index=8&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
4. [**Video:** Gradient Descent](https://www.youtube.com/watch?v=8zUqnc4PqOY&index=9&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
5. [**Video:** Derivatives](https://www.youtube.com/watch?v=sMrv5ZIctVA&index=10&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
6. [**Video:** More Derivative Examples](https://www.youtube.com/watch?v=4EYUtSW_7lg&index=11&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
7. [**Video:** Computation graph](https://www.youtube.com/watch?v=I_ixUbXoNYY&index=12&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
8. [**Video:** Derivatives with a Computation Graph](https://www.youtube.com/watch?v=kABtf1z88RY&index=13&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
9. [**Video:** Logistic Regression Gradient Descent](https://www.youtube.com/watch?v=mUuCIMFQ1Zg&index=14&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
10. [**Video:** Gradient Descent on m Examples](https://www.youtube.com/watch?v=ERIFzbTdIm0&index=15&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
11. [**Video:** Vectorization](https://www.youtube.com/watch?v=9YHWgxwzwD8&index=16&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
12. [**Video:** More Vectorization Examples](https://www.youtube.com/watch?v=Y0H4c49Hkec&index=17&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
13. [**Video:** Vectorizing Logistic Regression](https://www.youtube.com/watch?v=MDRnIjPiD_4&index=18&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
14. [**Video:** Vectorizing Logistic Regression's Gradient Output](https://www.youtube.com/watch?v=m5o0UdLtjwU&index=19&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
15. [**Video:** Broadcasting in Python](https://www.youtube.com/watch?v=BiJLpVN0nwE&index=20&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
16. [**Video:** A note on python/numpy vectors](https://www.youtube.com/watch?v=ImW9zGMw7cc&index=21&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
17. [**Video:** Quick tour of Jupyter/iPython Notebooks](https://www.youtube.com/watch?v=GufvjWt7xnc&index=22&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
18. [**Video:** Explanation of logistic regression cost function (optional)](https://www.youtube.com/watch?v=QTeq8Ru51R8&index=23&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
19. [**Notebook:** Python Basics with numpy (optional) - From Anderson B.'s github](https://github.com/andersy005/deep-learning-specialization-coursera/blob/master/01-Neural-Networks-and-Deep-Learning/week2/Programming-Assignments/Python%2BBasics%2BWith%2BNumpy%2Bv2.ipynb)
20. [**Notebook:** Logistic Regression with a Neural Network mindset](https://nbviewer.jupyter.org/github/jeffreyrnorton/Notebooks_MachineLearning/blob/master/LogisticRegressionWithANeuralNetworkMindset.ipynb)
21. [**Video:** Pieter Abbeel interview](https://www.youtube.com/watch?v=dmkPJpWCVcI)

### Week 3: Shallow Neural Networks

1. [**Video:** Neural Networks Overview](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/qg83v/neural-networks-overview)
2. [**Video:** Neural Network Representation](https://www.youtube.com/watch?v=jTsR1xrlzCo&index=25&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
3. [**Video:** Computing a Neural Network's Output](https://www.youtube.com/watch?v=-jxrYTq0uNo&index=26&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
4. [**Video:** Vectorizing across multiple examples](https://www.youtube.com/watch?v=4lRL7sFEalg&index=27&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
5. [**Video:** Explanation for Vectorized Implementation](https://www.youtube.com/watch?v=7WmBBb488yo&index=28&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
6. [**Video:** Activation functions](https://www.youtube.com/watch?v=8nBmMc__bM4&index=29&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
7. [**Video:** Why do you need non-linear activation functions?](https://www.youtube.com/watch?v=aHQgFpJ_xp8&index=30&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
8. [**Video:** Derivatives of activation functions](https://www.youtube.com/watch?v=-CG_t8e9Bac&index=31&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
9. [**Video:** Gradient descent for Neural Networks](https://www.youtube.com/watch?v=6D0rQxgnrZM&index=32&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
10. [**Video:** Backpropagation intuition (optional)](https://www.youtube.com/watch?v=uyM_3Z8Hn5c&index=33&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
11. [**Video:** Random Initialization of weights](https://www.youtube.com/watch?v=RuCdqZkpsQk&index=34&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
12. [**Notebook:** Planar data classification with a hidden layer](https://nbviewer.jupyter.org/github/jeffreyrnorton/Notebooks_MachineLearning/blob/master/PlanarDataClassificationWithOneHiddenLayer.ipynb)
13. [**Video:** Ian Goodfellow interview](https://www.youtube.com/watch?v=pWAc9B2zJS4)

### Week 4: Deep Neural Networks

1. [**Video:** Deep L-layer neural network](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/7dP6E/deep-l-layer-neural-network)
2. [**Video:** Forward Propagation in a Deep Network](https://www.youtube.com/watch?v=QuJnEPzTKlU&index=36&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
3. [**Video:** Getting your matrix dimensions right](https://www.youtube.com/watch?v=Vs7lDornvg0&index=37&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
4. [**Video:** Why deep representations?](https://www.youtube.com/watch?v=MKJ968Ik4_s&index=38&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
5. [**Video:** Building blocks of deep neural networks](https://www.youtube.com/watch?v=RWci2OQmAK4&index=39&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
6. [**Video:** Forward and Backward Propagation](https://www.youtube.com/watch?v=hcnIBkMjqTU&index=40&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
7. [**Video:** Parameters vs Hyperparameters](https://www.youtube.com/watch?v=PkHS3G3u6pM&index=41&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
8. [**Video:** What does this have to do with the brain?](https://www.youtube.com/watch?v=c79E2Bh7ayM&index=42&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
9. [**Notebook:** Building your Deep Neural Network: Step by Step](https://nbviewer.jupyter.org/github/jeffreyrnorton/Notebooks_MachineLearning/blob/master/BuildingYourDeepNeuralNetwork_StepbyStep.ipynb)
10. [**Notebook:** Deep Neural Network - Application](https://nbviewer.jupyter.org/github/jeffreyrnorton/Notebooks_MachineLearning/blob/master/DeepNeuralNetwork_Application.ipynb)

## Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization

### Week 1: Practical Aspects of Deep Learning

1. [**Video:** Train / Dev / Test sets](https://www.coursera.org/learn/deep-neural-network/lecture/cxG1s/train-dev-test-sets)
2. [**Video:** Bias / Variance](https://www.youtube.com/watch?v=K8J_63XdZW8&index=44&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
3. [**Video:** Basic Recipe for Machine Learning](https://www.youtube.com/watch?v=DkgJ_VkU5jM&index=45&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
4. [**Video:** Regularization](https://www.youtube.com/watch?v=35ir2VqYyKk&index=46&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
5. [**Video:** Why regularization reduces overfitting?](https://www.youtube.com/watch?v=kqj6ltmD3aA&index=47&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
6. [**Video:** Dropout Regularization](https://www.youtube.com/watch?v=vNvX2IYmaRY&index=48&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
7. [**Video:** Understanding Dropout](https://www.youtube.com/watch?v=pQ9iuEy8kkI&index=49&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
8. [**Video:** Other regularization methods](https://www.youtube.com/watch?v=U6-mvGhSeT4&index=50&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
9. [**Video:** Normalizing inputs](https://www.youtube.com/watch?v=VzFRL4PZfNo&index=51&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
10. [**Video:** Vanishing / Exploding gradients](https://www.youtube.com/watch?v=4cwbbTJZx-w&index=52&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
11. [**Video:** Weight Initialization for Deep Networks](https://www.youtube.com/watch?v=QaIrKMOd3hE&index=53&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
12. [**Video:** Numerical approximation of gradients](https://www.youtube.com/watch?v=GiVFM7KPzL4&index=54&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
13. [**Video:** Gradient checking](https://www.youtube.com/watch?v=yfJCp9u-C_M&index=55&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
14. [**Video:** Gradient Checking Implementation Notes](https://www.youtube.com/watch?v=-4n1tk89FmE&index=56&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
15. **Notebook:** Initialization - <TODO>
16. **Notebook:** Regularization - <TODO>
17. **Notebook:** Gradient Checking - <TODO>
18. [**Video:** Yoshua Bengio interview](https://www.youtube.com/watch?v=pnTLZQhFpaE)

### Week 2: Hyperparameter tuning, Batch Normalization and Programming Frameworks

1. [**Video:** Tuning process](https://www.coursera.org/learn/deep-neural-network/lecture/dknSn/tuning-process)
2. [**Video:** Using an appropriate scale to pick hyperparameters]()
3. [**Video:** Hyperparameters tuning in practice: Pandas vs. Caviar](https://www.youtube.com/watch?v=wP-DzAtE71g&index=69&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
4. [**Video:** Normalizing activations in a network](https://www.youtube.com/watch?v=fv1Luwd-LOI&index=70&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
5. [**Video:** Fitting Batch Norm into a neural network](https://www.youtube.com/watch?v=h2LUhYVkmlw&index=71&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
6. [**Video:** Why does Batch Norm work?](https://www.youtube.com/watch?v=Y5wA5iFb7Mg&index=72&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
7. [**Video:** Batch Norm at test time](https://www.youtube.com/watch?v=JU7Bda4Z1zU&index=73&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
8. [**Video:** Softmax Regression](https://www.youtube.com/watch?v=9r7iSBzhBs4&index=74&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
9. [**Video:** Training a softmax classifier](https://www.youtube.com/watch?v=43bLqM6kkd4&index=75&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
10. [**Video:** Deep learning frameworks](https://www.youtube.com/watch?v=3uonEeJzHEk&index=76&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
11. [**Video:** TensorFlow](https://www.youtube.com/watch?v=B5nCOPAJfoE&index=77&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
12. **Notebook:** Tensorflow - <TODO>

## Course 3: Structuring Machine Learning Projects

### Week 1: ML Strategy

1. [**Video:** Why ML Strategy](https://www.coursera.org/learn/machine-learning-projects/lecture/yeHYT/why-ml-strategy)
2. [**Video:** Orthogonalization](https://www.youtube.com/watch?v=_xFe47-MUng&index=79&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
3. [**Video:** Single number evaluation metric](https://www.youtube.com/watch?v=uYaGcvHdw3o&index=80&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
4. [**Video:** Satisficing and Optimizing metric](https://www.youtube.com/watch?v=aZ3PpmtL3uw&index=81&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
5. [**Video:** Train/dev/test distributions](https://www.youtube.com/watch?v=acPqxirP_CM&index=82&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
6. [**Video:** Size of the dev and test sets](https://www.youtube.com/watch?v=agFffntKiAk&index=83&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
7. [**Video:** When to change dev/test sets and metrics](https://www.youtube.com/watch?v=DNsb7iH0zys&index=84&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
8. [**Video:** Why human-level performance?](https://www.youtube.com/watch?v=tWLf-VRrVRM&index=85&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
9. [**Video:** Avoidable bias](https://www.youtube.com/watch?v=HRo89hU8ysk&index=86&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
10. [**Video:** Understanding human-level performance](https://www.youtube.com/watch?v=OFEfbu2Ykaw&index=87&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
11. [**Video:** Surpassing human-level performance](https://www.youtube.com/watch?v=WNlvKrPK8Ys&index=88&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
12. [**Video:** Improving your model performance](https://www.youtube.com/watch?v=NlTyIsH4ig4&index=89&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
13. **Reading:** Machine Learning flight simulator - N/A
14. [**Video:** Andrej Karpathy interview](https://www.youtube.com/watch?v=_au3yw46lcg)

### Week 2: ML Strategy (2)

1. [**Video:** Carrying out error analysis](https://www.coursera.org/learn/machine-learning-projects/lecture/GwViP/carrying-out-error-analysis)
2. [**Video:** Cleaning up incorrectly labeled data](https://www.youtube.com/watch?v=ts-lEbWtTo0&index=91&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
3. [**Video:** Build your first system quickly, then iterate](https://www.youtube.com/watch?v=OoKwl-A0alM&index=92&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
4. [**Video:** Training and testing on different distributions](https://www.youtube.com/watch?v=eEsF7wqnrVA&index=93&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
5. [**Video:** Bias and Variance with mismatched data distributions](https://www.youtube.com/watch?v=dMtpXNyULd0&index=94&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
6. [**Video:** Addressing data mismatch](https://www.youtube.com/watch?v=B6QwwfZVsLY&index=95&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
7. [**Video:** Transfer learning](https://www.youtube.com/watch?v=uDo19Dt8SRw&index=96&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
8. [**Video:** Multi-task learning](https://www.youtube.com/watch?v=tw4I6EdJKMo&index=97&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
9. [**Video:** What is end-to-end deep learning?](https://www.youtube.com/watch?v=G6VEypvJv3E&index=98&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
10. [**Video:** Whether to use end-to-end deep learning](https://www.youtube.com/watch?v=S8aXbIETQlA&index=99&t=0s&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K)
11. [**Video:** Ruslan Salakhutdinov interview](https://www.youtube.com/watch?v=EveYfHKXvfc)

## Course 4: Convolutional Neural Networks

### Week 1: Foundations of Convolutional Neural Networks 

1. [**Video:** Computer Vision](https://www.coursera.org/learn/convolutional-neural-networks/lecture/Ob1nR/computer-vision)
2. [**Video:** Edge Detection Example](https://www.youtube.com/watch?v=TFrO-kS9tDg&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=3&t=2s)
3. [**Video:** More Edge Detection](https://www.youtube.com/watch?v=EltFF6mMnoo&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=4&t=7s)
4. [**Video:** Padding](https://www.youtube.com/watch?v=lllsnQuv7LA&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=5&t=0s)
5. [**Video:** Strided Convolutions](https://www.youtube.com/watch?v=iLNc3_C0uBI&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=6&t=4s)
6. [**Video:** Convolutions Over Volume](https://www.youtube.com/watch?v=Uolm8oLPocA&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=7&t=0s)
7. [**Video:** One Layer of a Convolutional Network](https://www.youtube.com/watch?v=tSdY69FFSpo&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=8&t=0s)
8. [**Video:** Simple Convolutional Network Example](https://www.youtube.com/watch?v=xQGREGyusys&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=9&t=0s)
9. [**Video:** Pooling Layers](https://www.youtube.com/watch?v=xm0JwZmJmig&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=10&t=0s)
10. [**Video:** CNN Example](https://www.youtube.com/watch?v=w2sRcGha9nM&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=11&t=0s)
11. [**Video:** Why Convolutions?](https://www.youtube.com/watch?v=C_U2Ymf9qgY&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=12&t=0s)
12. **Notebook:** Convolutional Model: step by step <TODO>
13. **Notebook:** Convolutional Model: application <TODO>
14. [**Video:** Yann LeCun Interview](https://www.youtube.com/watch?v=Svb1c6AkRzE)

### Week 2: Deep Convolutional models: case studies

1. [**Video:** Why look at case studies?](https://www.coursera.org/learn/convolutional-neural-networks/lecture/KvAM9/why-look-at-case-studies)
2. [**Video:** Classic Networks](https://www.youtube.com/watch?v=WkQYOrznC48&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=14&t=0s)
3. [**Video:** ResNets](https://www.youtube.com/watch?v=K0uoBKBQ1gA&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=15&t=0s)
4. [**Video:** Why ResNets Work](https://www.youtube.com/watch?v=GSsKdtoatm8&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=16&t=0s)
5. [**Video:** Networks in Networks and 1x1 Convolutions](https://www.youtube.com/watch?v=9EZVpLTPGz8&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=17&t=0s)
6. [**Video:** Inception Network Motivation](https://www.youtube.com/watch?v=HunX473yXEI&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=18&t=0s)
7. [**Video:** Inception Network](https://www.youtube.com/watch?v=LUaRa65J_Ms&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=19&t=0s)
8. [**Video:** Using Open-Source Implementation](https://www.youtube.com/watch?v=-7iMlG-fhkA&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=20&t=0s)
9. [**Video:** Transfer Learning](https://www.youtube.com/watch?v=TaiWVP3JYxo&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=21&t=0s)
10. [**Video:** Data Augmentation](https://www.youtube.com/watch?v=AxC-L46vSmY&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=22&t=0s)
11. [**Video:** State of Computer Vision](https://www.youtube.com/watch?v=P-MxNkLyFNA&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=23&t=0s)
12. **Notebook:** Keras Tutorial - The Happy House (not graded) <TODO>
13. **Notebook:** Residual Networks <TODO>

### Week 3: Object Detection

1. [**Video:** Object Localization](https://www.coursera.org/learn/convolutional-neural-networks/lecture/nEeJM/object-localization)
2. [**Video:** Landmark Detection](https://www.youtube.com/watch?v=FAo0VmkuASE&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=25&t=0s)
3. [**Video:** Object Detection](https://www.youtube.com/watch?v=UBTRCUJEZww&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=26&t=0s)
4. [**Video:** Convolutional Implementation of Sliding Windows](https://www.youtube.com/watch?v=mFunGvD5sVc&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=27&t=0s)
5. [**Video:** Bounding Box Predictions](https://www.youtube.com/watch?v=DFjHkXhkYzA&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=28&t=0s)
6. [**Video:** Intersection Over Union](https://www.youtube.com/watch?v=DNEm4fJ-rto&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=29&t=0s)
7. [**Video:** Non-max Suppression](https://www.youtube.com/watch?v=A46HZGR5fMw&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=30&t=0s)
8. [**Video:** Anchor Boxes](https://www.youtube.com/watch?v=Pf7iFeRPYK8&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=31&t=0s)
9. [**Video:** YOLO Algorithm](https://www.youtube.com/watch?v=YQYtgzOf9g4&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=32&t=0s)
10. [**Video:** (Optional) Region Proposals](https://www.youtube.com/watch?v=ruP04YTNmCA&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=33&t=0s)
11. **Notebook:** Car detection with YOLOv2 <TODO>

### Week 4: Special applications: Face recognition & Neural style transfer 

1. [**Video:** What is face recognition?](https://www.coursera.org/learn/convolutional-neural-networks/lecture/lUBYU/what-is-face-recognition)
2. [**Video:** One Shot Learning](https://www.youtube.com/watch?v=Qgdn5AQfpoI&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=35&t=0s)
3. [**Video:** Siamese Network](https://www.youtube.com/watch?v=wIyLlY9E4ME&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=36&t=0s)
4. [**Video:** Triplet Loss](https://www.youtube.com/watch?v=jyPYdnEqCAk&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=37&t=0s)
5. [**Video:** Face Verification and Binary Classification](https://www.youtube.com/watch?v=2Cd4BjtFkKs&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=38&t=0s)
6. [**Video:** What is neural style transfer?](https://www.youtube.com/watch?v=Re2C9INXCNc&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=39&t=0s)
7. [**Video:** What are deep ConvNets learning?](https://www.youtube.com/watch?v=Uzjt07ZD-zI&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=40&t=0s)
8. [**Video:** Cost Function](https://www.youtube.com/watch?v=QbCWY93ySO8&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=41&t=0s)
9. [**Video:** Content Cost Function](https://www.youtube.com/watch?v=_KjiZnXY5-Y&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=42&t=0s)
10. [**Video:** Style Cost Function](https://www.youtube.com/watch?v=pj1QuA7Nlgo&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=43&t=0s)
11. [**Video:** 1D and 3D Generalizations](https://www.youtube.com/watch?v=C4wL3M0eCOE&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud&index=44&t=0s)
12. **Notebook:** Art generation with Neural Style Transfer <TODO>
13. **Notebook:** Face Recognition for the Happy House <TODO>

## Course 5: Sequence Models

### Week 1: Recurrent Neural Networks

1. [**Video:** Why sequence models](https://www.coursera.org/learn/nlp-sequence-models/lecture/0h7gT/why-sequence-models)
2. [**Video:** Notation](https://www.youtube.com/watch?v=T38JKvcHg3c&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=3&t=0s)
3. [**Video:** Recurrent Neural Network Model](https://www.youtube.com/watch?v=poIosUbeu-I&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=4&t=0s). 
4. [**Video:** Backpropagation through time](https://www.youtube.com/watch?v=Oxj_-bKq4Ns&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=5&t=0s).
5. [**Video:** Different types of RNNs](https://www.youtube.com/watch?v=K76bjxTcqhI&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=6&t=0s)
6. [**Video:** Language model and sequence generation](https://www.youtube.com/watch?v=J2kg4HpR8kw&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=7&t=0s)
7. [**Video:** Sampling novel sequences](https://www.youtube.com/watch?v=tsHkxyy4Dw0&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=8&t=0s)
8. [**Video:** Vanishing gradients with RNNs](https://www.youtube.com/watch?v=_5PDuD62UQ0&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=9&t=0s)
9. [**Video:** Gated Recurrent Unit (GRU)](https://www.youtube.com/watch?v=wSabaLGEegM&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=10&t=0s)
10. [**Video:** Long Short Term Memory (LSTM)](https://www.youtube.com/watch?v=fdY10i0MAQc&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=11&t=0s).
11. [**Video:** Bidirectional RNN](https://www.youtube.com/watch?v=qIU-Vep24T4&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=12&t=0s)
12. [**Video:** Deep RNNs](https://www.youtube.com/watch?v=NYU9IwhtVio&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=13&t=0s).
13. **Notebook:** Building a recurrent neural network - step by step <TODO>
14. **Notebook:** Dinosaur Island - Character-Level Language Modeling <TODO>
15. **Notebook:** Jazz improvisation with LSTM <TODO>

### Week 2: Natural Language Processing & Word Embeddings 

1. [**Video:** Word Representation](https://www.coursera.org/learn/nlp-sequence-models/lecture/6Oq70/word-representation).
2. [**Video:** Using word embeddings](https://www.youtube.com/watch?v=JKpm3DMSSMI&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=15&t=0s)
3. [**Video:** Properties of word embeddings](https://www.youtube.com/watch?v=fmfpwyyg6mQ&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=16&t=0s).
4. [**Video:** Embedding matrix](https://www.youtube.com/watch?v=eZ_HocUaJhU&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=17&t=0s)
5. [**Video:** Learning word embeddings](https://www.youtube.com/watch?v=StvYWjZ330w&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=18&t=0s)
6. [**Video:** Word2Vec](https://www.youtube.com/watch?v=diUiV48q-5c&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=19&t=0s).
7. [**Video:** Negative Sampling](https://www.youtube.com/watch?v=B5rvknrwixE&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=20&t=0s)
8. [**Video:** GloVe word vectors](https://www.youtube.com/watch?v=CE3PXQkbupk&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=21&t=0s).
9. [**Video:** Sentiment Classification](https://www.youtube.com/watch?v=UHJvuhhjc0E&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=22&t=0s)
10. [**Video:** Debiasing word embeddings](https://www.youtube.com/watch?v=V-tudqFc5y4&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=23&t=0s)
11. **Notebook:** Operations on word vectors - Debiasing <TODO>
12. **Notebook:** Emojify <TODO>

### Week 3: Sequence models & mechanism 

1. [**Video:** Basic Models](https://www.coursera.org/learn/nlp-sequence-models/lecture/HyEui/basic-models)
2. [**Video:** Picking the most likely sentence](https://www.youtube.com/watch?v=xfplLFXsdjc&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=25&t=0s)
3. [**Video:** Beam Search](https://www.youtube.com/watch?v=zPv7nWB4_ts&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=26&t=0s)
4. [**Video:** Refinements to Beam Search](https://www.youtube.com/watch?v=4DOOHnt_5Js&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=27&t=0s)
5. [**Video:** Error analysis in beam search](https://www.youtube.com/watch?v=gGw_YS7qCdg&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=28&t=0s)
6. [**Video:** Bleu Score (optional)](https://www.youtube.com/watch?v=KwNo9_8Ys9U&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=29&t=0s)
7. [**Video:** Attention Model Intuition](https://www.youtube.com/watch?v=nV2a9wT5xPU&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=30&t=0s)
8. [**Video:** Attention Model](https://www.youtube.com/watch?v=aSrOdArCp_4&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=31&t=0s)
9. [**Video:** Speech recognition](https://www.youtube.com/watch?v=Bl2rca9HPRM&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=32&t=0s)
10. [**Video:** Trigger Word Detection](https://www.youtube.com/watch?v=KofGXJhbXBc&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=33&t=0s)
11. [**Video:** Conclusion and thank you](https://www.youtube.com/watch?v=xAk2ifcKpdk&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=34&t=0s)
12. **Notebook:** Neural Machine Translation with Attention <TODO>
13. **Notebook:** Trigger word detection <TODO>

# Deep Learning Reading List

[Source](deeplearning.net/reading-list).  Additional references have been added.

## Books

* [Deep Learning](http://deeplearningbook.org), Yoshua Bengio, Ian Goodfellow, Aaron Courville, MIT Press, In preparation.

## Review Papers

* [Representation Learning: A Review and New Perspectives](http://arxiv.org/abs/1206.5538), Yoshua Bengio, Aaron Courville, Pascal Vincent, Arxiv, 2012.
* The monograph or review paper [Learning Deep Architectures for AI](http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/239) (Foundations & Trends in Machine Learning, 2009).
* Deep Machine Learning – A New Frontier in Artificial Intelligence Research – a [survey paper](http://www.ece.utk.edu/~itamar/Papers/DML_Arel_2010.pdf) by Itamar Arel, Derek C. Rose, and Thomas P. Karnowski.
* Graves, A. (2012). [*Supervised sequence labelling with recurrent neural networks*](https://www.cs.toronto.edu/~graves/preprint.pdf)(Vol. 385). Springer.
* Schmidhuber, J. (2014). Deep Learning in Neural Networks: An Overview. 75 pages, 850+ references, <http://arxiv.org/abs/1404.7828>, PDF & LATEX source & complete public BIBTEX file under <http://www.idsia.ch/~juergen/deep-learning-overview.html>.
* LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. [“Deep learning.”](https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf) *Nature* 521, no. 7553 (2015): 436-444.

## Reinforcement Learning

  - Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. “Playing Atari with deep reinforcement learning.” *arXiv preprint arXiv:1312.5602* (2013).

  - Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu. “[Recurrent Models of Visual Attention](http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf)” ArXiv e-print, 2014.

  - Silver, David. "[Reinforcement Learning](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)". University College London, COMPM050 course. 2015

- ### Computer Vision

* [ImageNet Classification with Deep Convolutional Neural Networks](http://books.nips.cc/papers/files/nips25/NIPS2012_0534.pdf), Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, NIPS 2012.
* [Going Deeper with Convolutions](http://arxiv.org/abs/1409.4842), Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, 19-Sept-2014.
* [Learning Hierarchical Features for Scene Labeling](http://yann.lecun.com/exdb/publis/pdf/farabet-pami-13.pdfhttp://), Clement Farabet, Camille Couprie, Laurent Najman and Yann LeCun, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013.
* [ Learning Convolutional Feature Hierachies for Visual Recognition](http://yann.lecun.com/exdb/publis/pdf/koray-nips-10.pdf), Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau, Karol Gregor, Michaël Mathieu and Yann LeCun, Advances in Neural Information Processing Systems (NIPS 2010), 23, 2010.
* Graves, Alex, et al. [“A novel connectionist system for unconstrained handwriting recognition.”](http://profs.info.uaic.ro/~ancai/DIP/articole/handwriting%20recognition_2008.pdf) *Pattern Analysis and Machine Intelligence, IEEE Transactions on* 31.5 (2009): 855-868.
* Cireşan, D. C., Meier, U., Gambardella, L. M., & Schmidhuber, J. (2010). [Deep, big, simple neural nets for handwritten digit recognition.](http://arxiv.org/pdf/1003.0358) *Neural computation*, *22*(12), 3207-3220.
* Ciresan, Dan, Ueli Meier, and Jürgen Schmidhuber. [“Multi-column deep neural networks for image classification.” ](http://arxiv.org/pdf/1202.2745)*Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on*. IEEE, 2012.
* Ciresan, D., Meier, U., Masci, J., & Schmidhuber, J. (2011, July). [A committee of neural networks for traffic sign classification.](http://www.idsia.ch/~masci/papers/2011_ijcnn.pdf) In *Neural Networks (IJCNN), The 2011 International Joint Conference on* (pp. 1918-1921). IEEE.
* [Stanford CS231n — Convolutional Neural Networks for Visual Recognition](https://www.youtube.com/watch?v=g-PvXUjD6qg&list=PLlJy-eBtNFt6EuMxFYRiNRS07MCWN5UIA) [class link](http://cs231n.stanford.edu/). Winter 2016.

## NLP and Speech

* [Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing](http://www.hds.utc.fr/~bordesan/dokuwiki/lib/exe/fetch.php?id=en%3Apubli&cache=cache&media=en:bordes12aistats.pdf), Antoine Bordes, Xavier Glorot, Jason Weston and Yoshua Bengio (2012), in: Proceedings of the 15th International Conference on Artificial Intelligence and Statistics (AISTATS)
* [Dynamic pooling and unfolding recursive autoencoders for paraphrase detection](http://www.google.ca/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CDcQFjAA&url=http%3A%2F%2Fnlp.stanford.edu%2Fpubs%2FSocherHuangPenningtonNgManning_NIPS2011.pdf&ei=lOrQUKjWNMS8yAHwzYDYAQ&usg=AFQjCNHGo9FDJKPaQVGSWzfIzsf6UiLXeQ&bvm=bv.1355534169,d.aWc). Socher, R., Huang, E. H., Pennington, J., Ng, A. Y., and Manning, C. D. (2011a).  In NIPS’2011.
* [Semi-supervised recursive autoencoders for predicting sentiment distributions](http://nlp.stanford.edu/pubs/SocherPenningtonHuangNgManning_EMNLP2011.pdf). Socher, R., Pennington, J., Huang, E. H., Ng, A. Y., and Manning, C. D. (2011b).  In EMNLP’2011.
* Mikolov Tomáš: [Statistical Language Models based on Neural Networks](http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf). PhD thesis, Brno University of Technology, 2012.
* Graves, Alex, and Jürgen Schmidhuber. “[Framewise phoneme classification with bidirectional LSTM and other neural network architectures.](http://www.researchgate.net/publication/7647316_Framewise_phoneme_classification_with_bidirectional_LSTM_and_other_neural_network_architectures/file/60b7d51704ddf9793d.pdf)” *Neural Networks* 18.5 (2005): 602-610.
* Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. [“Distributed representations of words and phrases and their compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality).” In *Advances in Neural Information Processing Systems*, pp. 3111-3119. 2013.
* K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, Y. Bengio. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](http://arxiv.org/abs/1406.1078). EMNLP 2014.
* Sutskever, Ilya, Oriol Vinyals, and Quoc VV Le. “[Sequence to sequence learning with neural networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf).” *Advances in Neural Information Processing Systems*. 2014.
* Socher, Richard,[Stanford CS224n — Natural Language Processing with Deep Learning](https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6) ([class link](http://web.stanford.edu/class/cs224n/)), Winter 2017.
* Blunson, Phil. https://github.com/oxford-cs-deepnlp-2017/lectures, 2017.

## Disentangling Factors and Variations with Depth

* Goodfellow, Ian, et al. “Measuring invariances in deep networks.” *Advances in neural information processing systems* 22 (2009): 646-654.
* Bengio, Yoshua, et al. “Better Mixing via Deep Representations.” *arXiv preprint arXiv:1207.4404* (2012).
* [Xavier Glorot](http://www.iro.umontreal.ca/~lisa/publications2/index.php/authors/show/333), [Antoine Bordes](http://www.iro.umontreal.ca/~lisa/publications2/index.php/authors/show/358) and [Yoshua Bengio](http://www.iro.umontreal.ca/~lisa/publications2/index.php/authors/show/1), [Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach](http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/494), in: Proceedings of the Twenty-eight International Conference on Machine Learning (ICML’11), pages 97-110, 2011.

## Transfer Learning and domain adaptation

* Raina, Rajat, et al. “Self-taught learning: transfer learning from unlabeled data.” *Proceedings of the 24th international conference on Machine learning*. ACM, 2007.
* [Xavier Glorot](http://www.iro.umontreal.ca/~lisa/publications2/index.php/authors/show/333), [Antoine Bordes](http://www.iro.umontreal.ca/~lisa/publications2/index.php/authors/show/358) and [Yoshua Bengio](http://www.iro.umontreal.ca/~lisa/publications2/index.php/authors/show/1), [Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach](http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/494), in: Proceedings of the Twenty-eight International Conference on Machine Learning (ICML’11), pages 97-110, 2011.
* R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa. **Natural Language Processing (Almost) from Scratch**. *Journal of Machine Learning Research, 12:2493-2537, 2011*.
* Mesnil, Grégoire, et al. “Unsupervised and transfer learning challenge: a deep learning approach.” *Unsupervised and Transfer Learning Workshop, in conjunction with ICML*. 2011.
* Ciresan, D. C., Meier, U., & Schmidhuber, J. (2012, June). [Transfer learning for Latin and Chinese characters with deep neural networks](http://www.idsia.ch/~ciresan/data/ijcnn2012_v9.pdf). In *Neural Networks (IJCNN), The 2012 International Joint Conference on* (pp. 1-6). IEEE.
* Goodfellow, Ian, Aaron Courville, and Yoshua Bengio. “[Large-Scale Feature Learning With Spike-and-Slab Sparse Coding.](http://arxiv.org/pdf/1206.6407)” *ICML 2012*.

## Practical Tricks and Guides

* [“Improving neural networks by preventing co-adaptation of feature detectors.”](http://arxiv.org/pdf/1207.0580v1) Hinton, Geoffrey E., et al.  arXiv preprint arXiv:1207.0580 (2012).
* [Practical recommendations for gradient-based training of deep architectures](http://arxiv.org/abs/1206.5533), Yoshua Bengio, U. Montreal, arXiv report:1206.5533, Lecture Notes in Computer Science Volume 7700, Neural Networks: Tricks of the Trade Second Edition, Editors: Grégoire Montavon, Geneviève B. Orr, Klaus-Robert Müller, 2012.
* A practical [guide](http://www.cs.utoronto.ca/~hinton/absps/guideTR.pdf) to training Restricted Boltzmann Machines, by Geoffrey Hinton.

## Sparse Coding

* [Emergence of simple-cell receptive field properties by learning a sparse code for natural images](http://people.cs.umass.edu/~eshelham/files/papers/olshausen-field-emergence-simple-cells.pdf), Bruno Olhausen, Nature 1996.
* Kavukcuoglu, Koray, Marc’Aurelio Ranzato, and Yann LeCun. “[Fast inference in sparse coding algorithms with applications to object recognition.](http://arxiv.org/pdf/1010.3467)” *arXiv preprint arXiv:1010.3467* (2010).
* Goodfellow, Ian, Aaron Courville, and Yoshua Bengio. “[Large-Scale Feature Learning With Spike-and-Slab Sparse Coding.](http://arxiv.org/pdf/1206.6407)” *ICML 2012*.
* Efficient sparse coding algorithms. Honglak Lee, Alexis Battle, Raina Rajat and Andrew Y. Ng. In *NIPS 19*, 2007. [pdf](http://www.cs.stanford.edu/~ang/papers/nips06-sparsecoding.pdf)
* “[Sparse coding with an overcomplete basis set: A strategy employed by VI?.](http://www.chaos.gwdg.de/~michael/CNS_course_2004/papers_max/OlshausenField1997.pdf)” . Olshausen, Bruno A., and David J. Field. *Vision research* 37.23 (1997): 3311-3326.

## Foundation Theory and Motivation

* Hinton, Geoffrey E. “Deterministic Boltzmann learning performs steepest descent in weight-space.” *Neural computation* 1.1 (1989): 143-150.
* Bengio, Yoshua, and Samy Bengio. “Modeling high-dimensional discrete data with multi-layer neural networks.” *Advances in Neural Information Processing Systems* 12 (2000): 400-406.
* Bengio, Yoshua, et al. “Greedy layer-wise training of deep networks.” *Advances in neural information processing systems* 19 (2007): 153.
* Bengio, Yoshua, Martin Monperrus, and Hugo Larochelle. “Nonlocal estimation of manifold structure.” *Neural Computation* 18.10 (2006): 2509-2528.
* Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. “Reducing the dimensionality of data with neural networks.” *Science* 313.5786 (2006): 504-507.
* Marc’Aurelio Ranzato, Y., Lan Boureau, and Yann LeCun. “Sparse feature learning for deep belief networks.” *Advances in neural information processing systems* 20 (2007): 1185-1192.
* Bengio, Yoshua, and Yann LeCun. “Scaling learning algorithms towards AI.” *Large-Scale Kernel Machines* 34 (2007).
* Le Roux, Nicolas, and Yoshua Bengio. “Representational power of restricted boltzmann machines and deep belief networks.” *Neural Computation* 20.6 (2008): 1631-1649.
* Sutskever, Ilya, and Geoffrey Hinton. “Temporal-Kernel Recurrent Neural Networks.” *Neural Networks* 23.2 (2010): 239-243.
* Le Roux, Nicolas, and Yoshua Bengio. “Deep belief networks are compact universal approximators.” *Neural computation* 22.8 (2010): 2192-2207.
* Bengio, Yoshua, and Olivier Delalleau. “On the expressive power of deep architectures.” *Algorithmic Learning Theory*. Springer Berlin/Heidelberg, 2011.
* Montufar, Guido F., and Jason Morton. “When Does a Mixture of Products Contain a Product of Mixtures?.” *arXiv preprint arXiv:1206.0387* (2012).
* Montúfar, Guido, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. “[On the Number of Linear Regions of Deep Neural Networks](http://arxiv.org/pdf/1402.1869v2.pdf).” arXiv preprint arXiv:1402.1869 (2014).

## Supervised Feedfoward Neural Networks

* [The Manifold Tangent Classifier](http://books.nips.cc/papers/files/nips24/NIPS2011_1240.pdf), Salah Rifai, Yann Dauphin, Pascal Vincent, Yoshua Bengio and Xavier Muller, in: NIPS’2011.
* “[Discriminative Learning of Sum-Product Networks.](http://books.nips.cc/papers/files/nips25/NIPS2012_1484.pdf)“, Gens, Robert, and Pedro Domingos, NIPS 2012 Best Student Paper.
* Goodfellow, I., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013). [Maxout networks](http://arxiv.org/pdf/1302.4389). Technical Report, Universite de Montreal.
* Hinton, Geoffrey E., et al. “[Improving neural networks by preventing co-adaptation of feature detectors](http://arxiv.org/pdf/1207.0580).” *arXiv preprint arXiv:1207.0580* (2012).
* Wang, Sida, and Christopher Manning. “Fast dropout training.” In *Proceedings of the 30th International Conference on Machine Learning (ICML-13)*, pp. 118-126. 2013.
* Glorot, Xavier, Antoine Bordes, and Yoshua Bengio. “[Deep sparse rectifier networks](http://eprints.pascal-network.org/archive/00008596/01/glorot11a.pdf).” In *Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume*, vol. 15, pp. 315-323. 2011.
* [ImageNet Classification with Deep Convolutional Neural Networks](http://books.nips.cc/papers/files/nips25/NIPS2012_0534.pdf), Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, NIPS 2012.

## Large Scale Deep Learning

* [Building High-level Features Using Large Scale Unsupervised Learning](http://research.google.com/archive/unsupervised_icml2012.pdf) Quoc V. Le, Marc’Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeffrey Dean, and Andrew Y. Ng, ICML 2012.
* Bengio, Yoshua, et al. “[Neural probabilistic language models.](http://www.ai.mit.edu/projects/jmlr/papers/volume3/tmp/bengio03a.pdf)” *Innovations in Machine Learning* (2006): 137-186. Specifically Section 3 of this paper discusses the asynchronous SGD.
* Dean, Jeffrey, et al. [“Large scale distributed deep networks.”](http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks) *Advances in Neural Information Processing Systems*. 2012.

## Recurrent Networks

* [Training Recurrent Neural Networks](http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf), Ilya Sutskever, PhD Thesis, 2012.
* Bengio, Yoshua, Patrice Simard, and Paolo Frasconi. [“Learning long-term dependencies with gradient descent is difficult.”](http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf) *Neural Networks, IEEE Transactions on* 5.2 (1994): 157-166.
* Mikolov Tomáš: [Statistical Language Models based on Neural Networks](http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf). PhD thesis, Brno University of Technology, 2012.
* Hochreiter, Sepp, and Jürgen Schmidhuber.[ “Long short-term memory.”](http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf) *Neural computation* 9.8 (1997): 1735-1780.
* Hochreiter, S., Bengio, Y., Frasconi, P., & Schmidhuber, J. (2001). [Gradient flow in recurrent nets: the difficulty of learning long-term dependencies.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.24.7321&rep=rep1&type=pdf)
* Schmidhuber, J. (1992). [Learning complex, extended sequences using the principle of history compression. ](ftp://ftp.idsia.ch/pub/juergen/chunker.pdf)*Neural Computation*, *4*(2), 234-242.
* Graves, A., Fernández, S., Gomez, F., & Schmidhuber, J. (2006, June). [Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks](http://machinelearning.wustl.edu/mlpapers/paper_files/icml2006_GravesFGS06.pdf). In *Proceedings of the 23rd international conference on Machine learning* (pp. 369-376). ACM.

## Hyper Parameters

* [“Practical Bayesian Optimization of Machine Learning Algorithms”](http://books.nips.cc/papers/files/nips25/NIPS2012_1338.pdf), Jasper Snoek, Hugo Larochelle, Ryan Adams, NIPS 2012.
* [Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf), James Bergstra and Yoshua Bengio (2012), in: Journal of Machine Learning Research, 13(281–305).
* [Algorithms for Hyper-Parameter Optimization](http://books.nips.cc/papers/files/nips24/NIPS2011_1385.pdf), James Bergstra, Rémy Bardenet, Yoshua Bengio and Balázs Kégl, in: NIPS’2011, 2011.

## Optimization

* [Training Deep and Recurrent Neural Networks with Hessian-Free Optimization](http://www.cs.utoronto.ca/~ilya/pubs/2012/HF_for_dnns_and_rnns.pdf), James Martens and Ilya Sutskever, Neural Networks: Tricks of the Trade, 2012.
* Schaul, Tom, Sixin Zhang, and Yann LeCun. [“No More Pesky Learning Rates.” ](http://arxiv.org/pdf/1206.1106)*arXiv preprint arXiv:1206.1106* (2012).
* Le Roux, Nicolas, Pierre-Antoine Manzagol, and Yoshua Bengio. “[Topmoumoute online natural gradient algorithm](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.188.692&rep=rep1&type=pdf).” *Neural Information Processing Systems (NIPS)*. 2007.
* Bordes, Antoine, Léon Bottou, and Patrick Gallinari. “[SGD-QN: Careful quasi-Newton stochastic gradient descen](http://eprints.pascal-network.org/archive/00006505/01/bordes09a.pdf)t.” *The Journal of Machine Learning Research* 10 (2009): 1737-1754.
* Glorot, Xavier, and Yoshua Bengio. [“Understanding the difficulty of training deep feedforward neural networks.”](http://jmlr.csail.mit.edu/proceedings/papers/v9/glorot10a/glorot10a.pdf) *Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS’10). Society for Artificial Intelligence and Statistics*. 2010.
* Glorot, Xavier, Antoine Bordes, and Yoshua Bengio. “[Deep Sparse Rectifier Networks.](http://eprints.pascal-network.org/archive/00008596/01/glorot11a.pdf)” *Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume*. Vol. 15. 2011.
* [“Deep learning via Hessian-free optimization.” ](http://icml2010.haifa.il.ibm.com/papers/458.pdf)*Martens, James. Proceedings of the 27th International Conference on Machine Learning (ICML)*. Vol. 951. 2010.
* Hochreiter, Sepp, and Jürgen Schmidhuber. [“Flat minima.”](http://www.bioinf.jku.at/publications/older/3304.pdf) *Neural Computation,* 9.1 (1997): 1-42.
* Pascanu, Razvan, and Yoshua Bengio. “[Revisiting natural gradient for deep networks](http://arxiv.org/pdf/1301.3584).” *arXiv preprint arXiv:1301.3584* (2013).
* Dauphin, Yann N., Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. “[Identifying and attacking the saddle point problem in high-dimensional non-convex optimization.](http://arxiv.org/pdf/1406.2572.pdf)” In *Advances in Neural Information Processing Systems*, pp. 2933-2941. 2014.

## Unsupervised Feature Learning

* Salakhutdinov, Ruslan, and Geoffrey E. Hinton. [“Deep boltzmann machines.”](http://www.cs.utoronto.ca/~rsalakhu/papers/dbm.pdf) Proceedings of the international conference on artificial intelligence and statistics. Vol. 5. No. 2. Cambridge, MA: MIT Press, 2009.
* [Scholarpedia page](http://www.scholarpedia.org/article/Deep_belief_networks) on Deep Belief Networks.

### **Deep Boltzmann Machines**

* [An Efficient Learning Procedure for Deep Boltzmann Machines](http://www.utstat.toronto.edu/~rsalakhu/papers/neco_DBM.pdf), Ruslan Salakhutdinov and Geoffrey Hinton, Neural Computation August 2012, Vol. 24, No. 8: 1967 — 2006.
* Montavon, Grégoire, and Klaus-Robert Müller. “[Deep Boltzmann Machines and the Centering Trick.](http://gregoire.montavon.name/publications/montavon-lncs12.pdf)” *Neural Networks: Tricks of the Trade* (2012): 621-637.
* Salakhutdinov, Ruslan, and Hugo Larochelle. “[Efficient learning of deep boltzmann machines.](http://www.mit.edu/~rsalakhu/papers/dbmrec.pdf)” *International Conference on Artificial Intelligence and Statistics*. 2010.
* Salakhutdinov, Ruslan. *Learning deep generative models*. Diss. University of Toronto, 2009.
* Goodfellow, Ian, et al. “[Multi-prediction deep Boltzmann machines](http://papers.nips.cc/paper/5024-multi-prediction-deep-boltzmann-machines).” *Advances in Neural Information Processing Systems*. 2013.

### RBMs

* [Unsupervised Models of Images by Spike-and-Slab RBMs](http://www.iro.umontreal.ca/~lisa/pointeurs/ICML2011_SSRBM.pdf), Aaron Courville, James Bergstra and Yoshua Bengio, in: ICML’2011
* Hinton, Geoffrey. [“A practical guide to training restricted Boltzmann machines.](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf)” *Momentum* 9.1 (2010): 926.

### Autoencoders

* [Regularized Auto-Encoders Estimate Local Statistics](http://arxiv.org/abs/1211.4246), Guillaume Alain, Yoshua Bengio and Salah Rifai, Université de Montréal, arXiv report 1211.4246, 2012
* [A Generative Process for Sampling Contractive Auto-Encoders](http://icml.cc/2012/papers/910.pdf), Salah Rifai, Yoshua Bengio, Yann Dauphin and Pascal Vincent, in: ICML’2012, Edinburgh, Scotland, U.K., 2012
* [Contracting Auto-Encoders: Explicit invariance during feature extraction](http://www.iro.umontreal.ca/~lisa/pointeurs/ICML2011_explicit_invariance.pdf), Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot and Yoshua Bengio, in: ICML’2011
* [Disentangling factors of variation for facial expression recognition](http://www-etud.iro.umontreal.ca/~rifaisal/material/rifai_eccv_2012.pdf), Salah Rifai, Yoshua Bengio, Aaron Courville, Pascal Vincent and Mehdi Mirza, in: ECCV’2012.
* Vincent, Pascal, et al. “[Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.](http://jmlr.csail.mit.edu/papers/volume11/vincent10a/vincent10a.pdf)” *The Journal of Machine Learning Research* 11 (2010): 3371-3408.
* Vincent, Pascal. “[A connection between score matching and denoising autoencoders](http://www.iro.umontreal.ca/~vincentp/Publications/smdae_techreport.pdf).” *Neural computation* 23.7 (2011): 1661-1674.
* Chen, Minmin, et al. “[Marginalized denoising autoencoders for domain adaptation.](http://arxiv.org/pdf/1206.4683)” *arXiv preprint arXiv:1206.4683* (2012).

## Miscellaneous

* The ICML 2009 Workshop on Learning Feature Hierarchies [webpage](http://www.cs.toronto.edu/~rsalakhu/deeplearning/index.html) has a [reading list](http://www.cs.toronto.edu/~rsalakhu/deeplearning/references.html).
* Stanford’s [UFLDL Recommended Readings](http://ufldl.stanford.edu/wiki/index.php/UFLDL_Recommended_Readings).
* The [LISA](http://www.iro.umontreal.ca/~lisa)[public wiki](http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/WebHome) has a [reading list](http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/ReadingOnDeepNetworks) and a [bibliography](http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/DeepNetworksBibliography).
* [Geoff Hinton](http://www.cs.toronto.edu/~hinton) has [readings](http://www.cs.toronto.edu/~hinton/deeprefs.html) [NIPS 2007 tutorial](http://videolectures.net/jul09_hinton_deeplearn/).
* The LISA publications database contains a [deep architectures](http://www.iro.umontreal.ca/~lisa/publications2/index.php/topics/single/27) category.
* A very brief introduction to [AI, Machine Learning](http://www.iro.umontreal.ca/~pift6266/H10/notes/mlintro.html), and [Deep Learning](http://www.iro.umontreal.ca/~pift6266/H10/notes/deepintro.html) in [Yoshua Bengio](http://www.iro.umontreal.ca/~bengioy)‘s [IFT6266 graduate class](http://www.iro.umontreal.ca/~pift6266)
* Deep learning resources page, <http://www.jeremydjacksonphd.com/?cat=7>