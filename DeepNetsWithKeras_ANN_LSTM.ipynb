{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Nets with Keras\n",
    "\n",
    "In [another notebook](https://notebooks.azure.com/jrnorton11/libraries/NeuralNetworksAndDeepLearning/html/CNNsWithTensorFlowAndKeras.ipynb), we programmed up a Convolutional Neural Network (CNN) with TensorFlow and Keras.  Noting that one of the courses on Udemy that gets into Machine Learning only presents their Python code with Keras, I decided to code up as many Deep Nets as I can just to really get to know Keras much better.  To me, it seemed like there are some definite advantages to using Keras:\n",
    "* Simpler interface.  This is both conceptual and operational\n",
    "    * Conceptual - We don't have to deal with graphs and sessions and that seems nice.\n",
    "    * Operational - The interface is more terse which is sometimes problematic, but the output is far nicer.\n",
    "    * Disadvantage - I'm not sure how to show the net topology with Keras - that will be a point of investigation.\n",
    "* Faster - I don't know why, but Keras worked far faster for me than TensorFlow.\n",
    "* Customizable - Choose your [backend](https://keras.io/backend/).  You can choose to run on TensorFlow, Theano, or CNTK from Microsoft.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "# ANN - Artificial Neural Network\n",
    "\n",
    "The following network is an Artificial Neural Network which we will use for classification.  The original network is from Udemy, but we are going to work with the data from the University of Wisconsin breast cancer study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data set\n",
    "dataset = pd.read_csv(\"datasets/breast-cancer-wisconsin-data.csv\")\n",
    "\n",
    "# Isolate the $independent and dependent values.\n",
    "X = dataset.iloc[:, 2:32]\n",
    "y = dataset.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical data\n",
    "\n",
    "At this point, we should encode categorical data for independent variables.  This data set doesn't have any.  But the code is listed below for completeness.\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:]\n",
    "```\n",
    "We should encode y.\n",
    "\n",
    "### Hot Encoding\n",
    "\n",
    "OK - what is \"hot encoding\"?  You should know this in case it comes up in a job interview question like it did with me.  Let's start with the one-dimensional categorical variable, True and False.  Note that True and False are not numbers - doh!  We need numbers.  As is normally done, we assign True as one and False as zero.\n",
    "\n",
    "But what happens if we have more than two categorical variables?  The wrong answer is to assign values \\[0, 0.5, 1\\] for three, \\[0, 0.33, 0.66, 1\\] for four, etc.  This artificially assigns importance - we are essential turning categorical values into continuous values (as the number of categories approaches infinity).  We note that in some cases, this approach may be ok - e.g., cold, warm, hot.  But in general it is not.\n",
    "\n",
    "But what if the labels are independent - that is, there is no ordinal relationship.  Consider \"dog\" and \"cat\".  Rather than take the first approach (an index), hot encoding approaches the problem as a probabalistic solution.  Let's create a probability table for \"dog\" and \"cat\" where what the animal actually is in the rows and the assign probability is in the columns:\n",
    "\n",
    "|     | Dog | Cat |\n",
    "| --- | --- | --- |\n",
    "| Dog |  1  |  0  |\n",
    "| Cat |  0  |  1  |\n",
    "\n",
    "\n",
    "*In these cases, we would like to give the network more expressive power to learn a probability-like number for each possible label value. This can help in both making the problem easier for the network to model. When a one hot encoding is used for the output variable, it may offer a more nuanced set of predictions than a single label.* [Brownlee](https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/)\n",
    "\n",
    "Hot Encoding, instead of assigning a single value to a categorical variable, we assign a vector.  For example, for RGB, we can assign R = \\[1 0 0\\], G = \\[0 1 0\\], and B = \\[0 0 1\\]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data\n",
    "\n",
    "Split the data into testing and training sets.  We use the same ratio as before because we are going to characterize the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rn\n",
    "\n",
    "# The below is necessary in Python 3.2.3 onwards to\n",
    "# have reproducible behavior for certain hash-based operations.\n",
    "# See these references for further details:\n",
    "# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n",
    "# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units = 23, activation = 'relu', input_dim = X_train.shape[1], kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 16, activation = 'relu', kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keras Model Visualization**\n",
    "\n",
    "This is how you visualize a network in Keras - that was an unanswered question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(classifier, to_file=\"tmp/ann.png\", show_shapes=True, show_layer_names=True)\n",
    "\n",
    "import time\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tmp/ann.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder_y = LabelEncoder()\n",
    "y_train = labelencoder_y.fit_transform(y_train)\n",
    "y_test = labelencoder_y.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3 - Making the predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "# Also convert y_test\n",
    "y_test = (y_test > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix results are pretty impressive and as we see below, the area under the curve (AUC) is also very good (0.96).  However, this wasn't any better than what we got with our logistic regression model.  I won't pass judgement yet however on any of these approaches.  A number of models should be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "\n",
    "y_testv = pd.Series(y_test.flatten())\n",
    "y_predv = pd.Series(y_pred.flatten())\n",
    "y_testv = np.asarray(y_testv.replace('B', 0).replace('M', 1))\n",
    "y_predv = np.asarray(y_predv.replace('B', 0).replace('M', 1))\n",
    "false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_testv, y_predv)\n",
    "roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b',\n",
    "label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Recurrent neural networks, or RNNs ([Rumelhart](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf)), are a family of neural networks which can process sequential data represented as a sequence of values $x^{(1)}, ... , x^{(\\tau)}$. Recurrent networks can scale to much longer sequences than would be practical for networks without sequence-based specialization. Most recurrent networks can also process sequences of variable length.\n",
    "\n",
    "To go from multilayer networks to recurrent networks, RNNs share parameters across diﬀerent parts of the model. Parameter sharing makes it possible to extend and apply the model to examples of diﬀerent forms (diﬀerent lengths, here) and generalize across them. If there is a separate parameter for each value of the time index, it would not be possible to generalize to sequence lengths not seen during training, nor share statistical strength across diﬀerent sequence lengths and across diﬀerent positions in time. Such sharing is particularly important when a speciﬁc piece of information can occur at multiple positions within the sequence.\n",
    "\n",
    "Suppose we train a feedforward network that processes sentences of ﬁxed length. A traditional fully connected feedforward network would have separate parameters for each input feature - it needs to learn all the rules of the language separately at each position in the sentence. By comparison, a recurrent neural network shares the same weights across several time steps. \n",
    "In recurrent networks, each member of the output is a function of the previous members of the output. Each member of the output is produced using the same update rule applied to the previous outputs. This recurrent formulation results in the sharing of parameters through a very deep computational graph ([Adopted from the Deep Learning Book.](http://www.deeplearningbook.org/contents/rnn.html)). Sharing parameters in this fashion gives RNNs “memory” which captures information about what has been calculated so far. In theory RNNs can make use of information in arbitrarily long sequences, but in practice they are limited to looking back only a few steps.\n",
    "\n",
    "## Unfolding a Graph\n",
    "\n",
    "![](http://www.wildml.com/wp-content/uploads/2015/09/rnn.jpg)\n",
    "\n",
    "Unfold a recursive or recurrent computation into a computational graph that has a repetitive structure, typically corresponding to a chain of events.  Unfolding this graph results in the sharing of parameters across a deep network structure. By unfolding or unrolling we simply mean that we write out the network for the complete sequence. For example, if the sequence we care about is a sentence of 5 words, the network would be unrolled into a 5-layer neural network, one layer for each word. The formulas that govern the computation happening in a RNN are as follows:\n",
    "\n",
    "* $x_t$ is the input at time step $t$. For example, $x_1$ could be a one-hot vector corresponding to the second word of a sentence.  \n",
    "* $s_t$ is the hidden state at time step $t$. It’s the “memory” of the network. $s_t$ is calculated based on the previous hidden state and the input at the current step: $s_t=f(Ux_t + Ws_{t-1})$. The function f usually is a nonlinearity such as tanh or ReLU.  $s_{-1}$, which is required to calculate the first hidden state, is typically initialized to all zeroes.  \n",
    "* $o_t$ is the output at step $t$. For example, if we wanted to predict the next word in a sentence it would be a vector of probabilities across our vocabulary. $o_t = \\mathrm{softmax}(Vs_t)$.\n",
    "\n",
    "Note:\n",
    "\n",
    "* You can think of the hidden state $s_t$ as the memory of the network. $s_t$ captures information about what happened in all the previous time steps. The output at step $o_t$ is calculated solely based on the memory at time $t$. As briefly mentioned above, it’s a bit more complicated  in practice because $s_t$ typically can’t capture information from too many time steps ago.  \n",
    "* Unlike a traditional deep neural network, which uses different parameters at each layer, a RNN shares the same parameters ($U$, $V$, $W$ above) across all steps. This reflects the fact that we are performing the same task at each step, just with different inputs. This greatly reduces the total number of parameters we need to learn.\n",
    "\n",
    "* The above diagram has outputs at each time step, but depending on the task this may not be necessary. For example, when predicting the sentiment of a sentence we may only care about the final output, not the sentiment after each word. Similarly, we may not need inputs at each time step. The main feature of an RNN is its hidden state, which captures some information about a sequence.\n",
    "\n",
    "## What can RNNs do?\n",
    "\n",
    "* Language Modeling and Generating Texts  \n",
    "* Language Translation .\n",
    "* Speech Recognition\n",
    "* Generating Image Descriptions\n",
    "\n",
    "\n",
    "See http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/.\n",
    "\n",
    "\n",
    "\n",
    "### More References\n",
    "**Recurrent Neural Networks**\n",
    "\n",
    "Brilliant.  Recurrent Neural Network.  https://brilliant.org/wiki/recurrent-neural-network/  \n",
    "Goodfellow et al.  Deep Learning Book.  http://www.deeplearningbook.org/contents/rnn.html  \n",
    "Andrej Karpathy.  The Unreasonable Effectiveness of Recurrent Neural Networks.  http://karpathy.github.io/2015/05/21/rnn-effectiveness/  \n",
    "Victor Schmidt.  Keras recurrent tutorial.  https://github.com/Vict0rSch/deep_learning/tree/master/keras/recurrent  \n",
    "Dataset - https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption#  \n",
    "\n",
    "**Keras**\n",
    "Francois Chollet.  Keras Resources.  https://github.com/fchollet/keras-resources.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "I really like Jason Brownlee's blogs.  Let's start with his [blog on RNNs](https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/) and some other material that I found in trying to come to a better understanding of LSTMs.  LSTMs are a form of an RNN - or perhaps better said, a network of LSTMs is a type of RNN.\n",
    "\n",
    "LSTM stands for 'long short-term model'.  It is widely used because the architecture overcomes the vanishing and exposing gradient problem that plagues all recurrent neural networks, allowing very large and very deep networks to be created.\n",
    "\n",
    "Like other recurrent neural networks, LSTM networks maintain state. Let's determine exactly how state is maintained in LSTM networks by the Keras deep learning library.\n",
    "\n",
    "### Understanding the Vanishing Gradient\n",
    "\n",
    "For reference, use the paper by [Pascanu et al](http://proceedings.mlr.press/v28/pascanu13.pdf).  The following is the summary of [this statsexchange blog](https://stats.stackexchange.com/questions/185639/how-does-lstm-prevent-the-vanishing-gradient-problem).\n",
    "\n",
    "In one-dimension, assume we have a hidden state $h_t$ at time step $t$. To simplify, remove biases and inputs.  Then for the output from a node or neuron, we have\n",
    "\n",
    "$h_t = \\sigma(w h_{t-1})$\n",
    "\n",
    "It can be shown ([see around page 12 of this thesis](https://d-nb.info/1082034037/34)) that\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\frac{\\partial h_{t'}}{\\partial h_t} \n",
    "&= \\prod_{k=1}^{t' - t} w \\sigma'(w h_{t'-k})\\\\\n",
    "&= \\underbrace{w^{t' - t}}_{!!!}\\prod_{k=1}^{t' - t} \\sigma'(w h_{t'-k})\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Note that the term marked with the exclamation marks is the critical term.  If the term is less than one, then the gradient decays to zero and if the term is greater than one, then the gradient will grow exponentially fast.\n",
    "\n",
    "In LSTMs, you have the cell state $s_t$. The derivative there is of the form\n",
    "\n",
    "$\\frac{\\partial s_{t'}}{\\partial s_t} = \\prod_{k=1}^{t' - t} \\sigma(v_{t+k})$\n",
    "\n",
    "where $v_t$ is the input to the forget gate. There is no exponentially fast decaying factor involved. Consequently, there is at least one path where the gradient does not vanish. See the [thesis](https://d-nb.info/1082034037/34) for the complete derivation.\n",
    "\n",
    "### Cell States\n",
    "\n",
    "According to [Colah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), cell states are the key to understanding LSTMs. In particular, cell states are like a conveyor belt.  In the following picture of a node,\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png\" alt=\"LSTM node\" style=\"width:800px;\"/>\n",
    "\n",
    "the horizontal line is the \"conveyor belt\".  It is easy for the information to flow through unchanged.  It runs through the entire network with minor linear interactions.\n",
    "\n",
    "Information can be removed or added from the LSTM, carefully regulated by structures called gates.\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png\" alt=\"LSTM gate\" style=\"width:180px;\"/>\n",
    "\n",
    "Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.  Recall that the sigmoid is:\n",
    "\n",
    "$S(x) = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "and looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fca974be208>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJzt3Xt8FPW9//HXh4SLGAoCSoUg0YJ3KxIK3hDSYkWrqBUvqHj5FTnHVqyXora2SrWeorW11mstWLHHGjn1ctCiYDWIVUEuotyCBwWEgggKKGKAkM/vj5mkS8hlN2R2dpP38/HYx+7sfGfmvcOyn8ztO+buiIiIALSIO4CIiGQOFQUREamioiAiIlVUFEREpIqKgoiIVFFREBGRKioKktHM7CIzm5ZpyzWz6WY2Mp2ZUmFmA8xsadw5JPuoKEjszOxEM3vTzDab2Wdm9oaZfQvA3Z9w9++mO9OeLNfMxprZDjPbkvC4obEzVlumm1nPymF3f93dD4lymdI05cYdQJo3M/sa8AJwJTAJaAUMALbFmasRPOXuF8cdQiRV2lKQuB0M4O5PuvtOd//K3ae5+3sAZnaZmf2zsrGZfdfMloZbFQ+a2WuVu3HCtm+Y2T1mtsnMPjSz48P3V5nZJ2Z2acK82pvZ42a23sxWmtnPzaxFLcs92cxKw+XeD1hDPqyZrTCzwQnDY83sv8PXBeFf/Jea2UdmtsHMbk5om2NmPzOzD8zsCzOba2bdzWxG2OTdcKvkfDMbZGarE6Y9LNzltcnMFpnZ0IRxj5nZA2b293C+s8zsGw35fJL9VBQkbu8DO81sopmdamb71NbQzDoDfwN+CnQClgLHV2vWH3gvHP9XoBj4FtATuBi438zywrb3Ae2Bg4CBwCXA5bUs92ng50Bn4APghIZ82CSdCBwCfAe4xcwOC9+/DhgOnAZ8Dfh/wFZ3Pykcf7S757n7U9XytwSeB6YB+wGjgSfMLHH30nDgl8A+wDLgjig+mGQ+FQWJlbt/TvAj6MCfgPVmNtnMutTQ/DRgkbs/4+7lwB+Aj6u1We7uf3b3ncBTQHfgNnff5u7TgO1ATzPLAc4HfuruX7j7CuC3wIhalrvY3f/m7juA39ew3OrOC/8qr3x0rX9tVPlluMX0LvAucHT4/kjg5+6+1APvuvunSczvWCAPGOfu2939VYJddsMT2jzj7m+H6/UJoHcKeaUJUVGQ2Ln7Ene/zN3zgSOBrgQ/vNV1BVYlTOfA6mpt1iW8/ipsV/29PIK/+FsBKxPGrQS6JbncVTW0SzTJ3TskPNbU0z5RYsHZGuaFoMB9kMJ8KnUFVrl7RcJ71T9rbcuUZkZFQTKKu5cCjxEUh+rWAvmVA2ZmicMp2gDsAHokvHcA8K9altu92nK719AuGV8CbROGv57CtKuAhuzrXwN0rzxeEqrts0ozp6IgsTKzQ83sejPLD4e7E+zWmFlD878DR5nZWWaWC/yI1H5Uq4S7lyYBd5hZOzPrQbDP/r9rWe4RZvb9cLlXN3S5wHzgAjNraWZ9gWEpTDseuN3Melngm2bWKRy3juDYSE1mERSjG8LlDgLOIDjeIrILFQWJ2xcEB4dnmdmXBMVgIXB99YbuvgE4F7gL+BQ4HJhDw09fHU3wY/kh8E+CA9OP1rHcceFyewFvNHCZvyD4a38jwYHdv6Yw7e8ICtk04HNgArBXOG4sMDE8fnFetfzbgaHAqQRbSA8Cl4RbZSK7MN1kR7JVuDtkNXCRu5fEnUekKdCWgmQVMzvFzDqYWWvgZwTXC9S0q0lEGkBFQbLNcQRn4Gwg2C9+lrt/FW8kkaZDu49ERKSKthRERKRK1nWI17lzZy8oKGjQtF9++SV777134wZqBMqVGuVKXaZmU67U7EmuuXPnbnD3fett6O5Z9SgsLPSGKikpafC0UVKu1ChX6jI1m3KlZk9yAXM8id9Y7T4SEZEqKgoiIlJFRUFERKqoKIiISBUVBRERqRJZUTCzR8PbHy6sZbyZ2R/MbJmZvWdmfaLKIiIiyYlyS+ExYEgd408l6G2yFzAKeCjCLCIikoTILl5z9xlmVlBHkzOBx8PzZ2eGnZzt7+5ro8okIrvbvh02bQoeX30FZWWwbdvuz9u3Q0VFcg/3XYdrs3x5D2bMqD9jMr3x1NcmlXmsWFHAq6/W3z7dunZtx6BB0S4j0r6PwqLwgrvvdhctM3uB4J6x/wyHXwFudPc5NbQdRbA1QZcuXQqLixt2b5AtW7aQl5d5dxlUrtQoV/LKy41Vq9qyZEkOmze3Z926Nqxf35pPPmnNxo2t2LIll23bcuKOmVHMMrc/uCuvXMC5537WoGmLiormunvf+trF2c2F1fBejf8a7v4I8AhA3759fVADS+X06dNp6LRRUq7UKFfttm2D11+HqVNh+nRYsCB4r1KHDtC9OxxyCHz967DPPsF7HTpA+/bQti20aQOtW+/+3KoV5ORAixZ1P8x2H7aa/rfz73VW2/hEjdUmiblkxL9lTaZP/yzyXHEWhdXsep/bfIJ7yYpICtzh7bfhoYfgf/4Htm4NfsCPPx5Gj4bevWHbttmcd963yLANGXJygodkjjiLwmTgKjMrJrgd42YdTxBJnjs8/TT8+tcwbx7k5cHFF8PQoTBoECT2mzZ9+pcZVxAkM0VWFMzsSWAQ0NnMVgO3Ai0B3P1hYApwGrAM2ApcHlUWkaZm8WK44gp480047LBgK+Gii6Bdu7iTSbaL8uyj4fWMd+BHUS1fpClyh/vugzFjggIwYQJceql2wUjjybr7KYg0Vzt2BFsHEyfCGWfA+PGw335xp5KmRt1ciGSBsjI455ygIIwdC889p4Ig0dCWgkiG27kThg+H55+HBx+EK6+MO5E0ZdpSEMlwY8YEWwa//70KgkRPRUEkgxUXwz33wNVXw49/HHcaaQ5UFEQy1IoV8J//CccdB7/9bdxppLlQURDJQO4wcmTQmdwTT0Cujv5JmuirJpKBJk2CV16B+++HAw+MO400J9pSEMkwW7bAdddBnz7B7iORdNKWgkiGue8+WLMm6NxOVypLumlLQSSDfP45/OY38L3vBb2ciqSbioJIBrn3Xti4EX75y7iTSHOloiCSIcrKgl1Hp58OhYVxp5HmSkVBJEMUF8P69XDNNXEnkeZMRUEkA7gHu46OOAK+/e2400hzprOPRDLAW2/B/Pnwxz821n2GRRpGWwoiGWDiRGjbNugNVSROKgoiMSsrg6eeCu6XoNtpStxUFERiNnkybN4Ml1wSdxIRFQWR2P3lL5CfD0VFcSfJbldeeSXdunXDdFBmj6goiMTo889h2jQ47zx1abGnhg8fzrx58+KOkfV09pFIjF58EbZvh7PPjjtJ9jvppJPijtAkaEtBJEbPPAP77RfcSCed3J2jjz6aiRMnpjTdj370I37wgx9ElEoygYqCSEzKymDKFDjrrPTvOpo0aRIbN27kwgsvTGm6MWPG8MQTT7Bs2bKIkkncVBREYlJSEtw7IY5dR3/4wx8YMWIELVu2TGm6goICTjzxRB566KGIkkncVBREYvLSS7DXXjBoUOPO9+OPP+bSSy+lS5cutGjRAjOjqKgIM6OwsJBly5bx5ptvMmzYsF2me+211zAzXnzxxar3li9fzn777cfVV19d9d4555zDE088QUVFReMGl4ygoiASk6lTYeBAaNOm8eZZVlbG4MGDmTFjBnfddRfPP/88AwYMAGDUqFGMGTOGV155hb333pujjz56l2kHDhxIUVERt99+OwCbN2/m9NNPp1+/ftxzzz1V7Y4//njWrVvHggULaszg7pSXl9f7aGwjR44kPz8fgPz8fEaOHNnoy2gOdPaRSAxWroSlSxv/dpu/+tWvWLVqFYsXL6Zbt24AHHroofTs2ZMTTzyRCy64gFGjRnHYYYfRosXufxP+8pe/5KSTTmLatGn89re/pWXLlhQXF5OTcNDjiCOOICcnh7fffnu3wgIwceJELr/88nqzuvsefNLdjR8/vlHn11ypKIjEYOrU4PmUUxp3vk888QRXXHFFVUEAOOigg2jRogWbNm0Cgt1LnTt3rnH6AQMGMHjwYM4++2w6dOjArFmzyMvL26VNbm4uHTp04OOPP65xHmeccQazZ89upE8k6aaiIBKDqVODq5gPPbTx5llaWsqKFSsYPHjwLu+vX7+eiooK9t9/fyDYxdS2bdta59OzZ0/+8Y9/cO+991btjqmudevWlJWV1TiuY8eOtG/fvoGfQuKmYwoiaVZRAdOnw8knN2432atXrwZgv/322+X9qVOnkpuby8knnwwEP9qVWw3VPfLIIzz66KMcffTRde6O2bRpEx07dqxx3MSJE2nZsmW9j+rMLGMelQfmKx/NSaRbCmY2BLgXyAHGu/u4auMPACYCHcI2N7n7lCgzicRt0SL47LPgIHNj6tChAwBLly6lT58+QLBV8Ktf/YqioqKqv94POeQQ3nrrrd2mf/nll7nqqqsYP348Bx98MMcddxwvvvgip5566i7t1q9fz9atWzn44INrzNHQ3UeNfYxhT0yfPp1BjX1aWJaIrCiYWQ7wAHAysBqYbWaT3X1xQrOfA5Pc/SEzOxyYAhRElUkkE8yYETw3dq8MvXv35qCDDuLGG28kNzcXM+POO++krKyM0aNHV7U74YQTuO2221i/fj377rsvAIsWLeLcc8/lhhtu4JKwu9bBgwdz66237lYU5syZg5lx/PHH15ijU6dOdOrUqUGf4corr2Ty5MmsWbMmliKxadMmLr/8cq644gouuugi7rnnnt22vJq6KHcf9QOWufuH7r4dKAbOrNbGga+Fr9sDayLMI5IRZsyA7t2hoKBx55ubm8vkyZPp0aMHI0aM4Morr+TII49k5syZtEu4UcOgQYPo2LEjL730EgCffPIJp59+OieffHLV6agAv/jFL5g9ezZ///vfd1nOSy+9xMCBAxv8w1+XVDu1K2jgSiwtLWXIkCG7PF5++WU6dOjAgAEDuPXWWxk3blyzKwgAFlU1NrNhwBB3HxkOjwD6u/tVCW32B6YB+wB7A4PdfW4N8xoFjALo0qVLYXFxcYMybdmyZbczKTKBcqUmm3O5w7Bhx9GnzyZuvnlJmpLtnu2+++7jX//6F+PGjatjqt3t3Lmz6rTWymMUjZmrUlFRESUlJfVOf8EFF1DX78Fnn33GAw88wNq1a9m6dSuXX345A+vYb7d9+3buuusuNm/ezLhx43Y5FTcT7Ml3v6ioaK679623obtH8gDOJTiOUDk8ArivWpvrgOvD18cBi4EWdc23sLDQG6qkpKTB00ZJuVKTzbmWLnUH9z/+Mfo8iapnW7Vqlbdt29aXLl2a0nyefPJJ79mzp+/YsSOSXJWCn6b69ejRo9Zx5eXl/p3vfMfnzZvn7u7r1q3zbt261dp+x44dftlll/nixYt97Nix/uCDDyaVIZ325LsPzPEkfrujPNC8GuieMJzP7ruHfgAMIfgGvGVmbYDOwCcR5hKJzT//GTyHFxnHJj8/nwkTJrB27dpaDxjXxN2ZMGECubnxnc0+dOhQPvroIwDWrFlD7969ATj22GN5+OGHq9pNmTKFd999d5cL6eo6FTc3N5c///nPQHB1tw40N77ZQC8zOxD4F3ABUL1Lxo+A7wCPmdlhQBtgfYSZRGI1axZ06ACHHBJ3kmDXS6qGDx8eQZLUTJ48uep1QUEB8+fPr7Hde++9xw033MCYMWPSFa1JiOxAs7uXA1cBU4ElBGcZLTKz28xsaNjseuAKM3sXeBK4LNzMEWmSZs6E/v2hhh4mpJF17dqVl156ie3btwOwdu1a1q1bF3OqzBfpV9Pdp7j7we7+DXe/I3zvFnefHL5e7O4nuPvR7t7b3adFmUckTlu2wMKFcOyxcSfJXI3Zqd3FF19Mfn4+hx9+OL179+biiy9urJhNmrq5EEmTOXOCq5n79487SeZKtVO7FStW1DquZcuWKd9ZTtTNhUjazJwZPPfrF28OkbqoKIikycyZ0KsXRHDNl0ijUVEQSZO339auI8l8KgoiabB2bfDoW//1pCKxUlEQSYN33gmew85LRTKWioJIGsydG9w7Ibz4ViRjqSiIpMG8eXDwwZDQWalIRlJREEmDefO060iyg4qCSMQ2bICPPlJRkOygoiASMR1klmyioiASscpOPHWQWbKBioJIxBYsgG7doGPHuJOI1E9FQSRiCxbAUUfFnUIkOSoKIhEqL4clS+DII+NOIpIcFQWRCC1bBtu2aUtBsoeKgkiEFi4MnrWlINlCRUEkQgsWBLfePOywuJOIJEdFQSRCCxYE91DYa6+4k4gkR0VBJEILF2rXkWQXFQWRiGzdGhxo1kFmySYqCiIRWbwY3LWlINlFRUEkIpVnHmlLQbKJioJIRBYsgDZt4BvfiDuJSPJUFEQisnAhHH445OTEnUQkeSoKIhFRn0eSjVQURCLw6aewdq0OMkv2UVEQiYAOMku2UlEQiYD6PJJspaIgEoHFi+FrX4OuXeNOIpKaSIuCmQ0xs6VmtszMbqqlzXlmttjMFpnZX6PMI5IupaVBJ3hmcScRSU1uVDM2sxzgAeBkYDUw28wmu/vihDa9gJ8CJ7j7RjPbL6o8Ium0ZAl897txpxBJXZRbCv2AZe7+obtvB4qBM6u1uQJ4wN03Arj7JxHmEUmLzZuDM4/UXbZkI3P3aGZsNgwY4u4jw+ERQH93vyqhzXPA+8AJQA4w1t1fqmFeo4BRAF26dCksLi5uUKYtW7aQl5fXoGmjpFypyfRcS5a044c/LOT22xdw4omfxh0LyPx1lmmaYq6ioqK57t633obuHskDOBcYnzA8ArivWpsXgGeBlsCBBLuZOtQ138LCQm+okpKSBk8bJeVKTabneuwxd3BfujTePIkyfZ1lmqaYC5jjSfx2R7n7aDXQPWE4H1hTQ5v/dfcd7r4cWAr0ijCTSORKS6FlSzjooLiTiKQuyqIwG+hlZgeaWSvgAmBytTbPAUUAZtYZOBj4MMJMIpFbsiS421puZKdxiEQnsqLg7uXAVcBUYAkwyd0XmdltZjY0bDYV+NTMFgMlwBh3z4ydsCINVFoKhx4adwqRhon0bxl3nwJMqfbeLQmvHbgufIhkve3bg7utnXNO3ElEGkZXNIs0og8+gJ07dTqqZK96i4KZfcPMWoevB5nZ1WbWIfpoItmntDR41u4jyVbJbCk8Dew0s57ABIJTR9UdhUgNliwJnlUUJFslUxQqwoPGZwO/d/drgf2jjSWSnUpLIT8fMvC6J5GkJFMUdpjZcOBSgovNILjYTESqWbJExxMkuyVTFC4HjgPucPflZnYg8N/RxhLJPu46HVWyX72npHrQq+nVCcPLgXFRhhLJRhs2tGbLFm0pSHartSiY2SR3P8/MFgC79Zrn7t+MNJlIlvnoo7aAthQku9W1pfDj8Pn0dAQRyXaVRUFbCpLNai0K7r42fLm3J9wYB4LrFYCVEeYSyTorV7alfXvo0iXuJCINl8yB5klmdqMF9jKz+4BfRx1MJNt89FFbDj1Ut+CU7JZMUehP0AX2mwQ9n64huCmOiCRYtaqtjidI1kvqOgXgK2AvoA2w3N0rIk0lkmU+/zw4+0hFQbJdMkVhNkFR+BZwIjDczP4WaSqRLLN0afCsg8yS7ZLpOvsH7j4nfP0xcGZ4v2URCakjPGkq6t1SSCgImNneZnYRwV3URCRUWgo5ORW6BadkvWS6zm5lZmeZ2SRgLTAYeDjyZCJZZMkS6NbtK1qqVzDJcnVd0XwyMBw4heBWmX8B+rn75WnKJpI1SkvhgAO2AnvHHUVkj9S1pTAV+AZwortf7O7PAzrrSKSaHTuCW3AGRUEku9V1oLmQ4NjBP8zsQ6AYyElLKpEssnx5UBi6d1dRkOxX65aCu7/j7je6+zeAscAxQCsze9HMRqUroEimqzzzSFsK0hQkc50C7v6Gu18FdAN+T3B/BRFBRUGalmSuU6gSXsk8NXyICEFR2H9/yMvbGXcUkT2W1JaCiNROd1uTpqTWomBmU8ysIH1RRLKPe3CNgoqCNBV1bSk8Bkwzs5vNTJfkiNTgk09g0yYVBWk66rrJziQz+ztwCzDHzP5CwnUK7v67NOQTyWjq80iamvqOKewAvgRaA+2qPUSaPRUFaWrq6uZiCPA7YDLQx911vp1INaWl0LYt5OfDhx/GnUZkz9V1SurNwLnuvihdYUSyTWkpHHIItNB5fNJE1HVMYUA6g4hko9JSOE6XckoTEunfN2Y2xMyWmtkyM7upjnbDzMzNrG+UeUQa09atsHKl7rYmTUtkRcHMcoAHgFOBwwlu43l4De3aAVcDs6LKIhKF998PrlPQQWZpSqLcUugHLHP3D919O0Evq2fW0O524C6gLMIsIo1OZx5JU2TuHs2MzYYBQ9x9ZDg8AugfdqxX2eYY4Ofufo6ZTQd+knj7z4R2o4BRAF26dCksLi5uUKYtW7aQl5fXoGmjpFypyZRcjz1WwOOP9+Cll16nVauKjMlVk0zNplyp2ZNcRUVFc929/l307h7JAzgXGJ8wPAK4L2G4BTAdKAiHpwN965tvYWGhN1RJSUmDp42ScqUmU3Kdf777QQf9ezhTctUkU7MpV2r2JBcwx5P47Y5y99FqoHvCcD6wJmG4HXAkMN3MVgDHApN1sFmyhTrCk6YoyqIwG+hlZgeaWSuCu7hNrhzp7pvdvbO7F7h7ATATGOo17D4SyTQVFbB0qYqCND2RFQV3LweuIrj3whJgkrsvMrPbzGxoVMsVSYcVK6CsTEVBmp6UbrKTKnefAkyp9t4ttbQdFGUWkca0cGHwfNRR8eYQaWy6OF+kARYsCJ6POCLeHCKNTUVBpAEWLoSCAmin/oKliVFREGmABQvgyCPjTiHS+FQURFK0fXtw5pGOJ0hTpKIgkqKlS6G8XEVBmiYVBZEUVR5k1u4jaYpUFERStHAh5OYGN9cRaWpUFERStGBBcNFaq1ZxJxFpfCoKIilauFC7jqTpUlEQScEXXwRdXOggszRVKgoiKVi0KHjWloI0VSoKIimoPPNIWwrSVKkoiKRgwQLIy4MePeJOIhINFQWRFCxcGHSC10L/c6SJ0ldbJEnuwZaCdh1JU6aiIJKkTz6BDRtUFKRpU1EQSdI77wTP3/xmvDlEoqSiIJKkefOC52OOiTeHSJRUFESSNG8e9OwJ7dvHnUQkOioKIkmaNw/69Ik7hUi0VBREkvDZZ7B8uYqCNH0qCiJJqDzIXFgYbw6RqKkoiCRBB5mluVBREEnC3LlwwAHQqVPcSUSipaIgkoRZs6Bfv7hTiERPRUGkHuvWBfdQOPbYuJOIRE9FQaQes2YFz/37x5tDJB1UFETqMXMm5ObqdFRpHlQUROoxa1bQ31HbtnEnEYmeioJIHXbuhLff1vEEaT4iLQpmNsTMlprZMjO7qYbx15nZYjN7z8xeMTPdz0oyyuLFsGWLjidI8xFZUTCzHOAB4FTgcGC4mR1erdk7QF93/ybwN+CuqPKINMTrrwfPJ5wQbw6RdIlyS6EfsMzdP3T37UAxcGZiA3cvcfet4eBMID/CPCIpmzEDunWDgw6KO4lIepi7RzNjs2HAEHcfGQ6PAPq7+1W1tL8f+Njdf1XDuFHAKIAuXboUFhcXNyjTli1byMvLa9C0UVKu1KQrlzsMG3YcvXtv4he/WJIxuRoiU7MpV2r2JFdRUdFcd+9bb0N3j+QBnAuMTxgeAdxXS9uLCbYUWtc338LCQm+okpKSBk8bJeVKTbpyvf++O7g//HBy7TN1fblnbjblSs2e5ALmeBK/3bkNKjnJWQ10TxjOB9ZUb2Rmg4GbgYHuvi3CPCIpmTEjeD7ppHhziKRTlMcUZgO9zOxAM2sFXABMTmxgZscAfwSGuvsnEWYRSdlrr8G++8Khh8adRCR9IisK7l4OXAVMBZYAk9x9kZndZmZDw2a/AfKA/zGz+WY2uZbZiaSVO7zyCgwcCGZxpxFJnyh3H+HuU4Ap1d67JeH14CiXL9JQixbBmjVwyilxJxFJL13RLFKDqVODZxUFaW5UFERqMHUqHHYYdO9ef1uRpkRFQaSarVuDM4+0lSDNkYqCSDWvvQbbtqkoSPOkoiBSzbPPQl4eDBoUdxKR9FNREEmwcyc89xycdhq0aRN3GpH0U1EQSfDmm7B+PXz/+3EnEYmHioJIgmefhVat4NRT404iEg8VBZFQRQX87W9w8snwta/FnUYkHioKIqGSEli1Ci6+OO4kIvFRURAJPf54sIVw5pn1txVpqlQURAjuw/z003DeebDXXnGnEYmPioII8NRT8OWXcOmlcScRiZeKgjR77nDffXDkkXDCCXGnEYlXpF1ni2SDGTPg3XfhT3/SvRNEtKUgzd6990LHjnDhhXEnEYmfioI0awsWBBesXXkltG0bdxqR+KkoSLM2dmxwGup118WdRCQzqChIszVvHjzzDFx7bbD7SERUFKSZqqiAq6+Gzp3hmmviTiOSOXT2kTRLjz8Ob7wBEyZAhw5xpxHJHCoK0uysXQtjxsBxx8Fll8WdRpKxY8cOVq9eTVlZWVqW1759e5YsWZKWZaUimVxt2rQhPz+fli1bNmgZKgrSrFRUBFctf/llsJXQQjtQs8Lq1atp164dBQUFWBouJvniiy9o165d5MtJVX253J1PP/2U1atXc+CBBzZoGfovIc3KHXfAyy/D738Phx0WdxpJVllZGZ06dUpLQchmZkanTp32aItKRUGajb/+FW65Jega+4or4k4jqVJBSM6ericVBWkWnnkmOH5w0kkwfry6sxCpjYqCNHmPPhp0id23L/zv/0Lr1nEnkmx0xx13cMQRR/DNb36T3r17M2vWLEaOHMnixYsjXe5pp53Gpk2bdnt/7Nix3H333Y2+PB1olibrq6/gJz+BBx+EwYODrYUMPHYoWeCtt97ihRdeYN68ebRu3ZoNGzawfft2xo8fH/myp0yZEvkyEqkoSJNUUgL/+Z/w/vtw/fUwbhzk6tveJFxzDcyf37jz7N07OPmgNmvXrqVz5860DjczO3fuDMCgQYO4++676du3LxMmTODOO++ka9ddHj4qAAALxElEQVSu9OrVi9atW3P//fdz2WWXsddee1FaWsrKlSv585//zMSJE3nrrbfo378/jz32GABPPvkk//Vf/4W7873vfY8777wTgIKCAubMmUPnzp35zW9+w1NPPUX37t3Zd999KSwsbNwVgXYfSRPiDq+/DqedBt/+NmzfDv/4B9x9twqC7Jnvfve7rFq1ioMPPpgf/vCHvPbaa7uMX7NmDbfffjszZ87k5ZdfprS0dJfxGzdu5NVXX+Wee+7hjDPO4Nprr2XRokUsWLCA+fPns2bNGm688UZeffVV5s+fz+zZs3nuued2mcfcuXN5+umneeedd3jmmWeYPXt2JJ9V/1Ukq7kHWwMvvBBcpfzee9CpE9x1F4weDW3axJ1QGltdf9FHJS8vj7lz5/L6669TUlLC+eefz7hx46rGv/322wwcOJCOYSda5557Lu+//37V+DPOOAMz46ijjqJLly4cddRRABxxxBGsWLGClStXMmjQIPbdd18ALrroImbMmMFZZ51VNY/XX3+d008/nbZhd75Dhw6N5LNGWhTMbAhwL5ADjHf3cdXGtwYeBwqBT4Hz3X1FlJkku331FZSWtuP//g9mz4Zp02DlymDct74FDz0El1yibrCl8eXk5DBo0CAGDRrEUUcdxcSJE6vGuXud01budmrRokXV68rh8vJycpPclE3HabmRFQUzywEeAE4GVgOzzWyyuyceqv8BsNHde5rZBcCdwPlRZZL4VVQEu3W2bdv98dVXsGnTvx8bNwZdUqxaFTw++gg+/hjcg/2o7dtDURHcdBOccgo08AJOkXotXbqUFi1a0KtXLwDmz59Pjx49WLhwIQD9+vXj2muvZePGjbRr146nn366amsgGf379+fHP/4xGzZsYJ999uHJJ59k9OjRu7Q56aSTuOSSS7j11lspLy/n+eef5z/+4z8a70OGotxS6Acsc/cPAcysGDgTSCwKZwJjw9d/A+43M/P6ym4DPPoo3Hbbt6r+gkxcQvWlpXtcWdmxu5wmmSnZduw4fpd98XVNV98yysuDH/7yclKy115wwAHQvTsMGQI9eoDZQkaMOJKCAl1vIOmxZcsWRo8ezaZNm8jNzaVnz5488sgjDBs2DIBu3brxs5/9jP79+9O1a1cOP/xw2rdvn/T8999/f379619TVFSEu3Paaadx5pln7tKmT58+fP/736d379706NGDAQMGNOpnrGQR/P4GMzYbBgxx95Hh8Aigv7tfldBmYdhmdTj8QdhmQ7V5jQJGAXTp0qWwuLg45TxvvNGJF1/stMtmWuIPSvUfF7Pa10tjT7djxw5ataq9Pu++jNSX15DpduzYQcuWLZOebvdl7Dpdq1YVtGzptGxZUfVo1SoYzs2toHXrCvLyymnXrpy8vHL23ructm137vY5tmzZQl5eXp054pCpuSBzsyWbq3379vTs2TMNiQI7d+4kJycnpWkqP0t5eTkXXnghI0aM4Iwzzogl17Jly9i8efMu7xUVFc119771TuzukTyAcwmOI1QOjwDuq9ZmEZCfMPwB0Kmu+RYWFnpDlZSUNHjaKClXapQrdZmaLdlcixcvjjZINZ9//nnK01x//fV+9NFH+yGHHOKjR4/2ioqK2HLVtL6AOZ7Eb3eUu49WA90ThvOBNbW0WW1muUB74LMIM4mIRCKKq4vjEOV1CrOBXmZ2oJm1Ai4AJldrMxm4NHw9DHg1rGgiIrvQT0Ny9nQ9RVYU3L0cuAqYCiwBJrn7IjO7zcwqT7CdAHQys2XAdcBNUeURkezVpk0bPv30UxWGenh4P4U2e3CBTqTXKbj7FGBKtfduSXhdRnDsQUSkVvn5+axevZr169enZXllZWV79MMalWRyVd55raF0RbOIZLyWLVs2+E5iDTF9+nSOOeaYtC0vWenIpb6PRESkioqCiIhUUVEQEZEqkV3RHBUzWw+sbODknYEN9bZKP+VKjXKlLlOzKVdq9iRXD3fft75GWVcU9oSZzfFkLvNOM+VKjXKlLlOzKVdq0pFLu49ERKSKioKIiFRpbkXhkbgD1EK5UqNcqcvUbMqVmshzNatjCiIiUrfmtqUgIiJ1UFEQEZEqzaIomNlvzKzUzN4zs2fNrEPCuJ+a2TIzW2pmp6Q517lmtsjMKsysb8L7BWb2lZnNDx8PZ0KucFxs66tajrFm9q+EdXRaXFnCPEPCdbLMzDKmt18zW2FmC8J1NCfGHI+a2Sfh3RYr3+toZi+b2f+Fz/tkSK7Yv1tm1t3MSsxsSfh/8cfh+9Gvs2TuxJPtD+C7QG74+k7gzvD14cC7QGvgQII7v+WkMddhwCHAdKBvwvsFwMIY11dtuWJdX9UyjgV+Evd3K8ySE66Lg4BW4To6PO5cYbYVQOcMyHES0Cfxew3cBdwUvr6p8v9lBuSK/bsF7A/0CV+3A94P//9Fvs6axZaCu0/z4P4OADMJ7gIHcCZQ7O7b3H05sAzol8ZcS9x9abqWl6w6csW6vjJYP2CZu3/o7tuBYoJ1JSF3n8Hud1U8E5gYvp4InJXWUNSaK3buvtbd54WvvyC4J0030rDOmkVRqOb/AS+Gr7sBqxLGrQ7fywQHmtk7ZvaamQ2IO0wo09bXVeEuwUfj2PWQINPWSyIHppnZXDMbFXeYarq4+1oIfgSB/WLOkyhTvluYWQFwDDCLNKyzJnM/BTP7B/D1Gkbd7O7/G7a5GSgHnqicrIb2jXqObjK5arAWOMDdPzWzQuA5MzvC3T+POVfk62uXhdWREXgIuD1c/u3AbwkKfhzSul5SdIK7rzGz/YCXzaw0/OtYapcx3y0zywOeBq5x98/NavqqNa4mUxTcfXBd483sUuB04Dse7pAj+Iuue0KzfGBNOnPVMs02YFv4eq6ZfQAcDDTagcKG5CIN6ytRshnN7E/AC1HlSEJa10sq3H1N+PyJmT1LsKsrU4rCOjPb393Xmtn+wCdxBwJw93WVr+P8bplZS4KC8IS7PxO+Hfk6axa7j8xsCHAjMNTdtyaMmgxcYGatzexAoBfwdhwZE5nZvmaWE74+iCDXh/GmAjJofYX/ISqdDSysrW0azAZ6mdmBZtYKuIBgXcXKzPY2s3aVrwlOuIhzPVU3Gbg0fH0pUNsWalplwnfLgk2CCcASd/9dwqjo11mcR9jTeCR/GcE+3/nh4+GEcTcTnDmyFDg1zbnOJvgrcxuwDpgavn8OsIjgLJZ5wBmZkCvu9VUt41+ABcB74X+U/WP+jp1GcIbIBwS74GLLkpDpoPA79G74fYotF/AkwW7RHeF36wdAJ+AV4P/C544Zkiv27xZwIsHuq/cSfrdOS8c6UzcXIiJSpVnsPhIRkeSoKIiISBUVBRERqaKiICIiVVQURESkioqCSIKwd8rlZtYxHN4nHO5RS/uzzczN7NAk5t3XzP7Q2JlFGpNOSRWpxsxuAHq6+ygz+yOwwt1/XUvbSQQ9Wr7i7mPTGFMkEtpSENndPcCxZnYNwUVEv62pUdgvzQkEFzxdkPD+2Wb2Dwvsb2bvm9nXzWyQmb0QthmY0F//O5VXHovETUVBpBp33wGMISgO13jQHXZNzgJecvf3gc/MrE84/bPAx8CPgD8Bt7r7x9Wm/QnwI3fvDQwAvmr8TyKSOhUFkZqdStD9wZF1tBlOcO8EwufhCeNGAz8Ftrn7kzVM+wbwOzO7Gujg/77fh0ismkwvqSKNxcx6AycDxwL/NLNiD/uwT2jTCfg2cKSZOcHd19zMbvDgQF03oALoYmYt3L0icXp3H2dmfyfoz2ammQ1299LoP51I3bSlIJIg7J3yIYLdRh8BvwHurqHpMOBxd+/h7gXu3h1YDpxoZrnAn4ELCe6YdV0Ny/mGuy9w9zsJukSv9+wlkXRQURDZ1RXAR+7+cjj8IHComQ2s1m448Gy1954mKAQ/A15399cJCsJIMzusWttrzGyhmb1LcDzhRUQygE5JFRGRKtpSEBGRKioKIiJSRUVBRESqqCiIiEgVFQUREamioiAiIlVUFEREpMr/Bx92x53bXeFzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fca958552e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "x=np.linspace(-20,20,200)\n",
    "plt.plot(x,sigmoid(x),'b', label='Sigmoid')\n",
    "plt.grid()\n",
    "plt.xlabel('X Axis')\n",
    "plt.ylabel('Y Axis')\n",
    "plt.title('Sigmoid Function')\n",
    "plt.text(4,0.8,r'$\\sigma(x)=\\frac{1}{1+e^{-x}}$',fontsize=15)\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero filters all the signal, one filters none of the signal.\n",
    "\n",
    "The forget LSTM (from [1997](http://www.bioinf.jku.at/publications/older/2604.pdf)) has three of these gates, to protect and control the cell state: the input, forget, and output gate.\n",
    "\n",
    "<img src=\"images/LSTM_97.PNG\" style=\"width:600px;\"\\>\n",
    "\n",
    "In more detail, the $j^{th}$ *memory cell* in an LSTM is denoted $c_j$. The memory cell is built around a *CEC* or Constant Error Carrousel.  The CEC (extracted from the picture above below):\n",
    "\n",
    "<img src=\"images/CEC_1997.PNG\" style=\"width=100px;\"\\>\n",
    "\n",
    "does not suffer from decaying or exploding gradients because the multiplier is 1.0.\n",
    "\n",
    "$S_c$ is the current state of the memory cell, and $g y^{in}$ is the current input to it. The triple arrows on the left show where information flows into the cell at multiple points.  That combination of present input ($x_t$) and past cell state ($h_{t-1}$) is fed not only to the cell itself, but also to each of its three gates, which will decide how the input will be handled.  That decision is determined by the cell state $S_{c-1}$, e.g., 0 - ignore, 1 - accept.\n",
    "\n",
    "The gates determine whether to let new input in, erase the present cell state, and/or let that state impact the network’s output at the present time step.  For example, once the input comes in, the sigmoid layer called the \"forget gate layer\" decides what to throw away.  Each gate can be open or shut, and they will recombine their open and shut states at each step. The cell can forget its state, or not; be written to, or not; and be read from, or not, at each time step, and those flows are represented here.\n",
    "\n",
    "## LSTM Problem - Learn the Alphabet\n",
    "\n",
    "This one comes from [Jason Brownlee](https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/).  We will compare a number of different LSTM approaches; the context of these comparisons will be a simple sequence prediction problem of learning the alphabet. That is, given a letter of the alphabet, predict the next letter of the alphabet.\n",
    "\n",
    "This is a simple sequence prediction problem that once understood can be generalized to other sequence prediction problems like time series prediction and sequence classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks model numbers - map the letters of the alphabet to integer values. This can be done by creating a dictionary (map) of the letter index to the character. Also create a reverse lookup for converting predictions back into characters to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 1\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "\tseq_in = alphabet[i:i + seq_length]\n",
    "\tseq_out = alphabet[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "\tprint(seq_in, '->', seq_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to reshape the NumPy array into a format expected by the LSTM networks, that is \\[samples, time steps, features\\]. Once reshaped, normalize the input integers to the range 0-to-1, the range of the sigmoid activation functions used by the LSTM network.\n",
    "\n",
    "Consider this problem as a sequence classification task, where each of the 26 letters represents a different class. As such, we can convert the output (y) to a *one hot encoding*, using the Keras built-in function to_categorical().  Recall that the one hot encode will give us 26 vectors of 26 numbers which in this case look like the identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the alphabet properly encoded, we are prepared to compare LSTM models.\n",
    "\n",
    "### Naive LSTM for Learning One-Char to One-Char Mapping\n",
    "\n",
    "Design a simple LSTM to learn how to predict the next character in the alphabet given the context of just one character.\n",
    "Frame the problem as a random collection of one-letter input to one-letter output pairs. It turns out that this is a difficult framing of the problem for the LSTM to learn.\n",
    "\n",
    "Define an LSTM network with 32 units and an output layer with a softmax activation function for making predictions. Because this is a multi-class classification problem, we can use the log loss function (called “categorical_crossentropy” in Keras), and optimize the network using the ADAM optimization function.\n",
    "\n",
    "The model is fit over 500 epochs with a batch size of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the performance on the entire training dataset.\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run the training data through the network and generate predictions, converting both the input and output pairs back into their original character format to get a visual idea of how well the network learned the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tprint(seq_in, \"->\", result)\n",
    "\n",
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tprint(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this problem difficult for the LSTM to learn?\n",
    "\n",
    "The reason is that the LSTM units do not have any context to work with. Each input-output pattern is shown to the network in a random order and the state of the network is reset after each pattern (each batch where each batch contains one pattern).\n",
    "\n",
    "This is abuse of the LSTM network architecture, treating it like a standard multilayer Perceptron.  Try a different framing of the problem in order to provide more sequence to the network from which to learn.\n",
    "\n",
    "### Naive LSTM for a Three-Char Feature Window to One-Char Mapping\n",
    "A popular approach to adding more context to data for multilayer Perceptrons is to use the window method.\n",
    "The window method uses previous steps in the sequence to provide additional input features to the network. \n",
    "Here, we increase the sequence length from 1 to 3, for example.\n",
    "\n",
    "ABC => D  \n",
    "BCD => E  \n",
    "etc.\n",
    "\n",
    "Each element in the sequence is then provided as a new input feature to the network. This requires a modification of how the input sequences reshaped in the data preparation step:\n",
    "It also requires a modification for how the sample patterns are reshaped when demonstrating predictions from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Naive LSTM to learn three-char window to one-char mapping\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 3\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "\tseq_in = alphabet[i:i + seq_length]\n",
    "\tseq_out = alphabet[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "\tprint(seq_in, '->', seq_out)\n",
    "    \n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (len(dataX), 1, seq_length))\n",
    "\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# create and fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "\tx = np.reshape(pattern, (1, 1, len(pattern)))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tprint(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also a misuse of the LSTM network by a poor framing of the problem. The sequences of letters are time steps of one feature rather than one time step of separate features. There is more context to the network, but not more sequence as it expected.\n",
    "\n",
    "### Naive LSTM for a Three-Char Time Step Window to One-Char Mapping\n",
    "\n",
    "In Keras, the intende  The difference is that the reshaping of the input data takes the sequence as a time step sequence of one feature, rather than a single time step of multiple features, use of LSTMs is to provide context in the form of time steps, rather than windowed features like with other network types.\n",
    "\n",
    "We can take our first example and simply change the sequence length from 1 to 3.  The difference is that the reshaping of the input data takes the sequence as a time step sequence of one feature, rather than a single time step of multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive LSTM to learn three-char time steps to one-char mapping\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 3\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "\tseq_in = alphabet[i:i + seq_length]\n",
    "\tseq_out = alphabet[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "\tprint(seq_in, '->', seq_out)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-family:courier-new; font-size:14px;\">\n",
    "*# reshape X to be [samples, time steps, features]*  \n",
    "*# In the previous example, we used this code which is setting a window...*  \n",
    "*# X = np.reshape(dataX, (len(dataX), 1, seq_length))*  \n",
    "*#*  \n",
    "*# In this example, we reshape the input data with a time step sequence*  \n",
    "*# of one feature, rather than a single time step of multiple features.*  \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# create and fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tprint(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model learns the problem perfectly as evidenced by the model evaluation and the example predictions.\n",
    "It has learned a simpler problem - it has learned to predict the next letter from a sequence of three letters in the alphabet. It can be shown any random sequence of three letters from the alphabet and predict the next letter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can not enumerate the alphabet. Brownlee writes that he expects a larger enough multilayer perception network might be able to learn the same mapping using the window method.\n",
    "\n",
    "LSTM networks are stateful. They should be able to learn the whole alphabet sequence, but *by default the Keras implementation resets the network state after each training batch*.\n",
    "\n",
    "### LSTM State Within A Batch\n",
    "The Keras implementation of LSTMs resets the state of the network after each batch.\n",
    "This suggests that if we had a batch size large enough to hold all input patterns and if all the input patterns were ordered sequentially, that the LSTM could use the context of the sequence within the batch to better learn the sequence.\n",
    "\n",
    "We can demonstrate this easily by modifying the first example for learning a one-to-one mapping and increasing the batch size from 1 to the size of the training dataset.\n",
    "Additionally, Keras shuffles the training dataset before each training epoch. To ensure the training data patterns remain sequential, we can disable this shuffling.\n",
    "\n",
    "The network will learn the mapping of characters using the the within-batch sequence, but this context will not be available to the network when making predictions. We can evaluate both the ability of the network to make predictions randomly and in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive LSTM to learn one-char to one-char mapping with all data in each batch\n",
    "import numpy as np\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 1\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "\tseq_in = alphabet[i:i + seq_length]\n",
    "\tseq_out = alphabet[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "\tprint(seq_in, '->', seq_out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of lists to array and pad sequences if needed\n",
    "X = pad_sequences(dataX, maxlen=seq_length, dtype='float32')\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (X.shape[0], seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# create and fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(16, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Do not shuffle and set the batch size to be from 1 to the size of the training set.\n",
    "model.fit(X, y, epochs=5000, batch_size=len(dataX), verbose=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tprint(seq_in, \"->\", result)\n",
    "# demonstrate predicting random patterns\n",
    "print(\"Test a Random Pattern:\")\n",
    "for i in range(0,20):\n",
    "\tpattern_index = np.random.randint(len(dataX))\n",
    "\tpattern = dataX[pattern_index]\n",
    "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tprint(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is able to use the within-sequence context to learn the alphabet, achieving 100% accuracy on the training data.\n",
    "The network can make accurate predictions for the next letter in the alphabet for randomly selected characters.\n",
    "\n",
    "### Stateful LSTM for a One-Char to One-Char Mapping\n",
    "Raw data can be broken into fixed size sequences and this representation can be learned by the LSTM, but only to learn random mappings of 3 characters to 1 character.  However, we can vary batch size to offer more sequences to the network, but only during training.\n",
    "\n",
    "Ideally, we want to expose the network to the entire sequence and let it learn the inter-dependencies, rather than us define those dependencies explicitly in the framing of the problem.\n",
    "This can be done in Keras by making the LSTM layers stateful and manually resetting the state of the network at the end of the epoch, which is also the end of the training sequence.\n",
    "This is truly how the LSTM networks are intended to be used. By allowing the network itself to learn the dependencies between the characters, that we need a smaller network (half the number of units) and fewer training epochs (almost half).\n",
    "\n",
    "First, define our LSTM layer as stateful. Explicitly specify the batch size as a dimension on the input shape. This also means that the network is evaluated or used to make predictions, it is necessary to specify and adhere to this same batch size. This is not a problem now since a batch size of one is used. This could introduce difficulties when making predictions when the batch size is not one as predictions will need to be made in batch and in sequence.\n",
    "\n",
    "```\n",
    "batch_size = 1\n",
    "model.add(LSTM(16, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "```\n",
    "\n",
    "**Note**\n",
    "\n",
    "I have difficulty reproducing the results from Brownlee.  I do know that he was using an older version of code, but it is curious.  The approach used played with several factors in the formulation of the LSTM as well as the parameters on the optimizer.  Those are oulined in detail below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Stateful LSTM to learn one-char to one-char mapping\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "import math\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 1\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "\tseq_in = alphabet[i:i + seq_length]\n",
    "\tseq_out = alphabet[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# create and fit the model\n",
    "batch_size = 1\n",
    "stateful = True\n",
    "number_epochs = 400\n",
    "\n",
    "# Note - these two variables are used in formulation of the LSTM.  By default, they are\n",
    "# activation='tanh' and inner_activation=None.  However, these seem to work much better.\n",
    "activation = 'sigmoid'\n",
    "inner_activation = 'tanh'\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(16, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=stateful,\n",
    "              activation=activation, recurrent_activation=inner_activation))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Rate lr**\n",
    "\n",
    "The Adam optimizer offers parameters which drive the effectiveness, especially the learning rate and the decay rate.  The higher the learning the rate, the faster the optimizer converges - *until it diverges*.  Decay rate decays the learning rate on each iteration.  Adjustment of the two values allows for \"fast learning\" in the early stages and slower learning later which helps avoid divergence.\n",
    "\n",
    "These were done with a decay of 0.001.  But that seems too high once we settle on a good learning rate.  Intuitively, that makes sense.\n",
    "\n",
    "At lr = 0.002, convergence is around 300 epochs.  Note the divergent behavior after 300 epochs.  The best result is given at epoch 399 of 400 epochs.\n",
    "\n",
    "![](images/lstm_malmae_002.png)\n",
    "\n",
    "lr = 0.003 is too high of a learning rate - it is difficult to determine if the optimization converges.\n",
    "\n",
    "![](images/lstm_malmae_003.png)\n",
    "\n",
    "The learning rate lr = 0.0025 seems to give good convergence properties, until we hit divergence at about epoch 365.\n",
    "\n",
    "![](images/lstm_malmae_0025.png)\n",
    "\n",
    "Set lr = 0.0025.  Leave epochs at 400 as the code is written to pick out when the solution falls into the \"convergence well\" even though it is bouncing all around it at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.2772 - acc: 0.0400 - mean_absolute_error: 0.0740\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.2551 - acc: 0.0800 - mean_absolute_error: 0.0740\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.2463 - acc: 0.0800 - mean_absolute_error: 0.0739\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.2366 - acc: 0.1200 - mean_absolute_error: 0.0739\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.2238 - acc: 0.1200 - mean_absolute_error: 0.0739\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.2047 - acc: 0.0800 - mean_absolute_error: 0.0738\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.1704 - acc: 0.0800 - mean_absolute_error: 0.0737\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.1043 - acc: 0.0800 - mean_absolute_error: 0.0734\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.0318 - acc: 0.0800 - mean_absolute_error: 0.0729\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.9720 - acc: 0.1600 - mean_absolute_error: 0.0727\n",
      "Iteration 10\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.9269 - acc: 0.0800 - mean_absolute_error: 0.0725\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.9305 - acc: 0.0800 - mean_absolute_error: 0.0724\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.9433 - acc: 0.1200 - mean_absolute_error: 0.0724\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.8794 - acc: 0.1200 - mean_absolute_error: 0.0720\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.8506 - acc: 0.0800 - mean_absolute_error: 0.0718\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.8231 - acc: 0.0800 - mean_absolute_error: 0.0716\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.7850 - acc: 0.1200 - mean_absolute_error: 0.0714\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.7329 - acc: 0.0800 - mean_absolute_error: 0.0711\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.6762 - acc: 0.0400 - mean_absolute_error: 0.0709\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.6184 - acc: 0.0400 - mean_absolute_error: 0.0707\n",
      "Iteration 20\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.5701 - acc: 0.0400 - mean_absolute_error: 0.0705\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.5121 - acc: 0.0400 - mean_absolute_error: 0.0702\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.4801 - acc: 0.0400 - mean_absolute_error: 0.0701\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.4190 - acc: 0.0800 - mean_absolute_error: 0.0697\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.3831 - acc: 0.1200 - mean_absolute_error: 0.0695\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.3392 - acc: 0.1200 - mean_absolute_error: 0.0692\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.3007 - acc: 0.1600 - mean_absolute_error: 0.0689\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.2630 - acc: 0.2400 - mean_absolute_error: 0.0686\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.2241 - acc: 0.3200 - mean_absolute_error: 0.0683\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.1880 - acc: 0.3600 - mean_absolute_error: 0.0680\n",
      "Iteration 30\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.1554 - acc: 0.4400 - mean_absolute_error: 0.0677\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.1202 - acc: 0.3200 - mean_absolute_error: 0.0673\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.0962 - acc: 0.3200 - mean_absolute_error: 0.0671\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.0592 - acc: 0.3600 - mean_absolute_error: 0.0667\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.0424 - acc: 0.4000 - mean_absolute_error: 0.0665\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.0005 - acc: 0.4400 - mean_absolute_error: 0.0661\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.9831 - acc: 0.4800 - mean_absolute_error: 0.0659\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.9447 - acc: 0.4400 - mean_absolute_error: 0.0655\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.9235 - acc: 0.4400 - mean_absolute_error: 0.0653\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.8933 - acc: 0.4400 - mean_absolute_error: 0.0649\n",
      "Iteration 40\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.8693 - acc: 0.4400 - mean_absolute_error: 0.0646\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.8447 - acc: 0.4400 - mean_absolute_error: 0.0643\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.8209 - acc: 0.4400 - mean_absolute_error: 0.0640\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.8014 - acc: 0.4000 - mean_absolute_error: 0.0638\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.7801 - acc: 0.4000 - mean_absolute_error: 0.0635\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.7605 - acc: 0.4000 - mean_absolute_error: 0.0632\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.7397 - acc: 0.4000 - mean_absolute_error: 0.0629\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.7232 - acc: 0.4000 - mean_absolute_error: 0.0627\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.7027 - acc: 0.4000 - mean_absolute_error: 0.0624\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.6849 - acc: 0.4000 - mean_absolute_error: 0.0621\n",
      "Iteration 50\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.6671 - acc: 0.4400 - mean_absolute_error: 0.0619\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.6498 - acc: 0.4400 - mean_absolute_error: 0.0616\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.6338 - acc: 0.4800 - mean_absolute_error: 0.0613\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.6173 - acc: 0.4800 - mean_absolute_error: 0.0611\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.6026 - acc: 0.4800 - mean_absolute_error: 0.0608\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.5868 - acc: 0.5200 - mean_absolute_error: 0.0606\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.5725 - acc: 0.5600 - mean_absolute_error: 0.0603\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.5575 - acc: 0.5600 - mean_absolute_error: 0.0601\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.5419 - acc: 0.6000 - mean_absolute_error: 0.0598\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.5277 - acc: 0.6800 - mean_absolute_error: 0.0596\n",
      "Iteration 60\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.5143 - acc: 0.6800 - mean_absolute_error: 0.0593\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.5015 - acc: 0.6800 - mean_absolute_error: 0.0591\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.4886 - acc: 0.6800 - mean_absolute_error: 0.0589\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.4762 - acc: 0.6800 - mean_absolute_error: 0.0587\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.4643 - acc: 0.7200 - mean_absolute_error: 0.0584\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.4520 - acc: 0.7200 - mean_absolute_error: 0.0582\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.4403 - acc: 0.7200 - mean_absolute_error: 0.0580\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.4283 - acc: 0.7200 - mean_absolute_error: 0.0578\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.4211 - acc: 0.7600 - mean_absolute_error: 0.0577\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.4052 - acc: 0.7600 - mean_absolute_error: 0.0573\n",
      "Iteration 70\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3992 - acc: 0.7600 - mean_absolute_error: 0.0572\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3857 - acc: 0.7600 - mean_absolute_error: 0.0570\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3765 - acc: 0.7600 - mean_absolute_error: 0.0568\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3667 - acc: 0.7600 - mean_absolute_error: 0.0566\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3573 - acc: 0.7600 - mean_absolute_error: 0.0564\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3464 - acc: 0.7600 - mean_absolute_error: 0.0562\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3401 - acc: 0.7600 - mean_absolute_error: 0.0560\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3291 - acc: 0.7600 - mean_absolute_error: 0.0558\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3204 - acc: 0.7600 - mean_absolute_error: 0.0556\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3090 - acc: 0.7600 - mean_absolute_error: 0.0554\n",
      "Iteration 80\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.2954 - acc: 0.8000 - mean_absolute_error: 0.0551\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3003 - acc: 0.8000 - mean_absolute_error: 0.0552\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.2806 - acc: 0.8000 - mean_absolute_error: 0.0548\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.2741 - acc: 0.7600 - mean_absolute_error: 0.0546\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.2676 - acc: 0.7600 - mean_absolute_error: 0.0545\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.2565 - acc: 0.7600 - mean_absolute_error: 0.0543\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.2522 - acc: 0.7600 - mean_absolute_error: 0.0542\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.2423 - acc: 0.7600 - mean_absolute_error: 0.0539\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.2351 - acc: 0.7600 - mean_absolute_error: 0.0538\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.2292 - acc: 0.7600 - mean_absolute_error: 0.0536\n",
      "Iteration 90\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.2200 - acc: 0.7600 - mean_absolute_error: 0.0534\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.2121 - acc: 0.7600 - mean_absolute_error: 0.0532\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.2046 - acc: 0.7600 - mean_absolute_error: 0.0530\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1973 - acc: 0.7600 - mean_absolute_error: 0.0529\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1887 - acc: 0.7600 - mean_absolute_error: 0.0527\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1820 - acc: 0.7600 - mean_absolute_error: 0.0525\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1758 - acc: 0.7600 - mean_absolute_error: 0.0523\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1696 - acc: 0.7600 - mean_absolute_error: 0.0522\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1636 - acc: 0.7600 - mean_absolute_error: 0.0520\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1575 - acc: 0.7600 - mean_absolute_error: 0.0519\n",
      "Iteration 100\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1523 - acc: 0.7600 - mean_absolute_error: 0.0518\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1449 - acc: 0.8000 - mean_absolute_error: 0.0516\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1395 - acc: 0.8000 - mean_absolute_error: 0.0514\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1343 - acc: 0.8400 - mean_absolute_error: 0.0513\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1292 - acc: 0.8400 - mean_absolute_error: 0.0512\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1241 - acc: 0.8400 - mean_absolute_error: 0.0510\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1191 - acc: 0.8400 - mean_absolute_error: 0.0509\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1143 - acc: 0.8400 - mean_absolute_error: 0.0508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 0s - loss: 1.1095 - acc: 0.8400 - mean_absolute_error: 0.0506\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1048 - acc: 0.8400 - mean_absolute_error: 0.0505\n",
      "Iteration 110\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0996 - acc: 0.8400 - mean_absolute_error: 0.0504\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0934 - acc: 0.8400 - mean_absolute_error: 0.0502\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0897 - acc: 0.8400 - mean_absolute_error: 0.0501\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0853 - acc: 0.8400 - mean_absolute_error: 0.0500\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0811 - acc: 0.8400 - mean_absolute_error: 0.0499\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0761 - acc: 0.8400 - mean_absolute_error: 0.0498\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0727 - acc: 0.8400 - mean_absolute_error: 0.0497\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0647 - acc: 0.8400 - mean_absolute_error: 0.0494\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0578 - acc: 0.8000 - mean_absolute_error: 0.0492\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0518 - acc: 0.8000 - mean_absolute_error: 0.0491\n",
      "Iteration 120\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0467 - acc: 0.8000 - mean_absolute_error: 0.0489\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0419 - acc: 0.8000 - mean_absolute_error: 0.0488\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0366 - acc: 0.8400 - mean_absolute_error: 0.0487\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0309 - acc: 0.8400 - mean_absolute_error: 0.0485\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0275 - acc: 0.8400 - mean_absolute_error: 0.0484\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0239 - acc: 0.8400 - mean_absolute_error: 0.0483\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0209 - acc: 0.8400 - mean_absolute_error: 0.0483\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0186 - acc: 0.8000 - mean_absolute_error: 0.0482\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0175 - acc: 0.8000 - mean_absolute_error: 0.0481\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0157 - acc: 0.8000 - mean_absolute_error: 0.0481\n",
      "Iteration 130\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0146 - acc: 0.8000 - mean_absolute_error: 0.0481\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0126 - acc: 0.8000 - mean_absolute_error: 0.0480\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0098 - acc: 0.8000 - mean_absolute_error: 0.0479\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0049 - acc: 0.8000 - mean_absolute_error: 0.0478\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0007 - acc: 0.8000 - mean_absolute_error: 0.0477\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9965 - acc: 0.8400 - mean_absolute_error: 0.0475\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9913 - acc: 0.8400 - mean_absolute_error: 0.0474\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9864 - acc: 0.8400 - mean_absolute_error: 0.0473\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9813 - acc: 0.8800 - mean_absolute_error: 0.0471\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9765 - acc: 0.8800 - mean_absolute_error: 0.0470\n",
      "Iteration 140\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9719 - acc: 0.8800 - mean_absolute_error: 0.0468\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9676 - acc: 0.8800 - mean_absolute_error: 0.0467\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9637 - acc: 0.8800 - mean_absolute_error: 0.0466\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9602 - acc: 0.8800 - mean_absolute_error: 0.0465\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9572 - acc: 0.8800 - mean_absolute_error: 0.0464\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9544 - acc: 0.8800 - mean_absolute_error: 0.0464\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9518 - acc: 0.8800 - mean_absolute_error: 0.0463\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9493 - acc: 0.8800 - mean_absolute_error: 0.0462\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9471 - acc: 0.8800 - mean_absolute_error: 0.0462\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9450 - acc: 0.8800 - mean_absolute_error: 0.0461\n",
      "Iteration 150\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9428 - acc: 0.8800 - mean_absolute_error: 0.0460\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9407 - acc: 0.8800 - mean_absolute_error: 0.0460\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9386 - acc: 0.8800 - mean_absolute_error: 0.0459\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9364 - acc: 0.8800 - mean_absolute_error: 0.0459\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9337 - acc: 0.8800 - mean_absolute_error: 0.0458\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9305 - acc: 0.8800 - mean_absolute_error: 0.0457\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9267 - acc: 0.8800 - mean_absolute_error: 0.0456\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9223 - acc: 0.8800 - mean_absolute_error: 0.0455\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9177 - acc: 0.8800 - mean_absolute_error: 0.0453\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9131 - acc: 0.8800 - mean_absolute_error: 0.0452\n",
      "Iteration 160\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9086 - acc: 0.8800 - mean_absolute_error: 0.0451\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9042 - acc: 0.8800 - mean_absolute_error: 0.0449\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8994 - acc: 0.9200 - mean_absolute_error: 0.0448\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8952 - acc: 0.9200 - mean_absolute_error: 0.0446\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8911 - acc: 0.9200 - mean_absolute_error: 0.0445\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8871 - acc: 0.9200 - mean_absolute_error: 0.0444\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8833 - acc: 0.9200 - mean_absolute_error: 0.0443\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8795 - acc: 0.9600 - mean_absolute_error: 0.0441\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8758 - acc: 0.9600 - mean_absolute_error: 0.0440\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8721 - acc: 0.9600 - mean_absolute_error: 0.0439\n",
      "Iteration 170\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8685 - acc: 0.9600 - mean_absolute_error: 0.0438\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8637 - acc: 0.9600 - mean_absolute_error: 0.0436\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8605 - acc: 0.9600 - mean_absolute_error: 0.0435\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8572 - acc: 0.9600 - mean_absolute_error: 0.0434\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8539 - acc: 0.9600 - mean_absolute_error: 0.0433\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8505 - acc: 0.9600 - mean_absolute_error: 0.0432\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8478 - acc: 0.9600 - mean_absolute_error: 0.0431\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8438 - acc: 0.9600 - mean_absolute_error: 0.0430\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8400 - acc: 0.9600 - mean_absolute_error: 0.0428\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8366 - acc: 0.9600 - mean_absolute_error: 0.0427\n",
      "Iteration 180\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8332 - acc: 0.9600 - mean_absolute_error: 0.0426\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8299 - acc: 0.9600 - mean_absolute_error: 0.0425\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8266 - acc: 0.9600 - mean_absolute_error: 0.0424\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8233 - acc: 0.9600 - mean_absolute_error: 0.0423\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8200 - acc: 0.9600 - mean_absolute_error: 0.0421\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8167 - acc: 0.9600 - mean_absolute_error: 0.0420\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8134 - acc: 0.9600 - mean_absolute_error: 0.0419\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8101 - acc: 0.9600 - mean_absolute_error: 0.0418\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8150 - acc: 0.9600 - mean_absolute_error: 0.0420\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8065 - acc: 0.9600 - mean_absolute_error: 0.0417\n",
      "Iteration 190\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7994 - acc: 0.9600 - mean_absolute_error: 0.0414\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7957 - acc: 0.9600 - mean_absolute_error: 0.0413\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7935 - acc: 0.9600 - mean_absolute_error: 0.0412\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7911 - acc: 0.9600 - mean_absolute_error: 0.0411\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7884 - acc: 0.9600 - mean_absolute_error: 0.0410\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7857 - acc: 0.9600 - mean_absolute_error: 0.0409\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7828 - acc: 0.9600 - mean_absolute_error: 0.0408\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7800 - acc: 0.9600 - mean_absolute_error: 0.0407\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7772 - acc: 0.9600 - mean_absolute_error: 0.0406\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7743 - acc: 0.9600 - mean_absolute_error: 0.0405\n",
      "Iteration 200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7713 - acc: 0.9600 - mean_absolute_error: 0.0404\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7687 - acc: 0.9600 - mean_absolute_error: 0.0403\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7657 - acc: 0.9600 - mean_absolute_error: 0.0402\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7628 - acc: 0.9600 - mean_absolute_error: 0.0401\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7598 - acc: 0.9600 - mean_absolute_error: 0.0400\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7567 - acc: 0.9600 - mean_absolute_error: 0.0399\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7537 - acc: 0.9600 - mean_absolute_error: 0.0397\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7509 - acc: 0.9600 - mean_absolute_error: 0.0396\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7478 - acc: 0.9600 - mean_absolute_error: 0.0395\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7431 - acc: 0.9600 - mean_absolute_error: 0.0393\n",
      "Iteration 210\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7411 - acc: 0.9600 - mean_absolute_error: 0.0393\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7388 - acc: 0.9600 - mean_absolute_error: 0.0392\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7361 - acc: 0.9600 - mean_absolute_error: 0.0391\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7332 - acc: 0.9600 - mean_absolute_error: 0.0390\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7304 - acc: 0.9600 - mean_absolute_error: 0.0389\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7277 - acc: 0.9600 - mean_absolute_error: 0.0388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 0s - loss: 0.7249 - acc: 0.9600 - mean_absolute_error: 0.0386\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7220 - acc: 0.9600 - mean_absolute_error: 0.0385\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7188 - acc: 0.9600 - mean_absolute_error: 0.0384\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7165 - acc: 0.9600 - mean_absolute_error: 0.0383\n",
      "Iteration 220\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7141 - acc: 0.9600 - mean_absolute_error: 0.0382\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7112 - acc: 0.9600 - mean_absolute_error: 0.0381\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7085 - acc: 0.9600 - mean_absolute_error: 0.0380\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7060 - acc: 0.9600 - mean_absolute_error: 0.0379\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7031 - acc: 0.9600 - mean_absolute_error: 0.0378\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.6999 - acc: 0.9600 - mean_absolute_error: 0.0377\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6938 - acc: 0.9600 - mean_absolute_error: 0.0374\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6942 - acc: 0.9600 - mean_absolute_error: 0.0374\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6963 - acc: 0.9600 - mean_absolute_error: 0.0375\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6920 - acc: 0.9600 - mean_absolute_error: 0.0373\n",
      "Iteration 230\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6879 - acc: 0.9600 - mean_absolute_error: 0.0372\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6849 - acc: 0.9600 - mean_absolute_error: 0.0371\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6823 - acc: 0.9600 - mean_absolute_error: 0.0370\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6781 - acc: 0.9600 - mean_absolute_error: 0.0368\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6762 - acc: 0.9600 - mean_absolute_error: 0.0367\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6746 - acc: 0.9600 - mean_absolute_error: 0.0366\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6815 - acc: 0.9600 - mean_absolute_error: 0.0369\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6795 - acc: 0.9600 - mean_absolute_error: 0.0368\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6646 - acc: 0.9600 - mean_absolute_error: 0.0363\n",
      "Epoch 1/1\n",
      " - 1s - loss: 0.6591 - acc: 0.9600 - mean_absolute_error: 0.0360\n",
      "Iteration 240\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6554 - acc: 0.9600 - mean_absolute_error: 0.0359\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6544 - acc: 0.9600 - mean_absolute_error: 0.0358\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6533 - acc: 0.9600 - mean_absolute_error: 0.0358\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6516 - acc: 0.9600 - mean_absolute_error: 0.0357\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6502 - acc: 0.9600 - mean_absolute_error: 0.0357\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6488 - acc: 0.9600 - mean_absolute_error: 0.0356\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6473 - acc: 0.9600 - mean_absolute_error: 0.0355\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6458 - acc: 0.9600 - mean_absolute_error: 0.0355\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6443 - acc: 0.9600 - mean_absolute_error: 0.0354\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6428 - acc: 0.9600 - mean_absolute_error: 0.0353\n",
      "Iteration 250\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6408 - acc: 0.9600 - mean_absolute_error: 0.0352\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6378 - acc: 1.0000 - mean_absolute_error: 0.0351\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6375 - acc: 1.0000 - mean_absolute_error: 0.0351\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6361 - acc: 1.0000 - mean_absolute_error: 0.0350\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6341 - acc: 1.0000 - mean_absolute_error: 0.0349\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6320 - acc: 1.0000 - mean_absolute_error: 0.0348\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6295 - acc: 1.0000 - mean_absolute_error: 0.0347\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6274 - acc: 1.0000 - mean_absolute_error: 0.0346\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6256 - acc: 1.0000 - mean_absolute_error: 0.0346\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6236 - acc: 1.0000 - mean_absolute_error: 0.0345\n",
      "Iteration 260\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6214 - acc: 1.0000 - mean_absolute_error: 0.0344\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6193 - acc: 1.0000 - mean_absolute_error: 0.0343\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6174 - acc: 1.0000 - mean_absolute_error: 0.0342\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6151 - acc: 1.0000 - mean_absolute_error: 0.0341\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6129 - acc: 1.0000 - mean_absolute_error: 0.0340\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6108 - acc: 1.0000 - mean_absolute_error: 0.0339\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6087 - acc: 1.0000 - mean_absolute_error: 0.0338\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6067 - acc: 1.0000 - mean_absolute_error: 0.0338\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6041 - acc: 1.0000 - mean_absolute_error: 0.0336\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6016 - acc: 1.0000 - mean_absolute_error: 0.0335\n",
      "Iteration 270\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5997 - acc: 1.0000 - mean_absolute_error: 0.0334\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5983 - acc: 1.0000 - mean_absolute_error: 0.0334\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5965 - acc: 1.0000 - mean_absolute_error: 0.0333\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5941 - acc: 1.0000 - mean_absolute_error: 0.0332\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5916 - acc: 1.0000 - mean_absolute_error: 0.0331\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5892 - acc: 1.0000 - mean_absolute_error: 0.0330\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5867 - acc: 1.0000 - mean_absolute_error: 0.0329\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5844 - acc: 1.0000 - mean_absolute_error: 0.0328\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5823 - acc: 1.0000 - mean_absolute_error: 0.0327\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5787 - acc: 1.0000 - mean_absolute_error: 0.0325\n",
      "Iteration 280\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5778 - acc: 1.0000 - mean_absolute_error: 0.0324\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5756 - acc: 1.0000 - mean_absolute_error: 0.0323\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5759 - acc: 1.0000 - mean_absolute_error: 0.0323\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5765 - acc: 1.0000 - mean_absolute_error: 0.0323\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5770 - acc: 1.0000 - mean_absolute_error: 0.0323\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5771 - acc: 1.0000 - mean_absolute_error: 0.0323\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5761 - acc: 1.0000 - mean_absolute_error: 0.0323\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5743 - acc: 1.0000 - mean_absolute_error: 0.0322\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5725 - acc: 1.0000 - mean_absolute_error: 0.0321\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5707 - acc: 1.0000 - mean_absolute_error: 0.0320\n",
      "Iteration 290\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5690 - acc: 1.0000 - mean_absolute_error: 0.0319\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5673 - acc: 1.0000 - mean_absolute_error: 0.0319\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5657 - acc: 1.0000 - mean_absolute_error: 0.0318\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5641 - acc: 1.0000 - mean_absolute_error: 0.0317\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5626 - acc: 1.0000 - mean_absolute_error: 0.0317\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5611 - acc: 1.0000 - mean_absolute_error: 0.0316\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5592 - acc: 1.0000 - mean_absolute_error: 0.0315\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5575 - acc: 1.0000 - mean_absolute_error: 0.0314\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5563 - acc: 1.0000 - mean_absolute_error: 0.0314\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5550 - acc: 1.0000 - mean_absolute_error: 0.0313\n",
      "Iteration 300\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5535 - acc: 1.0000 - mean_absolute_error: 0.0313\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5521 - acc: 1.0000 - mean_absolute_error: 0.0312\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5507 - acc: 1.0000 - mean_absolute_error: 0.0312\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5493 - acc: 1.0000 - mean_absolute_error: 0.0311\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5480 - acc: 1.0000 - mean_absolute_error: 0.0310\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5466 - acc: 1.0000 - mean_absolute_error: 0.0310\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5453 - acc: 1.0000 - mean_absolute_error: 0.0309\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5440 - acc: 1.0000 - mean_absolute_error: 0.0309\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5428 - acc: 1.0000 - mean_absolute_error: 0.0308\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5414 - acc: 1.0000 - mean_absolute_error: 0.0308\n",
      "Iteration 310\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5401 - acc: 1.0000 - mean_absolute_error: 0.0307\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5388 - acc: 1.0000 - mean_absolute_error: 0.0307\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5376 - acc: 1.0000 - mean_absolute_error: 0.0306\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5362 - acc: 1.0000 - mean_absolute_error: 0.0305\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5338 - acc: 1.0000 - mean_absolute_error: 0.0304\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5330 - acc: 1.0000 - mean_absolute_error: 0.0304\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5320 - acc: 1.0000 - mean_absolute_error: 0.0304\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5307 - acc: 1.0000 - mean_absolute_error: 0.0303\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5293 - acc: 1.0000 - mean_absolute_error: 0.0302\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5284 - acc: 1.0000 - mean_absolute_error: 0.0302\n",
      "Iteration 320\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5271 - acc: 1.0000 - mean_absolute_error: 0.0301\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5257 - acc: 1.0000 - mean_absolute_error: 0.0301\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5245 - acc: 1.0000 - mean_absolute_error: 0.0300\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5233 - acc: 1.0000 - mean_absolute_error: 0.0300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 0s - loss: 0.5222 - acc: 1.0000 - mean_absolute_error: 0.0299\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5210 - acc: 1.0000 - mean_absolute_error: 0.0299\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5199 - acc: 1.0000 - mean_absolute_error: 0.0298\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5188 - acc: 1.0000 - mean_absolute_error: 0.0298\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5178 - acc: 1.0000 - mean_absolute_error: 0.0297\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5170 - acc: 1.0000 - mean_absolute_error: 0.0297\n",
      "Iteration 330\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5157 - acc: 1.0000 - mean_absolute_error: 0.0297\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5145 - acc: 1.0000 - mean_absolute_error: 0.0296\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5134 - acc: 1.0000 - mean_absolute_error: 0.0296\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5122 - acc: 1.0000 - mean_absolute_error: 0.0295\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5111 - acc: 1.0000 - mean_absolute_error: 0.0295\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5100 - acc: 1.0000 - mean_absolute_error: 0.0294\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5090 - acc: 1.0000 - mean_absolute_error: 0.0294\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5079 - acc: 1.0000 - mean_absolute_error: 0.0293\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5068 - acc: 1.0000 - mean_absolute_error: 0.0293\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5057 - acc: 1.0000 - mean_absolute_error: 0.0292\n",
      "Iteration 340\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5047 - acc: 1.0000 - mean_absolute_error: 0.0292\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5036 - acc: 1.0000 - mean_absolute_error: 0.0291\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5025 - acc: 1.0000 - mean_absolute_error: 0.0291\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5014 - acc: 1.0000 - mean_absolute_error: 0.0290\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5003 - acc: 1.0000 - mean_absolute_error: 0.0290\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4995 - acc: 1.0000 - mean_absolute_error: 0.0289\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4983 - acc: 1.0000 - mean_absolute_error: 0.0289\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4965 - acc: 1.0000 - mean_absolute_error: 0.0288\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4951 - acc: 1.0000 - mean_absolute_error: 0.0287\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4939 - acc: 1.0000 - mean_absolute_error: 0.0287\n",
      "Iteration 350\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4930 - acc: 1.0000 - mean_absolute_error: 0.0286\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4921 - acc: 1.0000 - mean_absolute_error: 0.0286\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4910 - acc: 1.0000 - mean_absolute_error: 0.0285\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4901 - acc: 1.0000 - mean_absolute_error: 0.0285\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4879 - acc: 1.0000 - mean_absolute_error: 0.0284\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4866 - acc: 1.0000 - mean_absolute_error: 0.0283\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4854 - acc: 1.0000 - mean_absolute_error: 0.0283\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4853 - acc: 1.0000 - mean_absolute_error: 0.0283\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4847 - acc: 1.0000 - mean_absolute_error: 0.0282\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4841 - acc: 1.0000 - mean_absolute_error: 0.0282\n",
      "Iteration 360\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4832 - acc: 1.0000 - mean_absolute_error: 0.0282\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4823 - acc: 1.0000 - mean_absolute_error: 0.0281\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4817 - acc: 1.0000 - mean_absolute_error: 0.0281\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4801 - acc: 1.0000 - mean_absolute_error: 0.0280\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4788 - acc: 1.0000 - mean_absolute_error: 0.0280\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4778 - acc: 1.0000 - mean_absolute_error: 0.0279\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4769 - acc: 1.0000 - mean_absolute_error: 0.0279\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4754 - acc: 1.0000 - mean_absolute_error: 0.0278\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4740 - acc: 1.0000 - mean_absolute_error: 0.0277\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4736 - acc: 1.0000 - mean_absolute_error: 0.0277\n",
      "Iteration 370\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4729 - acc: 1.0000 - mean_absolute_error: 0.0277\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4721 - acc: 1.0000 - mean_absolute_error: 0.0277\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4712 - acc: 1.0000 - mean_absolute_error: 0.0276\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4704 - acc: 1.0000 - mean_absolute_error: 0.0276\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4695 - acc: 1.0000 - mean_absolute_error: 0.0275\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4686 - acc: 1.0000 - mean_absolute_error: 0.0275\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4671 - acc: 1.0000 - mean_absolute_error: 0.0274\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4658 - acc: 1.0000 - mean_absolute_error: 0.0274\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4653 - acc: 1.0000 - mean_absolute_error: 0.0273\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4648 - acc: 1.0000 - mean_absolute_error: 0.0273\n",
      "Iteration 380\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4640 - acc: 1.0000 - mean_absolute_error: 0.0273\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4632 - acc: 1.0000 - mean_absolute_error: 0.0272\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4626 - acc: 1.0000 - mean_absolute_error: 0.0272\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4622 - acc: 1.0000 - mean_absolute_error: 0.0272\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4605 - acc: 1.0000 - mean_absolute_error: 0.0271\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4590 - acc: 1.0000 - mean_absolute_error: 0.0270\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4579 - acc: 1.0000 - mean_absolute_error: 0.0270\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4570 - acc: 1.0000 - mean_absolute_error: 0.0270\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4563 - acc: 1.0000 - mean_absolute_error: 0.0269\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4557 - acc: 1.0000 - mean_absolute_error: 0.0269\n",
      "Iteration 390\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4553 - acc: 1.0000 - mean_absolute_error: 0.0269\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4554 - acc: 1.0000 - mean_absolute_error: 0.0269\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4537 - acc: 1.0000 - mean_absolute_error: 0.0268\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4522 - acc: 1.0000 - mean_absolute_error: 0.0267\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4511 - acc: 1.0000 - mean_absolute_error: 0.0267\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4503 - acc: 1.0000 - mean_absolute_error: 0.0266\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4496 - acc: 1.0000 - mean_absolute_error: 0.0266\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4491 - acc: 1.0000 - mean_absolute_error: 0.0266\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4485 - acc: 1.0000 - mean_absolute_error: 0.0265\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4479 - acc: 1.0000 - mean_absolute_error: 0.0265\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters to the ones that give good convergence.\n",
    "optimizer = optimizers.Adam(lr=0.0025, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0005, amsgrad=False)\n",
    "\n",
    "# Interestingly, adding 'mae' to the metrics really helps improve the stability.\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
    "              metrics=['accuracy', 'mae'])\n",
    "\n",
    "# We are going to save the best weights for the model.  Keras has no way to save the best model\n",
    "# in an easy and efficient way (if not validating) except for this way which works well.\n",
    "model.save_weights('tmp/weights.hdf5')\n",
    "\n",
    "# Here we explicitely run each epoch and reset the state after every iteration.\n",
    "best_acc = 0.0\n",
    "worst_loss = worst_mae = math.inf\n",
    "best_i = 0\n",
    "\n",
    "# Note - we want to look at MAE, Loss, and Accuracy\n",
    "metrics = {'acc':list(), 'loss':list(), 'mae':list()}\n",
    "\n",
    "# Start the training iterations\n",
    "for i in range(number_epochs):\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration {}'.format(i))\n",
    "    history = model.fit(X, y, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "\n",
    "    metrics['acc'].append(history.history['acc'][0])\n",
    "    metrics['loss'].append(history.history['loss'][0])\n",
    "    metrics['mae'].append(history.history['mean_absolute_error'][0])\n",
    "    \n",
    "    # If the model has better accuracy and less loss and less mae, we consider it a better model\n",
    "    if history.history['acc'][0] >= best_acc and history.history['loss'][0] <= worst_loss and \\\n",
    "       history.history['mean_absolute_error'][0] <= worst_mae:\n",
    "        # Save it and update the best values.\n",
    "        model.save_weights('tmp/weights.hdf5')\n",
    "        best_acc = history.history['acc']\n",
    "        worst_loss = history.history['loss']\n",
    "        worst_mae = history.history['mean_absolute_error']\n",
    "        best_i = i\n",
    "    # Reset the states.\n",
    "    model.reset_states()\n",
    "\n",
    "# When finished, load up the best model (weights) and print out the accuracy and loss.\n",
    "model.load_weights('tmp/weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model on iteration 399 : Accuracy = [1.0], Loss = [0.44791293382644654]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5ce392e048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Best Model on iteration {} : Accuracy = {}, Loss = {}'.format(i, best_acc, worst_loss))\n",
    "\n",
    "# Plot the MAE and the accuracy - let's look at convergence...\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(metrics['acc'])\n",
    "plt.plot(metrics['mae'])\n",
    "plt.title('Model Accuracy and MAE (lr=0.0025)')\n",
    "plt.ylabel('Accuracy and MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the plot, that we could have stopped the optimization at 300 epochs.  This is because we chose a good combination of learning rate lr and decay rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 100.00%\n",
      "A -> B\n",
      "B -> C\n",
      "C -> D\n",
      "D -> E\n",
      "E -> F\n",
      "F -> G\n",
      "G -> H\n",
      "H -> I\n",
      "I -> J\n",
      "J -> K\n",
      "K -> L\n",
      "L -> M\n",
      "M -> N\n",
      "N -> O\n",
      "O -> P\n",
      "P -> Q\n",
      "Q -> R\n",
      "R -> S\n",
      "S -> T\n",
      "T -> U\n",
      "U -> V\n",
      "V -> W\n",
      "W -> X\n",
      "X -> Y\n",
      "Y -> Z\n",
      "New start:  K\n",
      "K -> B\n",
      "B -> B\n",
      "B -> C\n",
      "C -> C\n",
      "C -> C\n"
     ]
    }
   ],
   "source": [
    "# summarize performance of the model.  Note that this should match the values above.\n",
    "scores = model.evaluate(X, y, batch_size=batch_size, verbose=0)\n",
    "model.reset_states()\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "# demonstrate some model predictions\n",
    "seed = [char_to_int[alphabet[0]]]\n",
    "for i in range(0, len(alphabet)-1):\n",
    "\tx = np.reshape(seed, (1, len(seed), 1))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tprint(int_to_char[seed[0]], \"->\", int_to_char[index])\n",
    "\tseed = [index]\n",
    "model.reset_states()\n",
    "# demonstrate a random starting point\n",
    "letter = \"K\"\n",
    "seed = [char_to_int[letter]]\n",
    "print(\"New start: \", letter)\n",
    "for i in range(0, 5):\n",
    "\tx = np.reshape(seed, (1, len(seed), 1))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tprint(int_to_char[seed[0]], \"->\", int_to_char[index])\n",
    "\tseed = [index]\n",
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we can recognize the next letter in the sequential set, the training still is not doing all that well with an arbitrary start.  Really, we need to start looking at any length sequence of letters and training the LSTM to really \"know\" the alphabet.\n",
    "\n",
    "### LSTM with Variable-Length Input to One-Char Output\n",
    "\n",
    "In this section we explore a variation of the “stateless” LSTM that learns random subsequences of the alphabet and an effort to build a model that can be given arbitrary letters or subsequences of letters and predict the next letter in the alphabet.\n",
    "\n",
    "First, change the framing of the problem. Define a maximum input sequence length and set it to a small value like 5 to speed up training. This defines the maximum length of subsequences of the alphabet will be drawn for training. In extensions, this could just as set to the full alphabet (26) or longer if we allow looping back to the start of the sequence.\n",
    "\n",
    "We also need to define the number of random sequences to create for training.  Brownlee originally chose 1000 then after running the problem stated that we might need more sets to train on.  At 1500, we had great results.  What happens at 1000 with the tuning that we have in our optimization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PQRST -> U\n",
      "W -> X\n",
      "O -> P\n",
      "OPQ -> R\n",
      "IJKLM -> N\n",
      "QRSTU -> V\n",
      "ABCD -> E\n",
      "X -> Y\n",
      "GHIJ -> K\n",
      "M -> N\n",
      "XY -> Z\n",
      "QRST -> U\n"
     ]
    }
   ],
   "source": [
    "# LSTM with Variable Length Input Sequences to One Character Output\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K\n",
    "import math\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "num_inputs = 1000\n",
    "max_len = 5\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(num_inputs):\n",
    "    # Start randomly in the alphabet and end ramdomly 1-5 characters later.\n",
    "    start = np.random.randint(len(alphabet)-2)\n",
    "    end = np.random.randint(start, min(start+max_len,len(alphabet)-1))\n",
    "    sequence_in = alphabet[start:end+1]\n",
    "    sequence_out = alphabet[end + 1]\n",
    "    dataX.append([char_to_int[char] for char in sequence_in])\n",
    "    dataY.append(char_to_int[sequence_out])\n",
    "    if i < 12:\n",
    "        print(sequence_in, '->', sequence_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input sequences vary in length between 1 and max_len and therefore require zero padding. Use left-hand-side (prefix) padding with the Keras built in pad_sequences() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of lists to array and pad sequences if needed\n",
    "X = pad_sequences(dataX, maxlen=max_len, dtype='float32')\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(X, (X.shape[0], max_len, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# create and fit the model\n",
    "batch_size = 1\n",
    "number_epochs = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], 1)))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# Set the parameters to the ones that give good convergence.  Note that we use a golden ratio\n",
    "# type reduction scheme to reduce the learning rate when we appear to diverge.\n",
    "optimizer = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "# Interestingly, adding 'mae' to the metrics really helps improve the stability.\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
    "              metrics=['accuracy', 'mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Close Control on Optimization**\n",
    "\n",
    "Because Keras allows us to do one epoch or step at a time, we can really take control of the optimization to make it greedy as appropriate and then back off the learning rate as the problem converges.\n",
    "\n",
    "This approach carefully monitors MAE, loss, and accuracy.  The approach is to start with a large learning rate (already set to 0.01), but when we diverge (actually don't converge) twice, we reduce the learning rate by the golden ratio.  We choose the golden ratio from some of the older literature having to do with line search.  However, this method seems to work.\n",
    "\n",
    "We also set a stopping condition.  If we have already reached the minimum learning rate and are no longer converging ('non_convergence_conditions'), then stop.\n",
    "\n",
    "Lastly, as before, always keep the best iteration.  Whatever that is, we will use it when finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Epoch 1/1\n",
      " - 9s - loss: 2.7243 - acc: 0.1300 - mean_absolute_error: 0.0705\n",
      "Epoch 1/1\n",
      " - 6s - loss: 1.9852 - acc: 0.2360 - mean_absolute_error: 0.0636\n",
      "Epoch 1/1\n",
      "Epoch 1/1\n",
      " - 7s - loss: 1.3985 - acc: 0.4320 - mean_absolute_error: 0.0533\n",
      "Epoch 1/1\n",
      " - 6s - loss: 1.3117 - acc: 0.4640 - mean_absolute_error: 0.0508\n",
      "Epoch 1/1\n",
      " - 6s - loss: 1.1921 - acc: 0.5160 - mean_absolute_error: 0.0477\n",
      "Epoch 1/1\n",
      " - 6s - loss: 1.0761 - acc: 0.5640 - mean_absolute_error: 0.0440\n",
      "Epoch 1/1\n",
      " - 7s - loss: 1.0486 - acc: 0.5880 - mean_absolute_error: 0.0422\n",
      "Epoch 1/1\n",
      " - 8s - loss: 1.0233 - acc: 0.5800 - mean_absolute_error: 0.0414\n",
      "On step 8, did not have a convergent step\n",
      "Epoch 1/1\n",
      " - 7s - loss: 0.9443 - acc: 0.6250 - mean_absolute_error: 0.0385\n",
      "Iteration 10\n",
      "Epoch 1/1\n",
      " - 7s - loss: 0.8206 - acc: 0.6830 - mean_absolute_error: 0.0349\n",
      "Epoch 1/1\n",
      " - 7s - loss: 0.8767 - acc: 0.6410 - mean_absolute_error: 0.0358\n",
      "On step 11 (2nd non-convergence), reduced lr from 0.009999999776482582 to 0.006180339749357587\n",
      "Epoch 1/1\n",
      " - 7s - loss: 0.6161 - acc: 0.8030 - mean_absolute_error: 0.0280\n",
      "Epoch 1/1\n",
      " - 7s - loss: 0.6214 - acc: 0.7710 - mean_absolute_error: 0.0274\n",
      "On step 13, did not have a convergent step\n",
      "Epoch 1/1\n",
      " - 7s - loss: 0.5602 - acc: 0.8090 - mean_absolute_error: 0.0252\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.5209 - acc: 0.8190 - mean_absolute_error: 0.0231\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.6195 - acc: 0.7840 - mean_absolute_error: 0.0254\n",
      "On step 16 (2nd non-convergence), reduced lr from 0.0061803399585187435 to 0.0038196601563936987\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.4442 - acc: 0.8660 - mean_absolute_error: 0.0206\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.4017 - acc: 0.8710 - mean_absolute_error: 0.0182\n",
      "Epoch 1/1\n",
      " - 7s - loss: 0.4103 - acc: 0.8660 - mean_absolute_error: 0.0186\n",
      "On step 19, did not have a convergent step\n",
      "Iteration 20\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.4427 - acc: 0.8470 - mean_absolute_error: 0.0193\n",
      "On step 20 (2nd non-convergence), reduced lr from 0.0038196600507944822 to 0.00236067973686114\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.3588 - acc: 0.8870 - mean_absolute_error: 0.0164\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.3487 - acc: 0.8910 - mean_absolute_error: 0.0158\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.3425 - acc: 0.8810 - mean_absolute_error: 0.0155\n",
      "On step 23, did not have a convergent step\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.3450 - acc: 0.8970 - mean_absolute_error: 0.0158\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.3267 - acc: 0.8990 - mean_absolute_error: 0.0148\n",
      "Epoch 1/1\n",
      " - 7s - loss: 0.3303 - acc: 0.8890 - mean_absolute_error: 0.0149\n",
      "On step 26 (2nd non-convergence), reduced lr from 0.0023606796748936176 to 0.0014589802756353075\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.3111 - acc: 0.9080 - mean_absolute_error: 0.0142\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.3083 - acc: 0.9050 - mean_absolute_error: 0.0140\n",
      "On step 28, did not have a convergent step\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.3042 - acc: 0.9080 - mean_absolute_error: 0.0139\n",
      "Iteration 30\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.3018 - acc: 0.8980 - mean_absolute_error: 0.0137\n",
      "On step 30 (2nd non-convergence), reduced lr from 0.0014589802594855428 to 0.0009016993892772066\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2905 - acc: 0.9110 - mean_absolute_error: 0.0134\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2882 - acc: 0.9210 - mean_absolute_error: 0.0132\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2866 - acc: 0.9130 - mean_absolute_error: 0.0132\n",
      "On step 33, did not have a convergent step\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2848 - acc: 0.9220 - mean_absolute_error: 0.0131\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2813 - acc: 0.9170 - mean_absolute_error: 0.0129\n",
      "On step 35 (2nd non-convergence), reduced lr from 0.0009016994154080749 to 0.0005572808863581009\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2776 - acc: 0.9130 - mean_absolute_error: 0.0128\n",
      "On step 36, did not have a convergent step\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2757 - acc: 0.9180 - mean_absolute_error: 0.0128\n",
      "Epoch 1/1\n",
      " - 7s - loss: 0.2747 - acc: 0.9180 - mean_absolute_error: 0.0127\n",
      "On step 38 (2nd non-convergence), reduced lr from 0.0005572809022851288 to 0.00034441853889341853\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2709 - acc: 0.9240 - mean_absolute_error: 0.0126\n",
      "Iteration 40\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2701 - acc: 0.9230 - mean_absolute_error: 0.0126\n",
      "On step 40, did not have a convergent step\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2693 - acc: 0.9230 - mean_absolute_error: 0.0125\n",
      "On step 41 (2nd non-convergence), reduced lr from 0.0003444185422267765 to 0.00021286236545183876\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2670 - acc: 0.9220 - mean_absolute_error: 0.0125\n",
      "On step 42, did not have a convergent step\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2664 - acc: 0.9270 - mean_absolute_error: 0.0125\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2663 - acc: 0.9270 - mean_absolute_error: 0.0124\n",
      "On step 44 (2nd non-convergence), reduced lr from 0.00021286236005835235 to 0.0001315561734415798\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2648 - acc: 0.9200 - mean_absolute_error: 0.0124\n",
      "On step 45, did not have a convergent step\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2643 - acc: 0.9200 - mean_absolute_error: 0.0123\n",
      "On step 46 (2nd non-convergence), reduced lr from 0.0001315561676165089 to 8.130618301668075e-05\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2634 - acc: 0.9200 - mean_absolute_error: 0.0123\n",
      "On step 47, did not have a convergent step\n",
      "Epoch 1/1\n",
      " - 7s - loss: 0.2631 - acc: 0.9200 - mean_absolute_error: 0.0123\n",
      "On step 48 (2nd non-convergence), reduced lr from 8.130618516588584e-05 to 5.024998592810995e-05\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2624 - acc: 0.9200 - mean_absolute_error: 0.0123\n",
      "On step 49, did not have a convergent step\n",
      "Iteration 50\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2623 - acc: 0.9200 - mean_absolute_error: 0.0123\n",
      "On step 50 (2nd non-convergence), reduced lr from 5.024998608860187e-05 to 3.105619933696534e-05\n",
      "Epoch 1/1\n",
      " - 7s - loss: 0.2619 - acc: 0.9200 - mean_absolute_error: 0.0123\n",
      "On step 51, did not have a convergent step\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2618 - acc: 0.9200 - mean_absolute_error: 0.0123\n",
      "On step 52 (2nd non-convergence), reduced lr from 3.1056199077283964e-05 to 1.919378659114461e-05\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2615 - acc: 0.9200 - mean_absolute_error: 0.0123\n",
      "On step 53, did not have a convergent step\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2615 - acc: 0.9200 - mean_absolute_error: 0.0123\n",
      "On step 54 (2nd non-convergence), reduced lr from 1.919378701131791e-05 to 1.186241274582073e-05\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2613 - acc: 0.9200 - mean_absolute_error: 0.0123\n",
      "On step 55, did not have a convergent step\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2613 - acc: 0.9200 - mean_absolute_error: 0.0123\n",
      "On step 56 (2nd non-convergence), reduced lr from 1.1862412975460757e-05 to 1e-05\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2612 - acc: 0.9200 - mean_absolute_error: 0.0123\n",
      "On step 57, did not have a convergent step\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2612 - acc: 0.9200 - mean_absolute_error: 0.0123\n",
      "On step 58 (2nd non-convergence), reduced lr from 9.999999747378752e-06 to 1e-05\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.2612 - acc: 0.9200 - mean_absolute_error: 0.0123\n",
      "On step 59, did not have a convergent step\n",
      "Step 59 - No longer converging.  Stopping iterations\n"
     ]
    }
   ],
   "source": [
    "# Note - we want to look at MAE, Loss, and Accuracy\n",
    "metrics = {'acc':list(), 'loss':list(), 'mae':list()}\n",
    "\n",
    "# Here we explicitely run each epoch and reset the state after every iteration.\n",
    "best_acc = 0.0\n",
    "worst_loss = worst_mae = math.inf\n",
    "best_i = 0\n",
    "first_divergent_i = False\n",
    "\n",
    "minimum_learning_rate = 0.00001\n",
    "# Change of loss over n steps, e.g., if abs(loss[i]-loss[i-3]) < 0.0001, stop.\n",
    "non_convergence_conditions = [0.0001,3]\n",
    "\n",
    "# Start the training iterations\n",
    "for i in range(number_epochs):\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration {}'.format(i))\n",
    "    history = model.fit(X, y, epochs=1, batch_size=batch_size, verbose=2)\n",
    "\n",
    "    metrics['acc'].append(history.history['acc'][0])\n",
    "    metrics['loss'].append(history.history['loss'][0])\n",
    "    metrics['mae'].append(history.history['mean_absolute_error'][0])\n",
    "    \n",
    "    # If the model has better accuracy and less loss and less mae, we consider it a better model\n",
    "    if history.history['acc'][0] >= best_acc and history.history['loss'][0] <= worst_loss and \\\n",
    "       history.history['mean_absolute_error'][0] <= worst_mae:\n",
    "        # Save it and update the best values.\n",
    "        model.save_weights('tmp/weights.hdf5')\n",
    "        best_acc = history.history['acc']\n",
    "        worst_loss = history.history['loss']\n",
    "        worst_mae = history.history['mean_absolute_error']\n",
    "        best_i = i\n",
    "    \n",
    "    # Modify the learning rate if we take a second step in the wrong direction (greedy).\n",
    "    current_lr = float(K.get_value(model.optimizer.lr))\n",
    "    current_acc = round(metrics['acc'][len(metrics['acc'])-1],4)\n",
    "    prev_acc = round(metrics['acc'][len(metrics['acc'])-2], 4)\n",
    "    if i > 2 and current_acc <= prev_acc:\n",
    "        if first_divergent_i is False:\n",
    "            print('On step {}, did not have a convergent step'.format(i))\n",
    "            first_divergent_i = True\n",
    "        else:\n",
    "            first_divergent_i = False\n",
    "            new_lr = max(minimum_learning_rate, current_lr/1.6180339887498948482) # 1.6... is golden ratio\n",
    "            K.set_value(model.optimizer.lr, new_lr)\n",
    "            print('On step {} (2nd non-convergence), reduced lr from {} to {}'.format(i, current_lr, new_lr))\n",
    "    \n",
    "    # Check for what I term non-convergence.  That is, we have minimized the step as much as we want\n",
    "    # and the loss just is not dropping anymore.\n",
    "    if i > int(non_convergence_conditions[1]) and abs(current_lr - minimum_learning_rate) < 1.0e-12:\n",
    "        last_index = len(metrics['loss'])-1\n",
    "        prev_index = last_index - int(non_convergence_conditions[1])\n",
    "        if abs(metrics['loss'][last_index] - metrics['loss'][prev_index]) < non_convergence_conditions[0]:\n",
    "            print('Step {} - No longer converging.  Stopping iterations'.format(i))\n",
    "            break\n",
    "        \n",
    "# When finished, load up the best model (weights)\n",
    "model.load_weights('tmp/weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached (near)convergence on step 69 with loss=0.21629691293975445, accuracy=0.9753333333333334, and mae=0.010754402329143172\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f691be84b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Reached (near)convergence on step {} with loss={}, accuracy={}, and mae={}'.format(best_i, metrics['loss'][best_i], \\\n",
    "                                                                                          metrics['acc'][best_i], \\\n",
    "                                                                                          metrics['mae'][best_i]))\n",
    "\n",
    "# Plot the MAE and the accuracy - let's look at convergence...\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(metrics['acc'])\n",
    "plt.plot(metrics['mae'])\n",
    "plt.title('Model Accuracy and MAE')\n",
    "plt.ylabel('Accuracy and MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 97.53%\n",
      "['E'] -> F\n",
      "['G', 'H', 'I', 'J', 'K'] -> L\n",
      "['D', 'E', 'F', 'G'] -> H\n",
      "['T'] -> U\n",
      "['R', 'S', 'T', 'U', 'V'] -> W\n",
      "['H', 'I', 'J'] -> K\n",
      "['J', 'K', 'L'] -> M\n",
      "['C', 'D'] -> E\n",
      "['C'] -> D\n",
      "['H', 'I', 'J', 'K'] -> L\n",
      "['O', 'P', 'Q'] -> R\n",
      "['R', 'S', 'T'] -> U\n",
      "['R', 'S', 'T'] -> U\n",
      "['D', 'E', 'F', 'G'] -> H\n",
      "['A', 'B', 'C', 'D', 'E'] -> F\n",
      "['Q', 'R', 'S', 'T', 'U'] -> V\n",
      "['W'] -> X\n",
      "['K'] -> L\n",
      "['X', 'Y'] -> Z\n",
      "['V', 'W', 'X', 'Y'] -> Z\n"
     ]
    }
   ],
   "source": [
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# demonstrate some model predictions\n",
    "for i in range(20):\n",
    "\tpattern_index = np.random.randint(len(dataX))\n",
    "\tpattern = dataX[pattern_index]\n",
    "\tx = pad_sequences([pattern], maxlen=max_len, dtype='float32')\n",
    "\tx = np.reshape(x, (1, max_len, 1))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tprint(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is pretty awesome - we have taught a computer the ABCs.  Can we do something more useful like language translation?  Before answering that question with a worked example, let's capture programmatically our greedy optimization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import math\n",
    "\n",
    "def greedy_optimizer(X, y, batch_size, model, number_epochs, minimum_learning_rate = 0.00001,\n",
    "                     # Change of loss over n steps, e.g., if abs(loss[i]-loss[i-3]) < 0.0001, stop.\n",
    "                     non_convergence_conditions = [0.0001, 3], validation_split=None,\n",
    "                     # Note - we want to look at MAE, Loss, and Accuracy\n",
    "                     metrics = {'acc':list(), 'loss':list(), 'mae':list()}\n",
    "                    ):\n",
    "\n",
    "    # Here we explicitely run each epoch and reset the state after every iteration.\n",
    "    best_acc = 0.0\n",
    "    worst_loss = worst_mae = math.inf\n",
    "    best_i = 0\n",
    "    first_divergent_i = False\n",
    "\n",
    "    # Start the training iterations\n",
    "    for i in range(number_epochs):\n",
    "        if i % 10 == 0:\n",
    "            print('Iteration {} of {}'.format(i, number_epochs))\n",
    "        history = model.fit(X, y, epochs=1, batch_size=batch_size, verbose=2,\n",
    "                            validation_split=validation_split)\n",
    "\n",
    "        metrics['acc'].append(history.history['acc'][0])\n",
    "        metrics['loss'].append(history.history['loss'][0])\n",
    "        metrics['mae'].append(history.history['mean_absolute_error'][0])\n",
    "    \n",
    "        # If the model has better accuracy and less loss and less mae, we consider it a better model\n",
    "        if history.history['acc'][0] >= best_acc and history.history['loss'][0] <= worst_loss and \\\n",
    "           history.history['mean_absolute_error'][0] <= worst_mae:\n",
    "            # Save it and update the best values.\n",
    "            model.save_weights('tmp/wkweights.hdf5')\n",
    "            best_acc = history.history['acc']\n",
    "            worst_loss = history.history['loss']\n",
    "            worst_mae = history.history['mean_absolute_error']\n",
    "            best_i = i\n",
    "    \n",
    "        # Modify the learning rate if we take a second step in the wrong direction (greedy).\n",
    "        current_lr = float(K.get_value(model.optimizer.lr))\n",
    "        current_acc = round(metrics['acc'][len(metrics['acc'])-1],4)\n",
    "        prev_acc = round(metrics['acc'][len(metrics['acc'])-2], 4)\n",
    "        if i > 2 and current_acc <= prev_acc:\n",
    "            if first_divergent_i is False:\n",
    "                print('On step {}, did not have a convergent step'.format(i))\n",
    "                first_divergent_i = True\n",
    "            else:\n",
    "                first_divergent_i = False\n",
    "                new_lr = max(minimum_learning_rate, current_lr/1.6180339887498948482) # 1.6... is golden ratio\n",
    "                K.set_value(model.optimizer.lr, new_lr)\n",
    "                print('On step {} (2nd non-convergence), reduced lr from {} to {}'.format(i, current_lr, new_lr))\n",
    "    \n",
    "        # Check for what I term non-convergence.  That is, we have minimized the step as much as we want\n",
    "        # and the loss just is not dropping anymore.\n",
    "        if i > int(non_convergence_conditions[1]) and abs(current_lr - minimum_learning_rate) < 1.0e-12:\n",
    "            last_index = len(metrics['loss'])-1\n",
    "            prev_index = last_index - int(non_convergence_conditions[1])\n",
    "            if abs(metrics['loss'][last_index] - metrics['loss'][prev_index]) < non_convergence_conditions[0]:\n",
    "                print('Step {} - No longer converging.  Stopping iterations'.format(i))\n",
    "                break\n",
    "        \n",
    "    # When finished, load up the best model (weights)\n",
    "    model.load_weights('tmp/wkweights.hdf5')\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM in an RNN\n",
    "\n",
    "A network of LSTMs may be used to create an RNN.  One usage of an RNN using LSTMs is to build a network that can do language translation.  There is an awesome site at http://www.manythings.org/anki/ which provides many English / another language translation word pairs such that we can try a couple different of languages and see how this network runs.\n",
    "\n",
    "The method is outlined in [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078).  Pretending we are doing a great commercial set of translators, we will create a method to download the file from the site and store the trained weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence Learning\n",
    "\n",
    "Sequence to sequence learning (Seq2Seq) trains models to convert sequences from one domain (e.g. sentences in English) to sequences in another domain (e.g. the same sentences translated to French).  Seq2Seq was introduced by [Sutskever et al](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) in 2015.  Seq2Seq can be implemented using RNNs or [1-D convnets](https://datawarrior.wordpress.com/2017/05/11/convnet-seq2seq-for-machine-translation/).\n",
    "\n",
    "### The Trivial Case\n",
    "\n",
    "The trivial case is when the input and output sequences are of the same length.  For example, we could use an RNN, LSTM or [GRU](https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787128422/6/ch06lvl1sec44/gated-recurrent-unit--gru) to develop network to add numbers as characters, e.g., ```345+452``` is equal to ```0000797```.  For example, in the [script](https://github.com/keras-team/keras/blob/master/examples/addition_rnn.py), a model is constructed which is very similar to those we built for learning the alphabet.\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n",
    "# Note: In a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, num_feature).\n",
    "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
    "# As the decoder RNN's input, repeatedly provide with the last hidden state of\n",
    "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "model.add(layers.RepeatVector(DIGITS + 1))\n",
    "# The decoder RNN could be multiple layers stacked or a single layer.\n",
    "for _ in range(LAYERS):\n",
    "    # By setting return_sequences to True, return not only the last output but\n",
    "    # all the outputs so far in the form of (num_samples, timesteps,\n",
    "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "    # the first dimension to be the timesteps.\n",
    "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "# of the output sequence, decide which character should be chosen.\n",
    "model.add(layers.TimeDistributed(layers.Dense(len(chars))))\n",
    "model.add(layers.Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In General - Canonical sequence-to-sequence (Seq2Seq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the Algorithm\n",
    "Start with input sequences from a domain (e.g. English sentences) and corresponding target sequences from another domain (e.g. French sentences).  In our case, we have modified the original algorithm presented at [github](https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py) such that we can just fill in a dictionary to create the necessary model data for any language and create the model on the fly.\n",
    "\n",
    "In a nutshell:\n",
    "1. Get and prepare the training data into training sequences.  Specifically,\n",
    "Turn the sentences into 3 Numpy arrays, encoder_input_data, decoder_input_data, decoder_target_data:\n",
    "encoder_input_data is a 3D array of shape (num_pairs, max_english_sentence_length, num_english_characters) containing a one-hot vectorization of the English sentences.\n",
    "decoder_input_data is a 3D array of shape (num_pairs, max_french_sentence_length, num_french_characters) containg a one-hot vectorization of the French sentences.\n",
    "decoder_target_data is the same as decoder_input_data but offset by one timestep. decoder_target_data[:, t, :] will be the same as decoder_input_data[:, t + 1, :].\n",
    "\n",
    "2. An encoder LSTM turns input sequences to 2 state vectors (we keep the last LSTM state and discard the outputs).\n",
    "\n",
    "3. A decoder LSTM is trained to turn the target sequences into the same sequence but offset by one timestep in the future, a training process called \"teacher forcing\" in this context. Is uses as initial state the state vectors from the encoder. Effectively, the decoder learns to generate `targets[t+1...]` given `targets[...t]`, conditioned on the input sequence.\n",
    "4. In inference mode, when we want to decode unknown input sequences, we:\n",
    "    * Encode the input sequence into state vectors\n",
    "    * Start with a target sequence of size 1 (just the start-of-sequence character)\n",
    "    * Feed the state vectors and 1-char target sequence to the decoder to produce predictions for the next character\n",
    "    * Sample the next character using these predictions (we simply use argmax).\n",
    "    * Append the sampled character to the target sequence\n",
    "    * Repeat until we generate the end-of-sequence character or we hit the character limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import requests, zipfile, io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dictionary of languages to try.  I note that we have adopted this code from [github](https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py) which is covered by a [Keras blog](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = {\n",
    "    'french':{\n",
    "        'batch_size':64,     # Batch size for training.\n",
    "        'epochs':100,        # Number of epochs to train for.\n",
    "        'latent_dim':256,    # Latent dimensionality of the encoding space.\n",
    "        'num_samples':10000, # Number of samples to train on.\n",
    "        'datas_path':'http://www.manythings.org/anki/',\n",
    "        'lang_file':'fra-eng.zip', # Path to the zip file\n",
    "        'lang_txt_file':'fra.txt', # The actual text file\n",
    "        'weight_file':'tmp/languages/french_weights.hd5' # Weights from training.\n",
    "    },\n",
    "    'spanish':{\n",
    "        'batch_size':64,\n",
    "        'epochs':100,\n",
    "        'latent_dim':256,\n",
    "        'num_samples':10000,\n",
    "        'datas_path':'http://www.manythings.org/anki/',\n",
    "        'lang_file':'spa-eng.zip', # Path to the zip file\n",
    "        'lang_txt_file':'spa.txt', # The actual text file\n",
    "        'weight_file':'tmp/languages/spanish_weights.hd5' # Weights from training.\n",
    "    },\n",
    "    'german':{\n",
    "        'batch_size':64,\n",
    "        'epochs':100,\n",
    "        'latent_dim':256,\n",
    "        'num_samples':10000,\n",
    "        'datas_path':'http://www.manythings.org/anki/',\n",
    "        'lang_file':'deu-eng.zip', # Path to the zip file\n",
    "        'lang_txt_file':'deu.txt', # The actual text file\n",
    "        'weight_file':'tmp/languages/german_weights.hd5' # Weights from training.\n",
    "    },\n",
    "    'russian':{\n",
    "        'batch_size':64,\n",
    "        'epochs':100,\n",
    "        'latent_dim':256,\n",
    "        'num_samples':10000,\n",
    "        'datas_path':'http://www.manythings.org/anki/',\n",
    "        'lang_file':'rus-eng.zip', # Path to the zip file\n",
    "        'lang_txt_file':'rus.txt', # The actual text file\n",
    "        'weight_file':'tmp/languages/russian_weights.hd5' # Weights from training.\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to create an RNN for machine translation of English to some desired language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(language):\n",
    "  \n",
    "    batch_size = language['batch_size']  # Batch size for training.\n",
    "    epochs = language['epochs']          # Number of epochs to train for.\n",
    "    latent_dim = language['latent_dim']  # Latent dimensionality of the encoding space.\n",
    "    num_samples = language['num_samples']  # Number of samples to train on.\n",
    "    directory_path = language['datas_path']\n",
    "    language_file = language['lang_file']\n",
    "    text_file = \"tmp/languages/{}\".format(language['lang_txt_file'])\n",
    "    weight_path = language['weight_file']\n",
    "\n",
    "    # Vectorize the data.\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    input_characters = set()\n",
    "    target_characters = set()\n",
    "\n",
    "    # Get, unzip, and read the language training file\n",
    "    # Before downloading, let's see if we already got the file\n",
    "    local_lang_txt = Path(text_file)\n",
    "    if local_lang_txt.is_file() is False:\n",
    "        print('Downloading language file {}'.format(language_file))\n",
    "        url = '{}{}'.format(directory_path, language_file)\n",
    "        r = requests.get(url, stream=True)\n",
    "        if r.ok:\n",
    "            print('Successfully downloaded file {}'.format(url))\n",
    "            z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "            z.extractall('tmp/languages')\n",
    "        else:\n",
    "            print('FATAL: Cannot retrieve language file {}'.format(url))\n",
    "\n",
    "    else:\n",
    "        print('Language file {} found'.format(local_lang_txt))\n",
    "\n",
    "    # Read that text file into the vector...\n",
    "    with open(local_lang_txt, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "    for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "        input_text, target_text = line.split('\\t')\n",
    "        # We use \"tab\" as the \"start sequence\" character\n",
    "        # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "        target_text = '\\t' + target_text + '\\n'\n",
    "        input_texts.append(input_text)\n",
    "        target_texts.append(target_text)\n",
    "        for char in input_text:\n",
    "            if char not in input_characters:\n",
    "                input_characters.add(char)\n",
    "        for char in target_text:\n",
    "            if char not in target_characters:\n",
    "                target_characters.add(char)\n",
    "\n",
    "    input_characters = sorted(list(input_characters))\n",
    "    target_characters = sorted(list(target_characters))\n",
    "    num_encoder_tokens = len(input_characters)\n",
    "    num_decoder_tokens = len(target_characters)\n",
    "    max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "    max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "    print('Number of samples:', len(input_texts))\n",
    "    print('Number of unique input tokens:', num_encoder_tokens)\n",
    "    print('Number of unique output tokens:', num_decoder_tokens)\n",
    "    print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "    print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "    input_token_index = dict(\n",
    "        [(char, i) for i, char in enumerate(input_characters)])\n",
    "    target_token_index = dict(\n",
    "        [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "    encoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "        dtype='float32')\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "        dtype='float32')\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, char in enumerate(input_text):\n",
    "            encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "        for t, char in enumerate(target_text):\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = LSTM(latent_dim, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    # Base optimizer is RMSprop. As we have coded up our greedy version,\n",
    "    # we can now use RMSprop in a 'greedy environment'.\n",
    "    optimizer = optimizers.RMSprop(lr=0.025)\n",
    "\n",
    "    # Run training.  Have to set the metrics to use the greedy function.\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', 'mae'])\n",
    "\n",
    "    # It takes a long time to train, so if we have the\n",
    "    # weights, we won't be training\n",
    "    weight_file = Path(weight_path)\n",
    "    if weight_file.is_file() is False:\n",
    "\n",
    "        # We don't have weights - in this case we call the greedy optimizer to generate\n",
    "        # the weight files.  Even at that, it still takes hours and hours to generate\n",
    "        # the weights.\n",
    "        model = greedy_optimizer([encoder_input_data, decoder_input_data], decoder_target_data, batch_size,\n",
    "                                 model, number_epochs=epochs, validation_split=0.02\n",
    "                                )\n",
    "\n",
    "        # Save the model - we use this for deos.\n",
    "        model.save(weight_path)\n",
    "\n",
    "    else:   \n",
    "        # Load the existing weight model for the language we are working on.\n",
    "        print(\"Loading model with saved weights from {}\".format(weight_path))\n",
    "        model.load_weights(weight_path)\n",
    "\n",
    "    # Next: inference mode (sampling).\n",
    "    # Here's the drill:\n",
    "    # 1) encode input and retrieve initial decoder state\n",
    "    # 2) run one step of decoder with this initial state\n",
    "    # and a \"start of sequence\" token as target.\n",
    "    # Output will be the next target token\n",
    "    # 3) Repeat with the current target token and current states\n",
    "\n",
    "    # Define sampling models\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "\n",
    "    # Reverse-lookup token index to decode sequences back to\n",
    "    # something readable.\n",
    "    reverse_input_char_index = dict(\n",
    "        (i, char) for char, i in input_token_index.items())\n",
    "    reverse_target_char_index = dict(\n",
    "        (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "    def decode_sequence(input_seq):\n",
    "        # Encode the input as state vectors.\n",
    "        states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "        # Generate empty target sequence of length 1.\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        # Populate the first character of target sequence with the start character.\n",
    "        target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "        # Sampling loop for a batch of sequences\n",
    "        # (to simplify, here we assume a batch of size 1).\n",
    "        stop_condition = False\n",
    "        decoded_sentence = ''\n",
    "        while not stop_condition:\n",
    "            output_tokens, h, c = decoder_model.predict(\n",
    "                [target_seq] + states_value)\n",
    "\n",
    "            # Sample a token\n",
    "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "            sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "            decoded_sentence += sampled_char\n",
    "\n",
    "            # Exit condition: either hit max length\n",
    "            # or find stop character.\n",
    "            if (sampled_char == '\\n' or\n",
    "               len(decoded_sentence) > max_decoder_seq_length):\n",
    "                stop_condition = True\n",
    "\n",
    "            # Update the target sequence (of length 1).\n",
    "            target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "            target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "            # Update states\n",
    "            states_value = [h, c]\n",
    "\n",
    "        return decoded_sentence\n",
    "\n",
    "\n",
    "    print(\"_______________________________________________________________\")\n",
    "    for seq_index in range(100):\n",
    "        # Take one sequence (part of the training set)\n",
    "        # for trying out decoding.\n",
    "        input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "        decoded_sentence = decode_sequence(input_seq)\n",
    "        print('-')\n",
    "        print('Input sentence:', input_texts[seq_index])\n",
    "        print('Decoded sentence:', decoded_sentence)\n",
    "        \n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language file tmp/languages/fra.txt found\n",
      "Number of samples: 10000\n",
      "Number of unique input tokens: 71\n",
      "Number of unique output tokens: 94\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 59\n",
      "Loading model with saved weights from tmp/languages/french_weights.hd5\n",
      "_______________________________________________________________\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Divrais-je en mesure de nous voir ?\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Prenez vos jambes à votre cou !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Prenez vos jambes à votre cou !\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Il va bien.\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Prena travaille.\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Aidez-nous, je vous prie !\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Fais-le donc !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête de te quereller !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête de te quereller !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête de te quereller !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attends une seconde !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attends une seconde !\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Va avec Tom.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Va avec Tom.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Va avec Tom.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Je l'ai démarré.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: J'ai essayé.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je vous ai réveillées.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Je vous ai réveillées.\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Oh non !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attaque !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attaque !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Redresse-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Redresse-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Redresse-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Redresse-toi.\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Allez chercher votre matériel !\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Va doucement !\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Va doucement !\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Va doucement !\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Compris !\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Compris !\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Compris !\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Compris !\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Compris !\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Montez.\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Montez.\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Serrez-moi dans vos bras !\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Serrez-moi dans vos bras !\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je me suis senti tropatable.\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je me suis senti tropatable.\n",
      "\n",
      "-\n",
      "Input sentence: I know.\n",
      "Decoded sentence: Je le sais.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je vous ai laissée tomber.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Je vous ai laissée tomber.\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: J'ai perdu mes clés.\n",
      "\n",
      "-\n",
      "Input sentence: I'm 19.\n",
      "Decoded sentence: Je suis choqué.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je suis chatouilleuse.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Je suis chatouilleuse.\n",
      "\n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: Écoute bien.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Personne n'est chez lui.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Personne n'est chez lui.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Personne n'est chez lui.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Personne n'est chez lui.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Personne n'est chez lui.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Personne n'est chez lui.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Personne n'est chez lui.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Personne n'est chez lui.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Personne n'est chez lui.\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Lisez-la-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Lisez-la-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Lisez-la-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Thanks.\n",
      "Decoded sentence: Merci bien.\n",
      "\n",
      "-\n",
      "Input sentence: We try.\n",
      "Decoded sentence: Nous nous sommes rencontrés aujourd'hui.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous avons attendu.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous avons attendu.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous avons attendu.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Nous avons attendu.\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: Demande à quiconque !\n",
      "\n",
      "-\n",
      "Input sentence: Awesome!\n",
      "Decoded sentence: Fantastique !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez calme !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez calme !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Soyez calme !\n",
      "\n",
      "-\n",
      "Input sentence: Be cool.\n",
      "Decoded sentence: Soyez prudente !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez calmes !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez calmes !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez calmes !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez calmes !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez calmes !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Soyez calmes !\n",
      "\n",
      "-\n",
      "Input sentence: Be kind.\n",
      "Decoded sentence: Soyez prudente !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez prudente !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez prudente !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez prudente !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez prudente !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez prudente !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Soyez prudente !\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Bats les œufs.\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Appelle la sécurité !\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Appelle la sécurité !\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Appelle la sécurité !\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Appelle la sécurité !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Viens seul !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Viens seul !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Viens seul !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Viens seul !\n",
      "\n",
      "-\n",
      "Input sentence: Come on!\n",
      "Decoded sentence: Viens seul !\n",
      "\n",
      "-\n",
      "Input sentence: Come on.\n",
      "Decoded sentence: Viens seul !\n",
      "\n",
      "-\n",
      "Input sentence: Come on.\n",
      "Decoded sentence: Viens seul !\n",
      "\n",
      "-\n",
      "Input sentence: Come on.\n",
      "Decoded sentence: Viens seul !\n",
      "\n",
      "-\n",
      "Input sentence: Drop it!\n",
      "Decoded sentence: Laissez-le tomber !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "french_model, encoder_model, decoder_model = translate(languages['french'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_47 (InputLayer)           (None, None, 71)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_48 (InputLayer)           (None, None, 94)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_33 (LSTM)                  [(None, 256), (None, 335872      input_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_34 (LSTM)                  [(None, None, 256),  359424      input_48[0][0]                   \n",
      "                                                                 lstm_33[0][1]                    \n",
      "                                                                 lstm_33[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, None, 94)     24158       lstm_34[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 719,454\n",
      "Trainable params: 719,454\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "french_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_47 (InputLayer)        (None, None, 71)          0         \n",
      "_________________________________________________________________\n",
      "lstm_33 (LSTM)               [(None, 256), (None, 256) 335872    \n",
      "=================================================================\n",
      "Total params: 335,872\n",
      "Trainable params: 335,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_48 (InputLayer)           (None, None, 94)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_49 (InputLayer)           (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_50 (InputLayer)           (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_34 (LSTM)                  [(None, None, 256),  359424      input_48[0][0]                   \n",
      "                                                                 input_49[0][0]                   \n",
      "                                                                 input_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, None, 94)     24158       lstm_34[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 383,582\n",
      "Trainable params: 383,582\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language file tmp/languages/spa.txt found\n",
      "Number of samples: 10000\n",
      "Number of unique input tokens: 71\n",
      "Number of unique output tokens: 86\n",
      "Max sequence length for inputs: 17\n",
      "Max sequence length for outputs: 42\n",
      "Loading model with saved weights from tmp/languages/spanish_weights.hd5\n",
      "_______________________________________________________________\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Vete.\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Vete.\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Vete.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Hola.\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: ¡Corre!\n",
      "\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: ¿Quién?\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: ¡Órale!\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: ¡Fuego!\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: ¡Fuego!\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: ¡Fuego!\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Ayúdame.\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Ayúdame.\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Ayúdame.\n",
      "\n",
      "-\n",
      "Input sentence: Jump!\n",
      "Decoded sentence: Salte.\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Salte.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: ¡Parad!\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: ¡Parad!\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: ¡Parad!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: ¡Espera!\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Espera. No te vayas.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Entra.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Entra.\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: ¡Hola a todos!\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Yo confío en él.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Me pregunto a Tom.\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: ¡Oh, no!\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Terminen las matemáticas.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Sonríe.\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: ¡Al ataque!\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: ¡Al ataque!\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Sal de aquí.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Ve ahora mismo.\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: ¡Lo tengo!\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: ¿Lo pillas?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: ¿Lo pillas?\n",
      "\n",
      "-\n",
      "Input sentence: He ran.\n",
      "Decoded sentence: Él corrió.\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Retente por ti.\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Abrázame.\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Me sentía cantando.\n",
      "\n",
      "-\n",
      "Input sentence: I know.\n",
      "Decoded sentence: Sé que lo ha hecho él.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Yo los dejo ir.\n",
      "\n",
      "-\n",
      "Input sentence: I lied.\n",
      "Decoded sentence: Mentí.\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: Los quiero a Tom.\n",
      "\n",
      "-\n",
      "Input sentence: I quit.\n",
      "Decoded sentence: Dimito.\n",
      "\n",
      "-\n",
      "Input sentence: I quit.\n",
      "Decoded sentence: Dimito.\n",
      "\n",
      "-\n",
      "Input sentence: I work.\n",
      "Decoded sentence: Trabajo con él.\n",
      "\n",
      "-\n",
      "Input sentence: I'm 19.\n",
      "Decoded sentence: Soy muy feliz.\n",
      "\n",
      "-\n",
      "Input sentence: I'm up.\n",
      "Decoded sentence: Estoy levantado.\n",
      "\n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: Escuche usted un momento.\n",
      "\n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: Escuche usted un momento.\n",
      "\n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: Escuche usted un momento.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: ¡De ninguna manera!\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: ¡De ninguna manera!\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: ¡De ninguna manera!\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: ¡De ninguna manera!\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: ¡De ninguna manera!\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: ¡De ninguna manera!\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: ¡De ninguna manera!\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: ¡De ninguna manera!\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: ¡De ninguna manera!\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: ¡De ninguna manera!\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Lee esto ahora.\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Lee esto ahora.\n",
      "\n",
      "-\n",
      "Input sentence: Thanks.\n",
      "Decoded sentence: Gracias otra vez.\n",
      "\n",
      "-\n",
      "Input sentence: Thanks.\n",
      "Decoded sentence: Gracias otra vez.\n",
      "\n",
      "-\n",
      "Input sentence: Try it.\n",
      "Decoded sentence: Pruébalo.\n",
      "\n",
      "-\n",
      "Input sentence: We try.\n",
      "Decoded sentence: Lo procuramos.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Decoded sentence: Comemos perros.\n",
      "\n",
      "-\n",
      "Input sentence: Why me?\n",
      "Decoded sentence: ¿Por qué está enfadado?\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: Pregúntale a Tom.\n",
      "\n",
      "-\n",
      "Input sentence: Awesome!\n",
      "Decoded sentence: ¡Órale!\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Sé puntual.\n",
      "\n",
      "-\n",
      "Input sentence: Be cool.\n",
      "Decoded sentence: Sé puntual.\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sé razonable.\n",
      "\n",
      "-\n",
      "Input sentence: Be kind.\n",
      "Decoded sentence: Sé razonable.\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sé específica.\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Dejo a casa a Tom.\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Llama a la Policía.\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Llama a la Policía.\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Llama a la Policía.\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: Llama a la Policía.\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Ven a ayudarme.\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Ven a ayudarme.\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Ven a ayudarme.\n",
      "\n",
      "-\n",
      "Input sentence: Come on!\n",
      "Decoded sentence: ¡Pásenle!\n",
      "\n",
      "-\n",
      "Input sentence: Come on.\n",
      "Decoded sentence: Ven a ayudarme.\n",
      "\n",
      "-\n",
      "Input sentence: Come on.\n",
      "Decoded sentence: Ven a ayudarme.\n",
      "\n",
      "-\n",
      "Input sentence: Drop it!\n",
      "Decoded sentence: ¡Suelta el cuchillo!\n",
      "\n",
      "-\n",
      "Input sentence: Get Tom.\n",
      "Decoded sentence: Sal de aquí.\n",
      "\n",
      "-\n",
      "Input sentence: Get out!\n",
      "Decoded sentence: ¡Vete ya!\n",
      "\n",
      "-\n",
      "Input sentence: Get out.\n",
      "Decoded sentence: Sal de aquí.\n",
      "\n",
      "-\n",
      "Input sentence: Get out.\n",
      "Decoded sentence: Sal de aquí.\n",
      "\n",
      "-\n",
      "Input sentence: Get out.\n",
      "Decoded sentence: Sal de aquí.\n",
      "\n",
      "-\n",
      "Input sentence: Get out.\n",
      "Decoded sentence: Sal de aquí.\n",
      "\n",
      "-\n",
      "Input sentence: Get out.\n",
      "Decoded sentence: Sal de aquí.\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: ¡Lárgate!\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: ¡Lárgate!\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: ¡Lárgate!\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: ¡Lárgate!\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: ¡Lárgate!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spanish_model, _, _ = translate(languages['spanish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language file tmp/languages/deu.txt found\n",
      "Number of samples: 10000\n",
      "Number of unique input tokens: 69\n",
      "Number of unique output tokens: 85\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 74\n",
      "Loading model with saved weights from tmp/languages/german_weights.hd5\n",
      "_______________________________________________________________\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Hallo!\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Hallo!\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Schauen Sie auf zu schießen.\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Stirb mal Grünzen Sie auf.\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Stirb mal Grünzen Sie auf.\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Feuer!\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Hilf mir.\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Hilf mir.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Hör auf zu schreien!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Warte mal kurz!\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Hallo, Tom.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Ich hab's.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Ich werde nicht lachen.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Ich werde nicht lachen.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Lächeln!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Schließen Sie die e Füllen.\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Gehen Sie spazieren!\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Gehen Sie spazieren!\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Gute Nacht, Tom!\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Gute Nacht, Tom!\n",
      "\n",
      "-\n",
      "Input sentence: He ran.\n",
      "Decoded sentence: Er gab nach.\n",
      "\n",
      "-\n",
      "Input sentence: He ran.\n",
      "Decoded sentence: Er gab nach.\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Mach mit!\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Nimm mich in den Arm!\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Nimm mich in den Arm!\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Nimm mich in den Arm!\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Ich fühlte mich allein.\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Ich fühlte mich allein.\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Ich fühlte mich allein.\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Ich fühlte mich allein.\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Ich fühlte mich allein.\n",
      "\n",
      "-\n",
      "Input sentence: I know.\n",
      "Decoded sentence: Ich kenne den Weg.\n",
      "\n",
      "-\n",
      "Input sentence: I lied.\n",
      "Decoded sentence: Ich mag diese Arbeit.\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: Ich liebe den Montag!\n",
      "\n",
      "-\n",
      "Input sentence: I'm 19.\n",
      "Decoded sentence: Ich bin nicht verletzt.\n",
      "\n",
      "-\n",
      "Input sentence: I'm 19.\n",
      "Decoded sentence: Ich bin nicht verletzt.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Ich bin nicht verletzt.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Ich bin nicht verletzt.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Es hat mich niemand unterrichtet.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Es hat mich niemand unterrichtet.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Es hat mich niemand unterrichtet.\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: Es hat mich niemand unterrichtet.\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Hilf mir einen Koch.\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Hilf mir einen Koch.\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Hilf mir einen Koch.\n",
      "\n",
      "-\n",
      "Input sentence: Thanks.\n",
      "Decoded sentence: Danke!\n",
      "\n",
      "-\n",
      "Input sentence: Try it.\n",
      "Decoded sentence: Versuch doch, mich aufzuhalten!\n",
      "\n",
      "-\n",
      "Input sentence: Why me?\n",
      "Decoded sentence: Warum ich?\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: Frag Tom!\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: Frag Tom!\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: Frag Tom!\n",
      "\n",
      "-\n",
      "Input sentence: Be cool.\n",
      "Decoded sentence: Sei vernünftig!\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sei nett zu ihr.\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: Sei nett zu ihr.\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sei nett!\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: Sei nett!\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Scher dich weg!\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Scher dich weg!\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Scher dich weg!\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Scher dich weg!\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Scher dich weg!\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Scher dich weg!\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Scher dich weg!\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Scher dich weg!\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Scher dich weg!\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Scher dich weg!\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Scher dich weg!\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Scher dich weg!\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Scher dich weg!\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Scher dich weg!\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Scher dich weg!\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Scher dich weg!\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Scher dich weg!\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Rufen Sie die Polizei!\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Komm herein.\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: Komm herein.\n",
      "\n",
      "-\n",
      "Input sentence: Come on!\n",
      "Decoded sentence: Komm herein.\n",
      "\n",
      "-\n",
      "Input sentence: Come on!\n",
      "Decoded sentence: Komm herein.\n",
      "\n",
      "-\n",
      "Input sentence: Come on!\n",
      "Decoded sentence: Komm herein.\n",
      "\n",
      "-\n",
      "Input sentence: Come on!\n",
      "Decoded sentence: Komm herein.\n",
      "\n",
      "-\n",
      "Input sentence: Get out!\n",
      "Decoded sentence: Geh weg!\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: Geh schlafen!\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: Geh schlafen!\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: Geh schlafen!\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: Geh schlafen!\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: Geh schlafen!\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: Geh schlafen!\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: Geh schlafen!\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: Geh schlafen!\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: Geh schlafen!\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: Geh schlafen!\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: Geh schlafen!\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: Geh schlafen!\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: Geh schlafen!\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: Geh schlafen!\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: Geh schlafen!\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: Geh schlafen!\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: Geh schlafen!\n",
      "\n",
      "-\n",
      "Input sentence: Go away.\n",
      "Decoded sentence: Geh schlafen!\n",
      "\n",
      "-\n",
      "Input sentence: Go away.\n",
      "Decoded sentence: Geh schlafen!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "german_model, _, _ = translate(languages['german'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading language file rus-eng.zip\n",
      "Successfully downloaded file http://www.manythings.org/anki/rus-eng.zip\n",
      "Number of samples: 10000\n",
      "Number of unique input tokens: 72\n",
      "Number of unique output tokens: 91\n",
      "Max sequence length for inputs: 14\n",
      "Max sequence length for outputs: 40\n",
      "Iteration 0 of 100\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 260s - loss: 0.9100 - acc: 0.1343 - mean_absolute_error: 0.0131 - val_loss: 0.8519 - val_acc: 0.1857 - val_mean_absolute_error: 0.0125\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 238s - loss: 0.5688 - acc: 0.2113 - mean_absolute_error: 0.0115 - val_loss: 0.7159 - val_acc: 0.2367 - val_mean_absolute_error: 0.0117\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 240s - loss: 0.4642 - acc: 0.2409 - mean_absolute_error: 0.0107 - val_loss: 0.6845 - val_acc: 0.2480 - val_mean_absolute_error: 0.0113\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 224s - loss: 0.4159 - acc: 0.2536 - mean_absolute_error: 0.0104 - val_loss: 0.6438 - val_acc: 0.2625 - val_mean_absolute_error: 0.0109\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 229s - loss: 0.3857 - acc: 0.2621 - mean_absolute_error: 0.0102 - val_loss: 0.6387 - val_acc: 0.2652 - val_mean_absolute_error: 0.0107\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 242s - loss: 0.3663 - acc: 0.2674 - mean_absolute_error: 0.0100 - val_loss: 0.6262 - val_acc: 0.2731 - val_mean_absolute_error: 0.0107\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 227s - loss: 0.3499 - acc: 0.2719 - mean_absolute_error: 0.0099 - val_loss: 0.6002 - val_acc: 0.2767 - val_mean_absolute_error: 0.0105\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 241s - loss: 0.3380 - acc: 0.2750 - mean_absolute_error: 0.0098 - val_loss: 0.6028 - val_acc: 0.2810 - val_mean_absolute_error: 0.0105\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 228s - loss: 0.3281 - acc: 0.2778 - mean_absolute_error: 0.0098 - val_loss: 0.6094 - val_acc: 0.2794 - val_mean_absolute_error: 0.0104\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 234s - loss: 0.3172 - acc: 0.2811 - mean_absolute_error: 0.0097 - val_loss: 0.6258 - val_acc: 0.2793 - val_mean_absolute_error: 0.0105\n",
      "Iteration 10 of 100\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 225s - loss: 0.3105 - acc: 0.2829 - mean_absolute_error: 0.0096 - val_loss: 0.6279 - val_acc: 0.2729 - val_mean_absolute_error: 0.0104\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 231s - loss: 0.3049 - acc: 0.2846 - mean_absolute_error: 0.0096 - val_loss: 0.6234 - val_acc: 0.2829 - val_mean_absolute_error: 0.0103\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 235s - loss: 0.2985 - acc: 0.2862 - mean_absolute_error: 0.0095 - val_loss: 0.6022 - val_acc: 0.2854 - val_mean_absolute_error: 0.0103\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 221s - loss: 0.2930 - acc: 0.2879 - mean_absolute_error: 0.0095 - val_loss: 0.6208 - val_acc: 0.2900 - val_mean_absolute_error: 0.0102\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 231s - loss: 0.2887 - acc: 0.2890 - mean_absolute_error: 0.0094 - val_loss: 0.6088 - val_acc: 0.2894 - val_mean_absolute_error: 0.0102\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 214s - loss: 0.2836 - acc: 0.2905 - mean_absolute_error: 0.0094 - val_loss: 0.6361 - val_acc: 0.2862 - val_mean_absolute_error: 0.0102\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 213s - loss: 0.2795 - acc: 0.2915 - mean_absolute_error: 0.0094 - val_loss: 0.6046 - val_acc: 0.2936 - val_mean_absolute_error: 0.0101\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 206s - loss: 0.2764 - acc: 0.2923 - mean_absolute_error: 0.0093 - val_loss: 0.6153 - val_acc: 0.2887 - val_mean_absolute_error: 0.0102\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 220s - loss: 0.2720 - acc: 0.2937 - mean_absolute_error: 0.0093 - val_loss: 0.6199 - val_acc: 0.2894 - val_mean_absolute_error: 0.0101\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 224s - loss: 0.2682 - acc: 0.2947 - mean_absolute_error: 0.0093 - val_loss: 0.6356 - val_acc: 0.2875 - val_mean_absolute_error: 0.0102\n",
      "Iteration 20 of 100\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 249s - loss: 0.2668 - acc: 0.2953 - mean_absolute_error: 0.0093 - val_loss: 0.6553 - val_acc: 0.2882 - val_mean_absolute_error: 0.0102\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 247s - loss: 0.2636 - acc: 0.2962 - mean_absolute_error: 0.0092 - val_loss: 0.6117 - val_acc: 0.2888 - val_mean_absolute_error: 0.0101\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 246s - loss: 0.2616 - acc: 0.2970 - mean_absolute_error: 0.0092 - val_loss: 0.6452 - val_acc: 0.2794 - val_mean_absolute_error: 0.0104\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 230s - loss: 0.2584 - acc: 0.2978 - mean_absolute_error: 0.0092 - val_loss: 0.6643 - val_acc: 0.2855 - val_mean_absolute_error: 0.0100\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 258s - loss: 0.2547 - acc: 0.2990 - mean_absolute_error: 0.0092 - val_loss: 0.6905 - val_acc: 0.2796 - val_mean_absolute_error: 0.0102\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 275s - loss: 0.2542 - acc: 0.2990 - mean_absolute_error: 0.0092 - val_loss: 0.6302 - val_acc: 0.2961 - val_mean_absolute_error: 0.0100\n",
      "On step 25, did not have a convergent step\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 239s - loss: 0.2516 - acc: 0.2998 - mean_absolute_error: 0.0091 - val_loss: 0.6966 - val_acc: 0.2876 - val_mean_absolute_error: 0.0101\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 253s - loss: 0.2498 - acc: 0.3001 - mean_absolute_error: 0.0091 - val_loss: 0.6522 - val_acc: 0.2821 - val_mean_absolute_error: 0.0101\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 261s - loss: 0.2490 - acc: 0.3005 - mean_absolute_error: 0.0091 - val_loss: 0.6590 - val_acc: 0.2945 - val_mean_absolute_error: 0.0099\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 263s - loss: 0.2463 - acc: 0.3012 - mean_absolute_error: 0.0091 - val_loss: 0.6360 - val_acc: 0.2951 - val_mean_absolute_error: 0.0099\n",
      "Iteration 30 of 100\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 262s - loss: 0.2430 - acc: 0.3023 - mean_absolute_error: 0.0091 - val_loss: 0.6264 - val_acc: 0.2950 - val_mean_absolute_error: 0.0099\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 258s - loss: 0.2423 - acc: 0.3023 - mean_absolute_error: 0.0091 - val_loss: 0.6533 - val_acc: 0.2945 - val_mean_absolute_error: 0.0100\n",
      "On step 31 (2nd non-convergence), reduced lr from 0.02500000037252903 to 0.015450849948982973\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 257s - loss: 0.2173 - acc: 0.3100 - mean_absolute_error: 0.0089 - val_loss: 0.6396 - val_acc: 0.2971 - val_mean_absolute_error: 0.0099\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 233s - loss: 0.2055 - acc: 0.3135 - mean_absolute_error: 0.0088 - val_loss: 0.6890 - val_acc: 0.2930 - val_mean_absolute_error: 0.0099\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 260s - loss: 0.2005 - acc: 0.3149 - mean_absolute_error: 0.0087 - val_loss: 0.6775 - val_acc: 0.2985 - val_mean_absolute_error: 0.0098\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 235s - loss: 0.1976 - acc: 0.3157 - mean_absolute_error: 0.0087 - val_loss: 0.6803 - val_acc: 0.2977 - val_mean_absolute_error: 0.0098\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 219s - loss: 0.1943 - acc: 0.3169 - mean_absolute_error: 0.0087 - val_loss: 0.6944 - val_acc: 0.2971 - val_mean_absolute_error: 0.0098\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 202s - loss: 0.1923 - acc: 0.3171 - mean_absolute_error: 0.0087 - val_loss: 0.6823 - val_acc: 0.2987 - val_mean_absolute_error: 0.0098\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 192s - loss: 0.1901 - acc: 0.3175 - mean_absolute_error: 0.0086 - val_loss: 0.6636 - val_acc: 0.2975 - val_mean_absolute_error: 0.0097\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 192s - loss: 0.1893 - acc: 0.3181 - mean_absolute_error: 0.0086 - val_loss: 0.7251 - val_acc: 0.2926 - val_mean_absolute_error: 0.0098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40 of 100\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 195s - loss: 0.1878 - acc: 0.3184 - mean_absolute_error: 0.0086 - val_loss: 0.6909 - val_acc: 0.2951 - val_mean_absolute_error: 0.0098\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 196s - loss: 0.1862 - acc: 0.3189 - mean_absolute_error: 0.0086 - val_loss: 0.7324 - val_acc: 0.2951 - val_mean_absolute_error: 0.0098\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 198s - loss: 0.1856 - acc: 0.3188 - mean_absolute_error: 0.0086 - val_loss: 0.7076 - val_acc: 0.2952 - val_mean_absolute_error: 0.0098\n",
      "On step 42, did not have a convergent step\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 193s - loss: 0.1840 - acc: 0.3193 - mean_absolute_error: 0.0086 - val_loss: 0.7147 - val_acc: 0.2970 - val_mean_absolute_error: 0.0098\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 208s - loss: 0.1834 - acc: 0.3198 - mean_absolute_error: 0.0086 - val_loss: 0.7089 - val_acc: 0.2940 - val_mean_absolute_error: 0.0098\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 191s - loss: 0.1826 - acc: 0.3198 - mean_absolute_error: 0.0086 - val_loss: 0.7238 - val_acc: 0.2947 - val_mean_absolute_error: 0.0098\n",
      "On step 45 (2nd non-convergence), reduced lr from 0.015450850129127502 to 0.009549150534881498\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 215s - loss: 0.1693 - acc: 0.3235 - mean_absolute_error: 0.0085 - val_loss: 0.7358 - val_acc: 0.2984 - val_mean_absolute_error: 0.0097\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 215s - loss: 0.1626 - acc: 0.3252 - mean_absolute_error: 0.0084 - val_loss: 0.7481 - val_acc: 0.2949 - val_mean_absolute_error: 0.0098\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 215s - loss: 0.1602 - acc: 0.3260 - mean_absolute_error: 0.0084 - val_loss: 0.7541 - val_acc: 0.2935 - val_mean_absolute_error: 0.0097\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 230s - loss: 0.1585 - acc: 0.3264 - mean_absolute_error: 0.0084 - val_loss: 0.7597 - val_acc: 0.2970 - val_mean_absolute_error: 0.0097\n",
      "Iteration 50 of 100\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 231s - loss: 0.1569 - acc: 0.3270 - mean_absolute_error: 0.0083 - val_loss: 0.7583 - val_acc: 0.2969 - val_mean_absolute_error: 0.0097\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 214s - loss: 0.1557 - acc: 0.3272 - mean_absolute_error: 0.0083 - val_loss: 0.7419 - val_acc: 0.2989 - val_mean_absolute_error: 0.0097\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 212s - loss: 0.1549 - acc: 0.3271 - mean_absolute_error: 0.0083 - val_loss: 0.7907 - val_acc: 0.2936 - val_mean_absolute_error: 0.0098\n",
      "On step 52, did not have a convergent step\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 219s - loss: 0.1542 - acc: 0.3274 - mean_absolute_error: 0.0083 - val_loss: 0.7744 - val_acc: 0.2971 - val_mean_absolute_error: 0.0097\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 218s - loss: 0.1530 - acc: 0.3276 - mean_absolute_error: 0.0083 - val_loss: 0.7789 - val_acc: 0.2954 - val_mean_absolute_error: 0.0097\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 227s - loss: 0.1524 - acc: 0.3276 - mean_absolute_error: 0.0083 - val_loss: 0.7996 - val_acc: 0.2944 - val_mean_absolute_error: 0.0097\n",
      "On step 55 (2nd non-convergence), reduced lr from 0.009549150243401527 to 0.005901699414101475\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 219s - loss: 0.1447 - acc: 0.3297 - mean_absolute_error: 0.0082 - val_loss: 0.7928 - val_acc: 0.2990 - val_mean_absolute_error: 0.0097\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 210s - loss: 0.1411 - acc: 0.3304 - mean_absolute_error: 0.0082 - val_loss: 0.8063 - val_acc: 0.2947 - val_mean_absolute_error: 0.0097\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 214s - loss: 0.1400 - acc: 0.3304 - mean_absolute_error: 0.0082 - val_loss: 0.8210 - val_acc: 0.2957 - val_mean_absolute_error: 0.0097\n",
      "On step 58, did not have a convergent step\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 211s - loss: 0.1390 - acc: 0.3308 - mean_absolute_error: 0.0082 - val_loss: 0.8240 - val_acc: 0.2939 - val_mean_absolute_error: 0.0097\n",
      "Iteration 60 of 100\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 215s - loss: 0.1380 - acc: 0.3309 - mean_absolute_error: 0.0082 - val_loss: 0.8164 - val_acc: 0.2962 - val_mean_absolute_error: 0.0097\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 212s - loss: 0.1372 - acc: 0.3313 - mean_absolute_error: 0.0082 - val_loss: 0.8260 - val_acc: 0.2944 - val_mean_absolute_error: 0.0097\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 212s - loss: 0.1364 - acc: 0.3312 - mean_absolute_error: 0.0082 - val_loss: 0.8218 - val_acc: 0.2958 - val_mean_absolute_error: 0.0097\n",
      "On step 62 (2nd non-convergence), reduced lr from 0.005901699420064688 to 0.00364745083298552\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 212s - loss: 0.1312 - acc: 0.3328 - mean_absolute_error: 0.0081 - val_loss: 0.8541 - val_acc: 0.2960 - val_mean_absolute_error: 0.0097\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 191s - loss: 0.1295 - acc: 0.3329 - mean_absolute_error: 0.0081 - val_loss: 0.8532 - val_acc: 0.2944 - val_mean_absolute_error: 0.0097\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 191s - loss: 0.1283 - acc: 0.3331 - mean_absolute_error: 0.0081 - val_loss: 0.8615 - val_acc: 0.2923 - val_mean_absolute_error: 0.0097\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 191s - loss: 0.1278 - acc: 0.3331 - mean_absolute_error: 0.0081 - val_loss: 0.8735 - val_acc: 0.2947 - val_mean_absolute_error: 0.0097\n",
      "On step 66, did not have a convergent step\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 189s - loss: 0.1270 - acc: 0.3332 - mean_absolute_error: 0.0081 - val_loss: 0.8827 - val_acc: 0.2949 - val_mean_absolute_error: 0.0097\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 208s - loss: 0.1263 - acc: 0.3339 - mean_absolute_error: 0.0081 - val_loss: 0.8768 - val_acc: 0.2945 - val_mean_absolute_error: 0.0097\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 200s - loss: 0.1257 - acc: 0.3337 - mean_absolute_error: 0.0081 - val_loss: 0.8828 - val_acc: 0.2940 - val_mean_absolute_error: 0.0097\n",
      "On step 69 (2nd non-convergence), reduced lr from 0.0036474508233368397 to 0.002254248581115955\n",
      "Iteration 70 of 100\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 203s - loss: 0.1218 - acc: 0.3348 - mean_absolute_error: 0.0080 - val_loss: 0.8912 - val_acc: 0.2924 - val_mean_absolute_error: 0.0097\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 194s - loss: 0.1206 - acc: 0.3351 - mean_absolute_error: 0.0080 - val_loss: 0.9151 - val_acc: 0.2915 - val_mean_absolute_error: 0.0098\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 193s - loss: 0.1198 - acc: 0.3353 - mean_absolute_error: 0.0080 - val_loss: 0.9236 - val_acc: 0.2920 - val_mean_absolute_error: 0.0097\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 198s - loss: 0.1196 - acc: 0.3351 - mean_absolute_error: 0.0080 - val_loss: 0.9282 - val_acc: 0.2938 - val_mean_absolute_error: 0.0097\n",
      "On step 73, did not have a convergent step\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 199s - loss: 0.1189 - acc: 0.3355 - mean_absolute_error: 0.0080 - val_loss: 0.9034 - val_acc: 0.2934 - val_mean_absolute_error: 0.0097\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 203s - loss: 0.1183 - acc: 0.3357 - mean_absolute_error: 0.0080 - val_loss: 0.9324 - val_acc: 0.2909 - val_mean_absolute_error: 0.0097\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 245s - loss: 0.1180 - acc: 0.3359 - mean_absolute_error: 0.0080 - val_loss: 0.9328 - val_acc: 0.2909 - val_mean_absolute_error: 0.0097\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 217s - loss: 0.1174 - acc: 0.3359 - mean_absolute_error: 0.0080 - val_loss: 0.9242 - val_acc: 0.2915 - val_mean_absolute_error: 0.0097\n",
      "On step 77 (2nd non-convergence), reduced lr from 0.002254248596727848 to 0.001393202251869565\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 209s - loss: 0.1146 - acc: 0.3369 - mean_absolute_error: 0.0080 - val_loss: 0.9540 - val_acc: 0.2925 - val_mean_absolute_error: 0.0097\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 209s - loss: 0.1138 - acc: 0.3372 - mean_absolute_error: 0.0080 - val_loss: 0.9597 - val_acc: 0.2909 - val_mean_absolute_error: 0.0097\n",
      "Iteration 80 of 100\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 226s - loss: 0.1134 - acc: 0.3373 - mean_absolute_error: 0.0080 - val_loss: 0.9569 - val_acc: 0.2926 - val_mean_absolute_error: 0.0097\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 213s - loss: 0.1130 - acc: 0.3374 - mean_absolute_error: 0.0080 - val_loss: 0.9645 - val_acc: 0.2908 - val_mean_absolute_error: 0.0098\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 207s - loss: 0.1126 - acc: 0.3375 - mean_absolute_error: 0.0080 - val_loss: 0.9693 - val_acc: 0.2910 - val_mean_absolute_error: 0.0098\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 210s - loss: 0.1121 - acc: 0.3378 - mean_absolute_error: 0.0080 - val_loss: 0.9788 - val_acc: 0.2902 - val_mean_absolute_error: 0.0098\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 193s - loss: 0.1120 - acc: 0.3378 - mean_absolute_error: 0.0080 - val_loss: 0.9812 - val_acc: 0.2881 - val_mean_absolute_error: 0.0098\n",
      "On step 84, did not have a convergent step\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 192s - loss: 0.1116 - acc: 0.3378 - mean_absolute_error: 0.0080 - val_loss: 0.9764 - val_acc: 0.2862 - val_mean_absolute_error: 0.0098\n",
      "On step 85 (2nd non-convergence), reduced lr from 0.0013932022266089916 to 0.00086104632924639\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 194s - loss: 0.1096 - acc: 0.3388 - mean_absolute_error: 0.0080 - val_loss: 0.9902 - val_acc: 0.2876 - val_mean_absolute_error: 0.0098\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 195s - loss: 0.1091 - acc: 0.3388 - mean_absolute_error: 0.0079 - val_loss: 0.9932 - val_acc: 0.2876 - val_mean_absolute_error: 0.0098\n",
      "On step 87, did not have a convergent step\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 196s - loss: 0.1088 - acc: 0.3388 - mean_absolute_error: 0.0079 - val_loss: 0.9984 - val_acc: 0.2880 - val_mean_absolute_error: 0.0098\n",
      "On step 88 (2nd non-convergence), reduced lr from 0.0008610463119111955 to 0.0005321558866488623\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 191s - loss: 0.1074 - acc: 0.3398 - mean_absolute_error: 0.0079 - val_loss: 0.9980 - val_acc: 0.2891 - val_mean_absolute_error: 0.0098\n",
      "Iteration 90 of 100\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 204s - loss: 0.1072 - acc: 0.3398 - mean_absolute_error: 0.0079 - val_loss: 1.0081 - val_acc: 0.2890 - val_mean_absolute_error: 0.0098\n",
      "On step 90, did not have a convergent step\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 197s - loss: 0.1069 - acc: 0.3399 - mean_absolute_error: 0.0079 - val_loss: 1.0088 - val_acc: 0.2878 - val_mean_absolute_error: 0.0098\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 202s - loss: 0.1067 - acc: 0.3400 - mean_absolute_error: 0.0079 - val_loss: 1.0096 - val_acc: 0.2888 - val_mean_absolute_error: 0.0098\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 197s - loss: 0.1066 - acc: 0.3400 - mean_absolute_error: 0.0079 - val_loss: 1.0129 - val_acc: 0.2887 - val_mean_absolute_error: 0.0098\n",
      "On step 93 (2nd non-convergence), reduced lr from 0.0005321559146977961 to 0.0003288904425975277\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 204s - loss: 0.1056 - acc: 0.3408 - mean_absolute_error: 0.0079 - val_loss: 1.0138 - val_acc: 0.2886 - val_mean_absolute_error: 0.0098\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 201s - loss: 0.1054 - acc: 0.3408 - mean_absolute_error: 0.0079 - val_loss: 1.0179 - val_acc: 0.2887 - val_mean_absolute_error: 0.0098\n",
      "On step 95, did not have a convergent step\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 201s - loss: 0.1053 - acc: 0.3408 - mean_absolute_error: 0.0079 - val_loss: 1.0196 - val_acc: 0.2881 - val_mean_absolute_error: 0.0098\n",
      "On step 96 (2nd non-convergence), reduced lr from 0.0003288904554210603 to 0.00020326548002564738\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 202s - loss: 0.1047 - acc: 0.3414 - mean_absolute_error: 0.0079 - val_loss: 1.0208 - val_acc: 0.2877 - val_mean_absolute_error: 0.0098\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 207s - loss: 0.1046 - acc: 0.3415 - mean_absolute_error: 0.0079 - val_loss: 1.0230 - val_acc: 0.2886 - val_mean_absolute_error: 0.0098\n",
      "Train on 9800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      " - 211s - loss: 0.1045 - acc: 0.3415 - mean_absolute_error: 0.0079 - val_loss: 1.0232 - val_acc: 0.2882 - val_mean_absolute_error: 0.0098\n",
      "On step 99, did not have a convergent step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/keras/engine/topology.py:2364: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________________________________\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Идите сейчас.\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Идите сейчас.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Здрасте.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Здрасте.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Здрасте.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Здрасте.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Здрасте.\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Бегите!\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Бегите!\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Бегите!\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Бегите!\n",
      "\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Кто тебе сказал?\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Вот это да!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Вот это да!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Вот это да!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Вот это да!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Вот это да!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Вот это да!\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Пожар!\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Пожар!\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Помоги мне, Том.\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Помоги мне, Том.\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Помоги мне, Том.\n",
      "\n",
      "-\n",
      "Input sentence: Jump!\n",
      "Decoded sentence: Прыгай!\n",
      "\n",
      "-\n",
      "Input sentence: Jump!\n",
      "Decoded sentence: Прыгай!\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Прыгай!\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Перестань плакать!\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Перестань плакать!\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Перестань плакать!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Подождите немного.\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Подождите немного.\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Подождите немного.\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Подождите немного.\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Подождите немного.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Идите сядьте.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Идите сядьте.\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Привет!\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Привет!\n",
      "\n",
      "-\n",
      "Input sentence: Hurry!\n",
      "Decoded sentence: Давай быстрей.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Я вижу мальчика.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Я вижу мальчика.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Я пытаюсь.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Я пытаюсь.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Я пытаюсь.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я выиграла!\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я выиграла!\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я выиграла!\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я выиграла!\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Наши взгляды встретились.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Попустись.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбнитесь.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбнитесь.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбнитесь.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбнитесь.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбнитесь.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбнитесь.\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Страши Тома?\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Проверьте это.\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Проверьте это.\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Проверьте это.\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Проверьте это.\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Проверьте это.\n",
      "\n",
      "-\n",
      "Input sentence: Eat up.\n",
      "Decoded sentence: Поешь с нами.\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Замри!\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Замри!\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Замри!\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Поднимайся наверх.\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Поднимайся наверх.\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Поднимайся наверх.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите сядьте.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите сядьте.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите сядьте.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите сядьте.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите сядьте.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите сядьте.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите сядьте.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите сядьте.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите сядьте.\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Уходите.\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Уходите.\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Уходите.\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Уходите.\n",
      "\n",
      "-\n",
      "Input sentence: He ran.\n",
      "Decoded sentence: Он бежал.\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Как насчасно?\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Как насчасно?\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Обними меня.\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Я чувствовал себя счастливым.\n",
      "\n",
      "-\n",
      "Input sentence: I knit.\n",
      "Decoded sentence: Я знаю свою работу.\n",
      "\n",
      "-\n",
      "Input sentence: I know.\n",
      "Decoded sentence: Я знаю свою работу.\n",
      "\n",
      "-\n",
      "Input sentence: I know.\n",
      "Decoded sentence: Я знаю свою работу.\n",
      "\n",
      "-\n",
      "Input sentence: I know.\n",
      "Decoded sentence: Я знаю свою работу.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Я ушла.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Я ушла.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Я ушла.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Decoded sentence: Я ушла.\n",
      "\n",
      "-\n",
      "Input sentence: I lied.\n",
      "Decoded sentence: Я соврал.\n",
      "\n",
      "-\n",
      "Input sentence: I lied.\n",
      "Decoded sentence: Я соврал.\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: Я проиграла.\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: Я проиграла.\n",
      "\n",
      "-\n",
      "Input sentence: I paid.\n",
      "Decoded sentence: Я заплатила.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "russian_model, _, _ = translate(languages['russian'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting question - can we tell similarity of languages by weight patterns?  If so, Spanish and French will be more similar than with other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1.0503e+00  -8.3374e-01  -8.7131e-01 ...,  -7.7995e-01  -2.0009e-01\n",
      "   -5.5296e-01]\n",
      " [  3.7340e-02   1.1915e-01   1.9764e-01 ...,  -1.1331e+00  -3.7083e-01\n",
      "   -1.6986e-01]\n",
      " [  4.9896e-03  -4.1231e-02  -4.9719e-02 ...,   4.7060e-02  -3.2271e-02\n",
      "   -3.7802e-04]\n",
      " ..., \n",
      " [  1.0704e-01  -1.3072e-01   7.4463e-01 ...,  -2.3481e+00  -2.0706e+00\n",
      "   -1.5693e+00]\n",
      " [  1.9736e-01  -1.1157e-02   8.8278e-02 ...,   1.1215e-01  -1.3129e-02\n",
      "    1.1559e-01]\n",
      " [  4.6210e-01  -9.4595e-02   8.5379e-02 ...,  -2.7838e-01   3.3167e-01\n",
      "    1.7585e-01]]\n",
      "[[-0.8194 -0.0117  0.2343 ...,  0.7626  0.9406 -0.0624]\n",
      " [-0.6    -0.046  -0.042  ..., -1.3795  0.5039 -0.8089]\n",
      " [ 0.3094  1.808  -0.5267 ...,  0.3552 -0.478  -0.2018]\n",
      " ..., \n",
      " [-1.6392 -0.4398 -1.6586 ..., -0.0406 -0.1637  0.6896]\n",
      " [ 0.4509  0.5202  1.1556 ..., -0.1937  1.9012  0.8856]\n",
      " [ 2.1952  0.0353  0.4069 ...,  0.7803 -0.4042 -0.8309]]\n",
      "[-2.3298 -3.1635 -3.5883 ..., -2.9776 -1.9327 -2.7946]\n",
      "[[  4.5677e-01  -3.4324e-01   2.3304e-01 ...,  -6.6434e-02  -2.1162e+00\n",
      "   -1.3606e+00]\n",
      " [  5.3835e-02  -1.6378e-02  -3.9848e-03 ...,  -4.4025e-02  -5.6401e-02\n",
      "   -1.5674e-02]\n",
      " [ -4.6184e+00  -1.3960e+00  -2.0672e+00 ...,  -9.9844e-01  -3.7870e+00\n",
      "   -9.3050e-01]\n",
      " ..., \n",
      " [  4.3565e-02  -5.4203e-02   1.8954e-02 ...,  -7.0435e-02   6.8712e-02\n",
      "   -1.2124e-02]\n",
      " [  3.6583e-01   4.7775e-01   4.5050e-01 ...,  -6.6130e-01  -1.0889e+00\n",
      "   -9.8125e-03]\n",
      " [ -2.0482e-01   3.6272e-01   2.1286e-01 ...,  -2.2094e-01  -8.7099e-01\n",
      "   -3.0446e-01]]\n",
      "[[  6.1928e-01  -2.6023e-01  -1.1849e+00 ...,   6.2742e-01  -9.3171e-01\n",
      "    1.7074e+00]\n",
      " [ -1.3993e+00   2.9260e-02   2.0896e+00 ...,  -1.1467e+00  -2.0360e-03\n",
      "   -9.1740e-01]\n",
      " [ -5.2701e-01   5.6798e-01  -2.8233e+00 ...,   1.3975e+00  -8.4454e-01\n",
      "   -4.8871e-01]\n",
      " ..., \n",
      " [  1.1909e+00   5.8012e-01  -4.5236e-01 ...,  -9.1922e-01   1.4645e-01\n",
      "    8.4882e-01]\n",
      " [ -1.7692e+00   1.2553e+00  -1.3009e+00 ...,  -9.6450e-01  -2.0632e+00\n",
      "   -2.2524e-01]\n",
      " [ -5.1053e-01   1.1979e+00  -3.7539e+00 ...,  -1.1232e+00  -1.1928e+00\n",
      "    5.2977e-01]]\n",
      "[-1.4406 -0.8887 -1.9698 ..., -2.1249 -2.1879 -3.5443]\n",
      "[[ -5.1574e-02   2.2253e-01   3.1678e+00 ...,  -7.6733e-02   6.3637e-03\n",
      "    7.9150e-01]\n",
      " [  6.7748e-01   4.0627e+00   2.8204e+00 ...,   6.6311e-01   7.6236e+00\n",
      "    6.5046e+00]\n",
      " [ -3.4142e-01  -1.8542e+00  -9.1157e-01 ...,  -3.5341e-01  -1.0650e+00\n",
      "   -6.4087e+00]\n",
      " ..., \n",
      " [ -7.8469e-01  -2.8502e+00  -3.7141e+00 ...,  -5.8324e-01  -2.9899e+00\n",
      "   -4.3034e+00]\n",
      " [ -2.3256e+00  -1.0104e+00  -2.8592e-01 ...,  -2.2783e+00  -1.5380e+01\n",
      "   -2.3595e+00]\n",
      " [ -6.0685e-01  -1.6156e+00  -1.2443e+01 ...,  -6.3331e-01  -9.4709e+00\n",
      "   -1.6675e+01]]\n",
      "[-15.0815  -8.3454  -1.7698  -2.1456 -11.7339 -13.1676  -2.2855 -10.2966\n",
      " -12.5153   1.8088  -1.3575  -2.2017 -11.1739  -9.1638 -13.5209 -11.0279\n",
      " -15.0984 -10.8706 -13.2912  -5.2699  -3.2246   1.0937   2.6084   1.2567\n",
      "   0.4623  -1.2888   1.2119  -1.9995   0.394   -0.7624   1.0506  -3.2804\n",
      "  -1.9808   0.9702   0.2913   0.4872   0.5297  -0.9464   0.3249  -0.2423\n",
      "  -0.6818  -3.5612  -1.2078  -3.2966  -1.9751   0.1543  -0.6372  -0.8269\n",
      "  -2.1434  -0.4055   0.1942  -0.1458  -1.7835   1.7608   0.4506  -1.2107\n",
      "  -1.3689  -1.7792  -2.422   -0.8156   0.6139  -1.8319  -1.2524  -1.0383\n",
      "  -2.587   -1.3706 -12.3386  -0.8214  -0.0255  -1.5186   1.2057  -7.0739\n",
      "  -6.3009  -1.9265  -2.0623  -1.7205  -3.1362  -1.0553   0.2288   0.9026\n",
      "  -0.2236  -1.9251  -0.059  -12.2441   0.3434  -1.4317  -0.8218 -11.1442\n",
      "   0.8436   0.1697   1.0093 -15.0553   1.8377   0.4188]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=4)\n",
    "\n",
    "fw_larr = french_model.get_weights()\n",
    "\n",
    "for arr in fw_larr:\n",
    "    print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
